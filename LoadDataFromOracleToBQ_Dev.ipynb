{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import pyodbc # ms sql server\n",
    "import cx_Oracle # oracle\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e3ff4-4611-44a2-8f68-bdf4f6e08265",
   "metadata": {},
   "source": [
    "# Input Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a2529a92-9fb9-45ef-8ebf-117d868a950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=False\n",
    "is_py=False\n",
    "\n",
    "source_name=''\n",
    "\n",
    "source_name='vw_ipf'\n",
    "\n",
    "#source_name='yit_ar_aging_with_cost'\n",
    "#source_name='yip_wip_project'\n",
    "# source_name='yip_bg_account'\n",
    "\n",
    "#source_name=\"yip_invoice_monthly\" # df\n",
    "\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "#source_name='yip_ap_payment' # csv\n",
    "\n",
    "#source_name=\"yip_pj_status\" # csv\n",
    "\n",
    "# source_name='yip_po_listing' # df\n",
    "\n",
    "#source_name='yip_gl_account' # df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192bc4-6924-4e88-9fc1-981c816f918c",
   "metadata": {},
   "source": [
    "# Time To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "53bf1c7f-c8b9-4017-9b06-629aa83d885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 19:47:54\n",
      "2023-08-21 19:47:54\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# dtStr_imported='2023-06-13 17:11:54'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)\n",
    "\n",
    "\n",
    "env_path=r'D:\\ETL_Orable_To_BQ\\.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Constant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "# init_date_query='2023-01-01 00:00:00'\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "\n",
    "csv_temp_folder='csv_temp'\n",
    "csv_error_folder='csv_error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25b842-861c-4543-b7bf-abe21d688752",
   "metadata": {},
   "source": [
    "# Enter parameter on script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f80fe-b80e-42fb-98cb-cff290f5c031",
   "metadata": {},
   "source": [
    "# Check temp and error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "21674004-5a60-4ce3-bead-dfc1f8d5457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_temp_folder)==False:\n",
    "  os.mkdir(csv_temp_folder)\n",
    "if os.path.exists(csv_error_folder)==False:\n",
    "  os.mkdir(csv_error_folder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Const Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dab69b3d-b0bb-4e97-a8a2-cffdbdcd01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "temp_path=f'{csv_temp_folder}/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'\n",
    "etlDateCols=[updateCol,'creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorSubject,errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] =errorSubject \n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "34cb43ec-1209-4920-921f-bda3d5fbf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_error_file(): # if any error ,then move csv to investigte later\n",
    "    error_csv_path=''\n",
    "    try:\n",
    "        if os.path.exists(temp_path):\n",
    "         error_csv_file=f\"{source_name}_error_{ dt_imported.strftime('%d%m%y_%H%M%S')}.csv\"   \n",
    "         new_temp_path=f'{csv_temp_folder}/{error_csv_file}'\n",
    "         os.rename(temp_path,new_temp_path)  \n",
    "\n",
    "         error_csv_path=f'{csv_error_folder}/{error_csv_file}'\n",
    "         shutil.move(new_temp_path,error_csv_path )\n",
    "         \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "    return   error_csv_path      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        \n",
    "        error_message=f\"ETL Error on {source_name} at {dtStr_imported}\"\n",
    "        # move csv error file to examine later\n",
    "        error_csv_path  =move_error_file()\n",
    "        if error_csv_path!='':\n",
    "            error_message= f\"{error_csv_path} - {error_message}\"\n",
    "        error_message=f\"MIS-BI : {error_message}\"   \n",
    "        \n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        # add to error table into sqlite\n",
    "        recordError=dfError.to_records(index=False) \n",
    "        add_logError(recordError)\n",
    "        \n",
    "        # sened main\n",
    "        emailResult=sendMailForError(error_message, dfError.to_html(index=False))\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f267b-2d60-462c-8f44-f22834dfa5b2",
   "metadata": {},
   "source": [
    "# Read Credential Config from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c0af655e-f711-4b9e-af7b-a854ea13addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read all credential config values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = dotenv_values(dotenv_path=env_path)\n",
    "    projectId=config['PROJECT_ID']\n",
    "    region=config['REGION']\n",
    "    dataset_id=config['DATASET_ID']\n",
    "    table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "    \n",
    "\n",
    "    host = config['MAIL_IP']\n",
    "    port=  int(config['MAIL_PORT'])\n",
    "    sender=config['MAIL_SENDER']\n",
    "    receivers=[config['MAIL_RECEIVER']]\n",
    "    \n",
    "    \n",
    "    # print(f\"{projectId}-{region}-{dataset_id}-{table_id}\")\n",
    "    # print(f\"{host}-{port}-{sender}-{receivers}\")\n",
    "    print(\"Read all credential config values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=\"Not found .env file or invalid key in .env file\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef4c71-3905-4a5c-9ee0-c520d86f3450",
   "metadata": {},
   "source": [
    "# Setting BigQuery and Check DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e2cddaf3-a25c-449a-9c96-9c8678a61cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "# client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "client = bigquery.Client(project=projectId)\n",
    "\n",
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get data view as data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='vw_ipf'  \n",
      "id                                       vw_ipf\n",
      "first_load_col                         IPF_Date\n",
      "first_load_date                      2020-01-01\n",
      "partition_date_col                     IPF_Date\n",
      "partition_date_type                       MONTH\n",
      "cluster_col_list       IPF_Date,Seller_DeptCode\n",
      "date_col_list                          IPF_Date\n",
      "load_from_type                        dataframe\n",
      "datastore_id                                  2\n",
      "Name: 0, dtype: object\n",
      "Init date to query : 2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "            raise Exception(f\"Not found {data_source_name} in data_source table\")\n",
    "\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "init_date_query=f\"{ds_item['first_load_date']} 00:00:00\"\n",
    "print(ds_item)\n",
    "print(f\"Init date to query : {init_date_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50bb3f-ec4f-4d71-91cf-ed2896cf618a",
   "metadata": {},
   "source": [
    "# Get data store by data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "45fef9c2-b9f0-4c10-bd2a-3823acafe8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_store where id=2  \n",
      "mssql_yit - mssql\n",
      "Read database values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "# get data from data_store\n",
    "def get_data_store(datastore_id):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_store where id={datastore_id}  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        \n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "datastore=get_data_store(ds_item[\"datastore_id\"])\n",
    "db_product=datastore['database_product']\n",
    "print(f\"{datastore['data_store_name']} - {datastore['database_product']}\")\n",
    "\n",
    "try:\n",
    "     \n",
    "    _ip=datastore['database_ip']\n",
    "    _hostname=datastore['database_host']\n",
    "    _port=datastore['database_port']\n",
    "    _servicename=datastore['databases_service_name']\n",
    "    _username=datastore['databases_user']\n",
    "    _password=datastore['databases_password']\n",
    "    #print(f\"{_ip}#{_hostname}#{_port}#{_servicename}#{_username}#{_password}\")\n",
    "\n",
    "    \n",
    "    print(\"Read database values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=f\"Not found data store  {datastore['data_store_name']}\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d5a5a-068d-4c46-9dfd-06d1978e4114",
   "metadata": {},
   "source": [
    "# Load Data Configuration of each data source for BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Load data into BigQuery from dataframe\n",
      "Date Column as condition to load at first = ipf_date\n",
      "Partition : ipf_date - MONTH\n",
      "Cluster List: ['ipf_date', 'seller_deptcode']\n",
      "Date Cols: ['ipf_date']\n",
      "All Reuired Date Cols\n",
      "['ipf_date', 'last_update_date', 'creation_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    loading_from=ds_item['load_from_type']\n",
    "    print(f\"Load data into BigQuery from {loading_from}\")\n",
    "    \n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)\n",
    "    \n",
    "dateColsToConvert=[partitionCol]+dateCols+etlDateCols\n",
    "dateColsToConvert = list(dict.fromkeys(dateColsToConvert))\n",
    "print(\"All Reuired Date Cols\")\n",
    "print(dateColsToConvert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select etl_datetime,data_source_id from etl_transaction where data_source_id='vw_IPF' \n",
      "    order by etl_datetime desc limit 1\n",
      "    \n",
      "          etl_datetime data_source_id\n",
      "0  2023-08-21 18:24:25         vw_IPF\n",
      "Start Import on update_at of last ETL date :  2023-08-21 18:24:25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "\n",
    "if isLoadingAllItems==True:\n",
    " is_load_all=1\n",
    "else:\n",
    " is_load_all=0   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from DataBase  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c5997314-6afb-478c-adf1-6e328cab8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/sql/machine-learning/data-exploration/python-dataframe-pandas?view=sql-server-ver16\n",
    "def loadData_mssql(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+_hostname +';DATABASE='+_servicename+';Port='+str(_port)+';UID='+_username+';PWD='+_password)\n",
    "       cursor = cnxn.cursor()\n",
    "       if isReLoadAll==True:\n",
    "         sql=f\"\"\"select * from {source_name}   where  {colFirstLoad}>='{start_date_query}' \"\"\"    \n",
    "       else:   \n",
    "           sql =f\"\"\"select * from {source_name}  where  {updateCol}>='{start_date_query}' \"\"\"\n",
    "       print(sql)\n",
    "\n",
    "       dfAll = pd.read_sql(sql,  cnxn,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "\n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_oracle(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:   \n",
    "\n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3410466f-833c-48b2-8842-7b2615133b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from vw_IPF  where  last_update_date>='2023-08-01 00:00:00' \n",
      "isReLoadAll==False==0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mis-bi-service\\AppData\\Local\\Temp\\2\\ipykernel_10336\\451849642.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dfAll = pd.read_sql(sql,  cnxn,parse_dates=dateColsToConvert)\n"
     ]
    }
   ],
   "source": [
    "if db_product=='mssql':\n",
    "    dfAll=loadData_mssql(isLoadingAllItems)\n",
    "elif db_product=='oracle':\n",
    "    dfAll=loadData_oracle(isLoadingAllItems)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "# dfAll['last_update_date'].unique()\n",
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "\n",
    "# _dateValueTest='2023-06-13 17:11:54'\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849e7d2-6745-4750-909a-9017ca77377b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d9cc50-90a5-4f10-bbce-c9cfea3d658e",
   "metadata": {},
   "source": [
    "# Transform Dataframe prior to Ingesting it to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['ipf_id', 'ipf_no', 'ipf_date', 'ipf_createby', 'ipf_companycode', 'quotation_id', 'statuscode', 'statuslevel', 'nextstatuscode', 'nextstatuslevel', 'canceldate', 'cancelby', 'isdelete', 'deleteby', 'approvecompletedate', 'seller_adminuser', 'seller_doctypeid', 'seller_org_id', 'seller_companycode', 'seller_deptcode', 'seller_projectname', 'seller_salesrep_id', 'seller_salesrep_no', 'seller_salesrep_name', 'seller_account_code', 'seller_completedeliverydate', 'seller_completedeliveryby', 'seller_projectstatus', 'seller_refprojectno', 'buyer_adminuser', 'buyer_doctypeid', 'buyer_org_id', 'buyer_salesrep_id', 'buyer_salesrep_no', 'buyer_salesrep_name', 'buyer_companycode', 'buyer_deptcode', 'buyer_account_code', 'buyer_purchaseamount', 'buyer_customername', 'buyer_completedeliverydate', 'buyer_completedeliveryby', 'buyer_projectstatus', 'buyer_itemcode', 'buyer_account_code_project_close', 'ipftype', 'maintenance_datestart', 'maintenance_dateend', 'maintenance_day', 'costamount', 'sales_amount', 'currencycode', 'total_quantity', 'total_amount', 'description', 'remark', 'expend_org_id', 'expend_projectno', 'expend_taskno', 'expend_type', 'expend_date', 'expend_refprojectno', 'omorderno', 'ipf_referno', 'ipf_refprojectno', 'createdate', 'createby', 'updatedate', 'updateby', 'refnodept', 'deliverydateplace', 'warranry', 'incidentqty', 'confirmincidentqty', 'original_ipf_id', 'creation_date', 'last_update_date', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82 entries, 0 to 81\n",
      "Data columns (total 78 columns):\n",
      " #   Column                            Non-Null Count  Dtype         \n",
      "---  ------                            --------------  -----         \n",
      " 0   ipf_id                            82 non-null     int64         \n",
      " 1   ipf_no                            82 non-null     object        \n",
      " 2   ipf_date                          82 non-null     datetime64[ns]\n",
      " 3   ipf_createby                      82 non-null     object        \n",
      " 4   ipf_companycode                   82 non-null     object        \n",
      " 5   quotation_id                      51 non-null     float64       \n",
      " 6   statuscode                        82 non-null     object        \n",
      " 7   statuslevel                       82 non-null     int64         \n",
      " 8   nextstatuscode                    82 non-null     object        \n",
      " 9   nextstatuslevel                   82 non-null     int64         \n",
      " 10  canceldate                        8 non-null      datetime64[ns]\n",
      " 11  cancelby                          8 non-null      object        \n",
      " 12  isdelete                          0 non-null      object        \n",
      " 13  deleteby                          0 non-null      object        \n",
      " 14  approvecompletedate               38 non-null     datetime64[ns]\n",
      " 15  seller_adminuser                  82 non-null     object        \n",
      " 16  seller_doctypeid                  82 non-null     int64         \n",
      " 17  seller_org_id                     82 non-null     int64         \n",
      " 18  seller_companycode                82 non-null     object        \n",
      " 19  seller_deptcode                   82 non-null     object        \n",
      " 20  seller_projectname                82 non-null     object        \n",
      " 21  seller_salesrep_id                0 non-null      object        \n",
      " 22  seller_salesrep_no                82 non-null     object        \n",
      " 23  seller_salesrep_name              82 non-null     object        \n",
      " 24  seller_account_code               82 non-null     object        \n",
      " 25  seller_completedeliverydate       10 non-null     datetime64[ns]\n",
      " 26  seller_completedeliveryby         10 non-null     object        \n",
      " 27  seller_projectstatus              10 non-null     object        \n",
      " 28  seller_refprojectno               82 non-null     object        \n",
      " 29  buyer_adminuser                   80 non-null     object        \n",
      " 30  buyer_doctypeid                   80 non-null     float64       \n",
      " 31  buyer_org_id                      82 non-null     int64         \n",
      " 32  buyer_salesrep_id                 0 non-null      object        \n",
      " 33  buyer_salesrep_no                 82 non-null     object        \n",
      " 34  buyer_salesrep_name               82 non-null     object        \n",
      " 35  buyer_companycode                 82 non-null     object        \n",
      " 36  buyer_deptcode                    82 non-null     object        \n",
      " 37  buyer_account_code                82 non-null     object        \n",
      " 38  buyer_purchaseamount              82 non-null     float64       \n",
      " 39  buyer_customername                13 non-null     object        \n",
      " 40  buyer_completedeliverydate        4 non-null      datetime64[ns]\n",
      " 41  buyer_completedeliveryby          4 non-null      object        \n",
      " 42  buyer_projectstatus               4 non-null      object        \n",
      " 43  buyer_itemcode                    46 non-null     object        \n",
      " 44  buyer_account_code_project_close  0 non-null      object        \n",
      " 45  ipftype                           82 non-null     object        \n",
      " 46  maintenance_datestart             82 non-null     datetime64[ns]\n",
      " 47  maintenance_dateend               82 non-null     datetime64[ns]\n",
      " 48  maintenance_day                   82 non-null     int64         \n",
      " 49  costamount                        82 non-null     float64       \n",
      " 50  sales_amount                      82 non-null     float64       \n",
      " 51  currencycode                      82 non-null     object        \n",
      " 52  total_quantity                    51 non-null     float64       \n",
      " 53  total_amount                      51 non-null     float64       \n",
      " 54  description                       82 non-null     object        \n",
      " 55  remark                            8 non-null      object        \n",
      " 56  expend_org_id                     37 non-null     float64       \n",
      " 57  expend_projectno                  37 non-null     object        \n",
      " 58  expend_taskno                     37 non-null     object        \n",
      " 59  expend_type                       37 non-null     object        \n",
      " 60  expend_date                       37 non-null     object        \n",
      " 61  expend_refprojectno               81 non-null     object        \n",
      " 62  omorderno                         0 non-null      object        \n",
      " 63  ipf_referno                       7 non-null      object        \n",
      " 64  ipf_refprojectno                  27 non-null     object        \n",
      " 65  createdate                        82 non-null     datetime64[ns]\n",
      " 66  createby                          82 non-null     object        \n",
      " 67  updatedate                        82 non-null     datetime64[ns]\n",
      " 68  updateby                          82 non-null     object        \n",
      " 69  refnodept                         78 non-null     object        \n",
      " 70  deliverydateplace                 2 non-null      object        \n",
      " 71  warranry                          2 non-null      object        \n",
      " 72  incidentqty                       0 non-null      object        \n",
      " 73  confirmincidentqty                0 non-null      object        \n",
      " 74  original_ipf_id                   5 non-null      float64       \n",
      " 75  creation_date                     82 non-null     datetime64[ns]\n",
      " 76  last_update_date                  82 non-null     datetime64[ns]\n",
      " 77  ImportedAt                        82 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](12), float64(9), int64(7), object(50)\n",
      "memory usage: 50.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipf_id</th>\n",
       "      <th>ipf_no</th>\n",
       "      <th>ipf_date</th>\n",
       "      <th>ipf_createby</th>\n",
       "      <th>ipf_companycode</th>\n",
       "      <th>quotation_id</th>\n",
       "      <th>statuscode</th>\n",
       "      <th>statuslevel</th>\n",
       "      <th>nextstatuscode</th>\n",
       "      <th>nextstatuslevel</th>\n",
       "      <th>...</th>\n",
       "      <th>updateby</th>\n",
       "      <th>refnodept</th>\n",
       "      <th>deliverydateplace</th>\n",
       "      <th>warranry</th>\n",
       "      <th>incidentqty</th>\n",
       "      <th>confirmincidentqty</th>\n",
       "      <th>original_ipf_id</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>945</td>\n",
       "      <td>0402-0303-650001</td>\n",
       "      <td>2023-08-02</td>\n",
       "      <td>10002800</td>\n",
       "      <td>11</td>\n",
       "      <td>641.0</td>\n",
       "      <td>CREATE</td>\n",
       "      <td>10</td>\n",
       "      <td>SUBMIT</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>10002800</td>\n",
       "      <td>SBQ65-040</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-05-19 11:24:08.780</td>\n",
       "      <td>2023-08-02 16:28:29.570</td>\n",
       "      <td>2023-08-21 18:24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1333</td>\n",
       "      <td>0311-0302-650046</td>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>1002267</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POSTING</td>\n",
       "      <td>80</td>\n",
       "      <td>CONFIRM</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>1001423</td>\n",
       "      <td>NCQ/65-S092</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-09-23 14:35:33.450</td>\n",
       "      <td>2023-08-11 18:27:56.437</td>\n",
       "      <td>2023-08-21 18:24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792</td>\n",
       "      <td>0311-0301-660002</td>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>1002267</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POSTING</td>\n",
       "      <td>80</td>\n",
       "      <td>CONFIRM</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>1002517</td>\n",
       "      <td>NCQ/66-IN002</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-01-11 15:59:02.127</td>\n",
       "      <td>2023-08-09 17:17:21.023</td>\n",
       "      <td>2023-08-21 18:24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1056</td>\n",
       "      <td>0307-0301-650004</td>\n",
       "      <td>2023-08-04</td>\n",
       "      <td>1002267</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CONFIRM</td>\n",
       "      <td>90</td>\n",
       "      <td>CONFIRM</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>1001423</td>\n",
       "      <td>IOQ/65-125</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-06-22 15:49:40.730</td>\n",
       "      <td>2023-08-04 17:19:42.077</td>\n",
       "      <td>2023-08-21 18:24:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1911</td>\n",
       "      <td>0311-0302-660014</td>\n",
       "      <td>2023-08-03</td>\n",
       "      <td>1002267</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POSTING</td>\n",
       "      <td>80</td>\n",
       "      <td>CONFIRM</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>1001423</td>\n",
       "      <td>NCQ66-IN021</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-01 10:49:21.057</td>\n",
       "      <td>2023-08-03 16:29:17.663</td>\n",
       "      <td>2023-08-21 18:24:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ipf_id            ipf_no   ipf_date ipf_createby ipf_companycode   \n",
       "0     945  0402-0303-650001 2023-08-02     10002800              11  \\\n",
       "1    1333  0311-0302-650046 2023-08-11      1002267              11   \n",
       "2    1792  0311-0301-660002 2023-08-09      1002267              11   \n",
       "3    1056  0307-0301-650004 2023-08-04      1002267              11   \n",
       "4    1911  0311-0302-660014 2023-08-03      1002267              11   \n",
       "\n",
       "   quotation_id statuscode  statuslevel nextstatuscode  nextstatuslevel  ...   \n",
       "0         641.0     CREATE           10         SUBMIT               20  ...  \\\n",
       "1           NaN    POSTING           80        CONFIRM               70  ...   \n",
       "2           NaN    POSTING           80        CONFIRM               70  ...   \n",
       "3           NaN    CONFIRM           90        CONFIRM               70  ...   \n",
       "4           NaN    POSTING           80        CONFIRM               70  ...   \n",
       "\n",
       "   updateby     refnodept deliverydateplace warranry incidentqty   \n",
       "0  10002800     SBQ65-040              None     None        None  \\\n",
       "1   1001423   NCQ/65-S092              None     None        None   \n",
       "2   1002517  NCQ/66-IN002              None     None        None   \n",
       "3   1001423    IOQ/65-125              None     None        None   \n",
       "4   1001423   NCQ66-IN021              None     None        None   \n",
       "\n",
       "  confirmincidentqty  original_ipf_id           creation_date   \n",
       "0               None              NaN 2022-05-19 11:24:08.780  \\\n",
       "1               None              NaN 2022-09-23 14:35:33.450   \n",
       "2               None              NaN 2023-01-11 15:59:02.127   \n",
       "3               None              NaN 2022-06-22 15:49:40.730   \n",
       "4               None              NaN 2023-03-01 10:49:21.057   \n",
       "\n",
       "         last_update_date          ImportedAt  \n",
       "0 2023-08-02 16:28:29.570 2023-08-21 18:24:25  \n",
       "1 2023-08-11 18:27:56.437 2023-08-21 18:24:25  \n",
       "2 2023-08-09 17:17:21.023 2023-08-21 18:24:25  \n",
       "3 2023-08-04 17:19:42.077 2023-08-21 18:24:25  \n",
       "4 2023-08-03 16:29:17.663 2023-08-21 18:24:25  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.columns=[ col.lower() for col in  dfAll.columns   ]  \n",
    "\n",
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "4e7a3e0f-e334-4358-ae97-e44c1185b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ipf_date', 'seller_deptcode'] is in dataframe from mssql View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "# checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in [ x.lower() for x in  listColDF] ]\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from {db_product} View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from {db_product} View\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "# BigQuery Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBQSchemaByDF():\n",
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "    schema = []\n",
    "    srCols=dfAll.dtypes\n",
    "    try:\n",
    "        for name, type_name in srCols.items():\n",
    "            # print(name,type_name)\n",
    "            if str(type_name) in ['int32','int64']:\n",
    "              schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='float64':\n",
    "              schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='datetime64[ns]':\n",
    "              if name in   dateCols:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "              else:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) == 'bool':\n",
    "                 schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "            else: # if not found type , it will be converted to STRING\n",
    "               schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "\n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    schemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in schema])\n",
    "    \n",
    "    return  schema,schemaDictNameType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pbpython.com/pandas_dtypes.html\n",
    "      #https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html  \n",
    "      #https://datatofish.com/strings-to-datetime-pandas/ \n",
    "def convert_dfSchema_same_bqSChema(bqName,bqType): \n",
    "  try:  \n",
    "    if bqType in [\"INTEGER\",\"FLOAT\"]:\n",
    "        try:\n",
    "            if  bqType==\"INTEGER\":\n",
    "                dfAll[bqName]=dfAll[bqName].astype('int')\n",
    "            else:  \n",
    "                dfAll[bqName]=dfAll[bqName].astype('float')\n",
    "        except Exception as e:\n",
    "            print(f\"Extra : {bqName}-{bqType} has been converted by pd.to_numeric\")\n",
    "            dfAll[bqName]=pd.to_numeric(dfAll[bqName], errors='coerce')\n",
    "    elif bqType in [\"DATE\",\"DATETIME\"]:\n",
    "      dfAll[bqName]=pd.to_datetime(dfAll[bqName], errors='coerce',exact=False)   \n",
    "    elif bqType==\"BOOL\":\n",
    "      dfAll[bqName]=dfAll[bqName].astype('bool')\n",
    "    else:\n",
    "      dfAll[bqName]=dfAll[bqName].apply(lambda x: str(x))  \n",
    "    \n",
    "    print(f\"{bqName} has been converted  to {bqType} \")\n",
    "    \n",
    "  except Exception as ex:\n",
    "    raise ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(schema):\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of {db_product}\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"1#All Fields on BQ and DF are ok.\")  \n",
    "        print(\"==================================\")\n",
    "        \n",
    "    try:\n",
    "        __,SchemaDictNameType=createBQSchemaByDF()\n",
    "\n",
    "        for key_name, val_filed_type in ExistingSchemaDictNameType.items():\n",
    "            val2=SchemaDictNameType[key_name] \n",
    "            if  val_filed_type!=val2:\n",
    "              msg=f\"{key_name}-{val_filed_type} on existing schema is not them same as {key_name}-{val2} on loading schema.\"\n",
    "              print(msg)\n",
    "              convert_dfSchema_same_bqSChema( key_name,val_filed_type)\n",
    "        print(\"2#All Field Type on BQ and DF are the same.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        msg=f'{key_name} name on the loading schema does not exists  in the existing schema on Bigquery, so the system is unable to check field type matching.'\n",
    "        print(msg)\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "       \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"3#Partition Name Fields on BQ and DF is ok.\")    \n",
    "        print(\"==================================\" )    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"4#All Cluster on BQ and DF are ok.\")\n",
    "        print(\"==================================\")      \n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"5#All Date Column on BQ and DF are ok.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "# delete table and set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mismgntdata-bigquery.MIS_BI_DW.vw_IPF already exists.\n",
      "All 78 Fields as belows\n",
      "{'ipf_id': 'INTEGER', 'ipf_no': 'STRING', 'ipf_date': 'DATE', 'ipf_createby': 'STRING', 'ipf_companycode': 'STRING', 'quotation_id': 'FLOAT', 'statuscode': 'STRING', 'statuslevel': 'INTEGER', 'nextstatuscode': 'STRING', 'nextstatuslevel': 'INTEGER', 'canceldate': 'DATETIME', 'cancelby': 'STRING', 'isdelete': 'STRING', 'deleteby': 'STRING', 'approvecompletedate': 'DATETIME', 'seller_adminuser': 'STRING', 'seller_doctypeid': 'INTEGER', 'seller_org_id': 'INTEGER', 'seller_companycode': 'STRING', 'seller_deptcode': 'STRING', 'seller_projectname': 'STRING', 'seller_salesrep_id': 'STRING', 'seller_salesrep_no': 'STRING', 'seller_salesrep_name': 'STRING', 'seller_account_code': 'STRING', 'seller_completedeliverydate': 'DATETIME', 'seller_completedeliveryby': 'STRING', 'seller_projectstatus': 'STRING', 'seller_refprojectno': 'STRING', 'buyer_adminuser': 'STRING', 'buyer_doctypeid': 'FLOAT', 'buyer_org_id': 'INTEGER', 'buyer_salesrep_id': 'STRING', 'buyer_salesrep_no': 'STRING', 'buyer_salesrep_name': 'STRING', 'buyer_companycode': 'STRING', 'buyer_deptcode': 'STRING', 'buyer_account_code': 'STRING', 'buyer_purchaseamount': 'FLOAT', 'buyer_customername': 'STRING', 'buyer_completedeliverydate': 'DATETIME', 'buyer_completedeliveryby': 'STRING', 'buyer_projectstatus': 'STRING', 'buyer_itemcode': 'STRING', 'buyer_account_code_project_close': 'STRING', 'ipftype': 'STRING', 'maintenance_datestart': 'DATETIME', 'maintenance_dateend': 'DATETIME', 'maintenance_day': 'INTEGER', 'costamount': 'FLOAT', 'sales_amount': 'FLOAT', 'currencycode': 'STRING', 'total_quantity': 'FLOAT', 'total_amount': 'FLOAT', 'description': 'STRING', 'remark': 'STRING', 'expend_org_id': 'FLOAT', 'expend_projectno': 'STRING', 'expend_taskno': 'STRING', 'expend_type': 'STRING', 'expend_date': 'STRING', 'expend_refprojectno': 'STRING', 'omorderno': 'FLOAT', 'ipf_referno': 'STRING', 'ipf_refprojectno': 'STRING', 'createdate': 'DATETIME', 'createby': 'STRING', 'updatedate': 'DATETIME', 'updateby': 'STRING', 'refnodept': 'STRING', 'deliverydateplace': 'STRING', 'warranry': 'STRING', 'incidentqty': 'FLOAT', 'confirmincidentqty': 'STRING', 'original_ipf_id': 'FLOAT', 'creation_date': 'DATETIME', 'last_update_date': 'DATETIME', 'ImportedAt': 'DATETIME'}\n",
      "Partiton Field&Type: ipf_date - {'ipf_id': 'INTEGER', 'ipf_no': 'STRING', 'ipf_date': 'DATE', 'ipf_createby': 'STRING', 'ipf_companycode': 'STRING', 'quotation_id': 'FLOAT', 'statuscode': 'STRING', 'statuslevel': 'INTEGER', 'nextstatuscode': 'STRING', 'nextstatuslevel': 'INTEGER', 'canceldate': 'DATETIME', 'cancelby': 'STRING', 'isdelete': 'STRING', 'deleteby': 'STRING', 'approvecompletedate': 'DATETIME', 'seller_adminuser': 'STRING', 'seller_doctypeid': 'INTEGER', 'seller_org_id': 'INTEGER', 'seller_companycode': 'STRING', 'seller_deptcode': 'STRING', 'seller_projectname': 'STRING', 'seller_salesrep_id': 'STRING', 'seller_salesrep_no': 'STRING', 'seller_salesrep_name': 'STRING', 'seller_account_code': 'STRING', 'seller_completedeliverydate': 'DATETIME', 'seller_completedeliveryby': 'STRING', 'seller_projectstatus': 'STRING', 'seller_refprojectno': 'STRING', 'buyer_adminuser': 'STRING', 'buyer_doctypeid': 'FLOAT', 'buyer_org_id': 'INTEGER', 'buyer_salesrep_id': 'STRING', 'buyer_salesrep_no': 'STRING', 'buyer_salesrep_name': 'STRING', 'buyer_companycode': 'STRING', 'buyer_deptcode': 'STRING', 'buyer_account_code': 'STRING', 'buyer_purchaseamount': 'FLOAT', 'buyer_customername': 'STRING', 'buyer_completedeliverydate': 'DATETIME', 'buyer_completedeliveryby': 'STRING', 'buyer_projectstatus': 'STRING', 'buyer_itemcode': 'STRING', 'buyer_account_code_project_close': 'STRING', 'ipftype': 'STRING', 'maintenance_datestart': 'DATETIME', 'maintenance_dateend': 'DATETIME', 'maintenance_day': 'INTEGER', 'costamount': 'FLOAT', 'sales_amount': 'FLOAT', 'currencycode': 'STRING', 'total_quantity': 'FLOAT', 'total_amount': 'FLOAT', 'description': 'STRING', 'remark': 'STRING', 'expend_org_id': 'FLOAT', 'expend_projectno': 'STRING', 'expend_taskno': 'STRING', 'expend_type': 'STRING', 'expend_date': 'STRING', 'expend_refprojectno': 'STRING', 'omorderno': 'FLOAT', 'ipf_referno': 'STRING', 'ipf_refprojectno': 'STRING', 'createdate': 'DATETIME', 'createby': 'STRING', 'updatedate': 'DATETIME', 'updateby': 'STRING', 'refnodept': 'STRING', 'deliverydateplace': 'STRING', 'warranry': 'STRING', 'incidentqty': 'FLOAT', 'confirmincidentqty': 'STRING', 'original_ipf_id': 'FLOAT', 'creation_date': 'DATETIME', 'last_update_date': 'DATETIME', 'ImportedAt': 'DATETIME'}\n",
      "Cluster Field List: ['ipf_date', 'seller_deptcode']\n",
      "Date Field List: ['ipf_date']\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "1#All Fields on BQ and DF are ok.\n",
      "==================================\n",
      "omorderno-FLOAT on existing schema is not them same as omorderno-STRING on loading schema.\n",
      "omorderno has been converted  to FLOAT \n",
      "incidentqty-FLOAT on existing schema is not them same as incidentqty-STRING on loading schema.\n",
      "incidentqty has been converted  to FLOAT \n",
      "2#All Field Type on BQ and DF are the same.\n",
      "==================================\n",
      "3#Partition Name Fields on BQ and DF is ok.\n",
      "==================================\n",
      "4#All Cluster on BQ and DF are ok.\n",
      "==================================\n",
      "5#All Date Column on BQ and DF are ok.\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "isExistingTable=get_table()\n",
    "if isExistingTable:    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    bqSchema=table.schema\n",
    "    ExistingSchemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in bqSchema])\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All {len(ExistingSchemaDictNameType)} Fields as belows\")\n",
    "    print(ExistingSchemaDictNameType)\n",
    "    \n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {ExistingSchemaDictNameType}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    bqSchema,_=createBQSchemaByDF()\n",
    "    create_table(bqSchema)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 rows and 78 columns are about to be imported to BQ by dataframe\n",
      "ipf_id                         int64\n",
      "ipf_no                        object\n",
      "ipf_date              datetime64[ns]\n",
      "ipf_createby                  object\n",
      "ipf_companycode               object\n",
      "                           ...      \n",
      "confirmincidentqty            object\n",
      "original_ipf_id              float64\n",
      "creation_date         datetime64[ns]\n",
      "last_update_date      datetime64[ns]\n",
      "ImportedAt            datetime64[ns]\n",
      "Length: 78, dtype: object\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    no_rows=len(dfAll)\n",
    "    no_cols=len(dfAll.columns)\n",
    "   \n",
    "    if dfAll.empty==False:\n",
    "     if loading_from=='csv':\n",
    "        \n",
    "        dfAll.to_csv (temp_path,index=False)\n",
    "    print(f\"{no_rows} rows and {no_cols} columns are about to be imported to BQ by {loading_from}\")\n",
    "    print(dfAll.dtypes)\n",
    "    print(\"***********************************************************************************\")    \n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])  \n",
    "  logErrorMessage(listError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from dataframe\n",
      "Import data from vw_IPF on mssql into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:  \n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(listError)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=bqSchema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=bqSchema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config,)  \n",
    "\n",
    "\n",
    "    result_job=job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on {db_product} into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  msg=f\"BigQuery Error While Ingesting data with {str(e)}\"  \n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,is_load_all,completely)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if len(listError)>0:\n",
    " is_loaded_completely=0\n",
    "else:\n",
    " is_loaded_completely=1\n",
    "\n",
    "#dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'is_load_all':is_load_all,'completely':is_loaded_completely}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984432-453f-4084-ba4b-8e502237dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26cc1-01f1-40ba-8c8e-de4bc7354caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e1e63-dff7-4566-b2d1-7979d0a8004b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
