{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "# import pyodbc # ms sql server\n",
    "# import cx_Oracle # oracle\n",
    "import psycopg2 # postgres\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e3ff4-4611-44a2-8f68-bdf4f6e08265",
   "metadata": {},
   "source": [
    "# Input Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2529a92-9fb9-45ef-8ebf-117d868a950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=False  # True (fullload) \n",
    "option_write_to_bq=1 # 1=append (periodict job) 2=truncate (fullload) \n",
    "is_py=False\n",
    "\n",
    "source_name='view_demo_incident_inventory'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192bc4-6924-4e88-9fc1-981c816f918c",
   "metadata": {},
   "source": [
    "# Time To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53bf1c7f-c8b9-4017-9b06-629aa83d885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-30 19:39:22\n",
      "2024-01-30 19:39:22\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# dtStr_imported='2023-06-13 17:11:54'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Constant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_date_query='2020-01-01 00:00:00'\n",
    "# init_date_query='2023-01-01 00:00:00'\n",
    "\n",
    "data_base_file=r'D:\\PythonDev\\MyQuantFinProject\\MIS-FinData\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "json_credential_file=r'C:\\Windows\\pongthorn-5decdc5124f5.json'\n",
    "env_path=r'D:\\PythonDev\\MyQuantFinProject\\MIS-FinData\\.env'\n",
    "\n",
    "csv_temp_folder='csv_temp'\n",
    "csv_error_folder='csv_error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25b842-861c-4543-b7bf-abe21d688752",
   "metadata": {},
   "source": [
    "# Enter parameter on script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"For Loading Data Mode ,allow you to enter either 1=Full Load | 0=Incremental Load\")  \n",
    "            \n",
    "        option_write_to_bq= int(sys.argv[3])\n",
    "        if  option_write_to_bq not in [1,2]:\n",
    "         raise Exception(\"For Write to BQ Mode ,Allow you to enter either 1=Append or 2=Truncate\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        \n",
    "        load_option= int(input(\"Loading Data Mode option (1=Full Load | 0=Incremental Load): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"For Loading Data Mode ,allow you to enter either 1=Full Load | 0=Incremental Load\")  \n",
    "            \n",
    "\n",
    "        option_write_to_bq= int(input(\"Write to BQ  option (1=Append | 2=Truncate): \"))\n",
    "        if  option_write_to_bq not in [1,2]:\n",
    "         raise Exception(\"For Write to BQ Mode ,Allow you to enter either 1=Append or 2=Truncate\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems} and Option writing to BigQuery={option_write_to_bq}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action to do\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df750a-9032-42b3-8955-e6ab06b89ef1",
   "metadata": {},
   "source": [
    "# Summary What sytem is about to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4b405cba-e6fd-4aa6-bf22-cca63f2bc35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load view = view_demo_incident_inventory and Load All Data= False and Option writing toBQ=1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Load view = {source_name} and Load All Data= {isLoadingAllItems} and Option writing toBQ={option_write_to_bq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f80fe-b80e-42fb-98cb-cff290f5c031",
   "metadata": {},
   "source": [
    "# Check temp and error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21674004-5a60-4ce3-bead-dfc1f8d5457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_temp_folder)==False:\n",
    "  os.mkdir(csv_temp_folder)\n",
    "if os.path.exists(csv_error_folder)==False:\n",
    "  os.mkdir(csv_error_folder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Const Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dab69b3d-b0bb-4e97-a8a2-cffdbdcd01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "temp_path=f'{csv_temp_folder}/{source_name}.csv'\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'\n",
    "etlDateCols=[updateCol,'creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorSubject,errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] =errorSubject \n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34cb43ec-1209-4920-921f-bda3d5fbf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_error_file(): # if any error ,then move csv to investigte later\n",
    "    error_csv_path=''\n",
    "    try:\n",
    "        if os.path.exists(temp_path):\n",
    "         error_csv_file=f\"{source_name}_error_{ dt_imported.strftime('%d%m%y_%H%M%S')}.csv\"   \n",
    "         new_temp_path=f'{csv_temp_folder}/{error_csv_file}'\n",
    "         os.rename(temp_path,new_temp_path)  \n",
    "\n",
    "         error_csv_path=f'{csv_error_folder}/{error_csv_file}'\n",
    "         shutil.move(new_temp_path,error_csv_path )\n",
    "         \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "    return   error_csv_path      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        \n",
    "        error_message=f\"ETL Error on {source_name} at {dtStr_imported}\"\n",
    "        # move csv error file to examine later\n",
    "        error_csv_path  =move_error_file()\n",
    "        if error_csv_path!='':\n",
    "            error_message= f\"{error_csv_path} - {error_message}\"\n",
    "        error_message=f\"MIS-BI : {error_message}\"   \n",
    "        \n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        # add to error table into sqlite\n",
    "        recordError=dfError.to_records(index=False) \n",
    "        add_logError(recordError)\n",
    "        \n",
    "        # sened main\n",
    "        #emailResult=sendMailForError(error_message, dfError.to_html(index=False))\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f267b-2d60-462c-8f44-f22834dfa5b2",
   "metadata": {},
   "source": [
    "# Read Credential Config from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0af655e-f711-4b9e-af7b-a854ea13addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pongthorn - asia-southeast1 - SMartDW - pongthorn.SMartDW.view_demo_incident_inventory\n",
      "Read all credential config values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = dotenv_values(dotenv_path=env_path)\n",
    "    projectId=config['PROJECT_ID']\n",
    "    region=config['REGION']\n",
    "    dataset_id=config['DATASET_ID']\n",
    "    table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "    \n",
    "\n",
    "    host = config['MAIL_IP']\n",
    "    port=  int(config['MAIL_PORT'])\n",
    "    sender=config['MAIL_SENDER']\n",
    "    receivers=[config['MAIL_RECEIVER']]\n",
    "    \n",
    "    \n",
    "    print(f\"{projectId} - {region} - {dataset_id} - {table_id}\")\n",
    "    # print(f\"{host}-{port}-{sender}-{receivers}\")\n",
    "    print(\"Read all credential config values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=\"Not found .env file or invalid key in .env file\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef4c71-3905-4a5c-9ee0-c520d86f3450",
   "metadata": {},
   "source": [
    "# Setting BigQuery and Check the follwing\n",
    "* check data set\n",
    "* check data table , it is not allowed to do full loading  if table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e2cddaf3-a25c-449a-9c96-9c8678a61cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load to BQ by : WRITE_APPEND\n",
      "Dataset SMartDW already exists\n"
     ]
    }
   ],
   "source": [
    "# credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "# client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "client = bigquery.Client(project=projectId)\n",
    "print()\n",
    "\n",
    "write_to_bq_mode=\"WRITE_APPEND\" \n",
    "if option_write_to_bq==2:\n",
    " write_to_bq_mode=\"WRITE_TRUNCATE\"\n",
    "\n",
    "print(f\"Load to BQ by : {write_to_bq_mode}\")\n",
    "\n",
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ba48050-1a10-4653-baa0-7a112fa28132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table for full loading\n",
    "def get_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False\n",
    "     \n",
    "if isLoadingAllItems==True:\n",
    "    if get_table()==True:\n",
    "        msg=f\"Found {table_id}, isReLoadAll={isLoadingAllItems}, please delete table  prior to performing full load.\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get data view as data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='view_demo_incident_inventory'  \n",
      "id                     view_demo_incident_inventory\n",
      "first_load_col                    incident_datetime\n",
      "first_load_date                          2023-01-01\n",
      "partition_date_col                incident_datetime\n",
      "partition_date_type                           MONTH\n",
      "cluster_col_list                        brand,model\n",
      "date_col_list                                      \n",
      "load_from_type                            dataframe\n",
      "datastore_id                                      3\n",
      "Name: 0, dtype: object\n",
      "Init date to query : 2023-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "            raise Exception(f\"Not found {data_source_name} in data_source table\")\n",
    "\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "init_date_query=f\"{ds_item['first_load_date']} 00:00:00\"\n",
    "print(ds_item)\n",
    "print(f\"Init date to query : {init_date_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50bb3f-ec4f-4d71-91cf-ed2896cf618a",
   "metadata": {},
   "source": [
    "# Get data store by data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45fef9c2-b9f0-4c10-bd2a-3823acafe8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_store where id=3  \n",
      "postgresql_smartapp - postgresql\n",
      "Read database values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "# get data from data_store\n",
    "def get_data_store(datastore_id):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_store where id={datastore_id}  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        \n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "datastore=get_data_store(ds_item[\"datastore_id\"])\n",
    "db_product=datastore['database_product']\n",
    "print(f\"{datastore['data_store_name']} - {datastore['database_product']}\")\n",
    "\n",
    "try:\n",
    "     \n",
    "    _ip=datastore['database_ip']\n",
    "    _hostname=datastore['database_host']\n",
    "    _port=datastore['database_port']\n",
    "    _servicename=datastore['databases_service_name']\n",
    "    _username=datastore['databases_user']\n",
    "    _password=datastore['databases_password']\n",
    "    #print(f\"{_ip}#{_hostname}#{_port}#{_servicename}#{_username}#{_password}\")\n",
    "\n",
    "    \n",
    "    print(\"Read database values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=f\"Not found data store  {datastore['data_store_name']}\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d5a5a-068d-4c46-9dfd-06d1978e4114",
   "metadata": {},
   "source": [
    "# Load Data Configuration of each data source for BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Load data into BigQuery from dataframe\n",
      "Date Column as condition to load at first = incident_datetime\n",
      "Partition : incident_datetime - MONTH\n",
      "Cluster List: ['brand', 'model']\n",
      "[] (No Date cols)\n",
      "All Reuired Date Cols\n",
      "['incident_datetime', 'last_update_date', 'creation_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    loading_from=ds_item['load_from_type']\n",
    "    print(f\"Load data into BigQuery from {loading_from}\")\n",
    "    \n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)\n",
    "    \n",
    "dateColsToConvert=[partitionCol]+dateCols+etlDateCols\n",
    "dateColsToConvert = list(dict.fromkeys(dateColsToConvert))\n",
    "print(\"All Reuired Date Cols\")\n",
    "print(dateColsToConvert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load-Option==False and (Write-BQ Option=1\n",
      "select etl_datetime,data_source_id from etl_transaction where data_source_id='view_demo_incident_inventory' \n",
      "    order by etl_datetime desc limit 1\n",
      "    \n",
      "          etl_datetime                data_source_id\n",
      "0  2024-01-30 19:20:24  view_demo_incident_inventory\n",
      "Start Import on update_at of last ETL date :  2024-01-30 19:20:24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "print(f\"Load-Option=={isLoadingAllItems} and (Write-BQ Option={option_write_to_bq}\")    \n",
    "if (isLoadingAllItems==False) and (option_write_to_bq==1):\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "\n",
    "if isLoadingAllItems==True:\n",
    " is_load_all=1\n",
    "else:\n",
    " is_load_all=0   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from DataBase  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acee2552-4a9d-4f0d-be06-05200dc320d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadData_postgresql(isReLoadAll):\n",
    "    conn = psycopg2.connect(\n",
    "        database=_servicename, user=_username,\n",
    "        password=_password, host=_hostname,port=_port\n",
    "     )\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        if (isReLoadAll==True) or (option_write_to_bq==2):   \n",
    "             sql=f\"\"\"select * from {source_name}   where  {colFirstLoad}>='{start_date_query}' \"\"\"    \n",
    "        else:   \n",
    "               sql =f\"\"\"select * from {source_name}  where  {updateCol}>='{start_date_query}' \"\"\"\n",
    "        print(sql)\n",
    "    \n",
    "        cursor.execute(sql)\n",
    "        columns = [col[0] for col in cursor.description]\n",
    "        dataList = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "        df = pd.DataFrame(data=dataList) \n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c5997314-6afb-478c-adf1-6e328cab8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/sql/machine-learning/data-exploration/python-dataframe-pandas?view=sql-server-ver16\n",
    "def loadData_mssql(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       cnxn = pyodbc.connect('DRIVER={SQL Server};SERVER='+_hostname +';DATABASE='+_servicename+';Port='+str(_port)+';UID='+_username+';PWD='+_password)\n",
    "       cursor = cnxn.cursor()\n",
    "       # laod all Or trucate both are similar but load all is used if schema change \n",
    "       if (isReLoadAll==True) or (option_write_to_bq==2):   \n",
    "         sql=f\"\"\"select * from {source_name}   where  {colFirstLoad}>='{start_date_query}' \"\"\"    \n",
    "       else:   \n",
    "           sql =f\"\"\"select * from {source_name}  where  {updateCol}>='{start_date_query}' \"\"\"\n",
    "       print(sql)\n",
    "\n",
    "       dfAll = pd.read_sql(sql,  cnxn,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "\n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData_oracle(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       # laod all Or trucate both are similar but load all is used if schema change \n",
    "       if (isReLoadAll==True) or (option_write_to_bq==2): \n",
    "\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:   \n",
    "\n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3410466f-833c-48b2-8842-7b2615133b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from view_demo_incident_inventory  where  last_update_date>='2024-01-30 19:20:24' \n",
      "(2, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   id                 2 non-null      int64         \n",
      " 1   incident_datetime  2 non-null      datetime64[ns]\n",
      " 2   close_datetime     1 non-null      object        \n",
      " 3   incident_type      2 non-null      object        \n",
      " 4   severity           2 non-null      object        \n",
      " 5   brand              2 non-null      object        \n",
      " 6   model              2 non-null      object        \n",
      " 7   last_update_date   2 non-null      datetime64[ns]\n",
      " 8   creation_date      2 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](3), int64(1), object(5)\n",
      "memory usage: 272.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>incident_datetime</th>\n",
       "      <th>close_datetime</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>severity</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>creation_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4440</td>\n",
       "      <td>2024-01-30 19:37:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>Minor</td>\n",
       "      <td>YIP</td>\n",
       "      <td>Juniper SRX340JB</td>\n",
       "      <td>2024-01-30 19:39:03</td>\n",
       "      <td>2024-01-30 19:39:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4439</td>\n",
       "      <td>2024-01-29 11:02:00</td>\n",
       "      <td>2024-01-31 00:00:00</td>\n",
       "      <td>Software</td>\n",
       "      <td>Major</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>POWER BI</td>\n",
       "      <td>2024-01-30 19:37:00</td>\n",
       "      <td>2024-01-30 19:39:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   incident_datetime       close_datetime     incident_type severity  \\\n",
       "0  4440 2024-01-30 19:37:00                 None  General Incident    Minor   \n",
       "1  4439 2024-01-29 11:02:00  2024-01-31 00:00:00          Software    Major   \n",
       "\n",
       "       brand             model    last_update_date       creation_date  \n",
       "0        YIP  Juniper SRX340JB 2024-01-30 19:39:03 2024-01-30 19:39:59  \n",
       "1  Microsoft          POWER BI 2024-01-30 19:37:00 2024-01-30 19:39:59  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if db_product=='mssql':\n",
    "    dfAll=loadData_mssql(isLoadingAllItems)\n",
    "elif db_product=='oracle':\n",
    "    dfAll=loadData_oracle(isLoadingAllItems)\n",
    "elif db_product=='postgresql':\n",
    "    dfAll=loadData_postgresql(isLoadingAllItems)\n",
    "\n",
    "print(dfAll.shape) \n",
    "dfAll.info()\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ccd349f-cd08-4099-bb7d-0251912abe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rows are about  to import to BQ\n"
     ]
    }
   ],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"{dfAll.shape[0]} rows are about  to import to BQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "# dfAll['last_update_date'].unique()\n",
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "\n",
    "# _dateValueTest='2023-06-13 17:11:54'\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849e7d2-6745-4750-909a-9017ca77377b",
   "metadata": {},
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"{dfAll.shape[0]} rows are about  to import to BQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9cc50-90a5-4f10-bbce-c9cfea3d658e",
   "metadata": {},
   "source": [
    "# Transform Dataframe prior to Ingesting it to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['id', 'incident_datetime', 'close_datetime', 'incident_type', 'severity', 'brand', 'model', 'last_update_date', 'creation_date', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   id                 2 non-null      int64         \n",
      " 1   incident_datetime  2 non-null      datetime64[ns]\n",
      " 2   close_datetime     1 non-null      object        \n",
      " 3   incident_type      2 non-null      object        \n",
      " 4   severity           2 non-null      object        \n",
      " 5   brand              2 non-null      object        \n",
      " 6   model              2 non-null      object        \n",
      " 7   last_update_date   2 non-null      datetime64[ns]\n",
      " 8   creation_date      2 non-null      datetime64[ns]\n",
      " 9   ImportedAt         2 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](4), int64(1), object(5)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>incident_datetime</th>\n",
       "      <th>close_datetime</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>severity</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4440</td>\n",
       "      <td>2024-01-30 19:37:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>Minor</td>\n",
       "      <td>YIP</td>\n",
       "      <td>Juniper SRX340JB</td>\n",
       "      <td>2024-01-30 19:39:03</td>\n",
       "      <td>2024-01-30 19:39:59</td>\n",
       "      <td>2024-01-30 19:39:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4439</td>\n",
       "      <td>2024-01-29 11:02:00</td>\n",
       "      <td>2024-01-31 00:00:00</td>\n",
       "      <td>Software</td>\n",
       "      <td>Major</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>POWER BI</td>\n",
       "      <td>2024-01-30 19:37:00</td>\n",
       "      <td>2024-01-30 19:39:59</td>\n",
       "      <td>2024-01-30 19:39:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   incident_datetime       close_datetime     incident_type severity  \\\n",
       "0  4440 2024-01-30 19:37:00                 None  General Incident    Minor   \n",
       "1  4439 2024-01-29 11:02:00  2024-01-31 00:00:00          Software    Major   \n",
       "\n",
       "       brand             model    last_update_date       creation_date  \\\n",
       "0        YIP  Juniper SRX340JB 2024-01-30 19:39:03 2024-01-30 19:39:59   \n",
       "1  Microsoft          POWER BI 2024-01-30 19:37:00 2024-01-30 19:39:59   \n",
       "\n",
       "           ImportedAt  \n",
       "0 2024-01-30 19:39:22  \n",
       "1 2024-01-30 19:39:22  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.columns=[ col.lower() for col in  dfAll.columns   ]  \n",
    "\n",
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd13a93-e8f3-443a-9832-86fafbdd9b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "# BigQuery Schema Management and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incident_datetime', 'brand', 'model'] is in dataframe from postgresql View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "# checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in [ x.lower() for x in  listColDF] ]\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from {db_product} View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from {db_product} View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBQSchemaByDF():\n",
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "    schema = []\n",
    "    srCols=dfAll.dtypes\n",
    "    try:\n",
    "        for name, type_name in srCols.items():\n",
    "            # print(name,type_name)\n",
    "            if str(type_name) in ['int32','int64']:\n",
    "              schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='float64':\n",
    "              schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='datetime64[ns]':\n",
    "              if name in   dateCols:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "              else:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) == 'bool':\n",
    "                 schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "            else: # if not found type , it will be converted to STRING\n",
    "               schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "\n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    schemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in schema])\n",
    "    \n",
    "    return  schema,schemaDictNameType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pbpython.com/pandas_dtypes.html\n",
    "      #https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html  \n",
    "      #https://datatofish.com/strings-to-datetime-pandas/ \n",
    "def convert_dfSchema_same_bqSChema(bqName,bqType): \n",
    "  try:  \n",
    "    if bqType in [\"INTEGER\",\"FLOAT\"]:\n",
    "        try:\n",
    "            if  bqType==\"INTEGER\":\n",
    "                dfAll[bqName]=dfAll[bqName].astype('int')\n",
    "            else:  \n",
    "                dfAll[bqName]=dfAll[bqName].astype('float')\n",
    "        except Exception as e:\n",
    "            print(f\"Extra : {bqName}-{bqType} has been converted by pd.to_numeric\")\n",
    "            dfAll[bqName]=pd.to_numeric(dfAll[bqName], errors='coerce')\n",
    "    elif bqType in [\"DATE\",\"DATETIME\"]:\n",
    "      dfAll[bqName]=pd.to_datetime(dfAll[bqName], errors='coerce',exact=False)   \n",
    "    elif bqType==\"BOOL\":\n",
    "      dfAll[bqName]=dfAll[bqName].astype('bool')\n",
    "    else:\n",
    "      dfAll[bqName]=dfAll[bqName].apply(lambda x: str(x))  \n",
    "    \n",
    "    print(f\"{bqName} has been converted  to {bqType} \")\n",
    "    \n",
    "  except Exception as ex:\n",
    "    raise ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(schema):\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of {db_product}\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"1#All Fields on BQ and DF are ok.\")  \n",
    "        print(\"==================================\")\n",
    "        \n",
    "    try:\n",
    "        __,SchemaDictNameType=createBQSchemaByDF()\n",
    "\n",
    "        for key_name, val_filed_type in ExistingSchemaDictNameType.items():\n",
    "            val2=SchemaDictNameType[key_name] \n",
    "            if  val_filed_type!=val2:\n",
    "              msg=f\"{key_name}-{val_filed_type} on existing schema is not them same as {key_name}-{val2} on loading schema.\"\n",
    "              print(msg)\n",
    "              convert_dfSchema_same_bqSChema( key_name,val_filed_type)\n",
    "        print(\"2#All Field Type on BQ and DF are the same.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        msg=f'{key_name} name on the loading schema does not exists  in the existing schema on Bigquery, so the system is unable to check field type matching.'\n",
    "        print(msg)\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "       \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"3#Partition Name Fields on BQ and DF is ok.\")    \n",
    "        print(\"==================================\" )    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"4#All Cluster on BQ and DF are ok.\")\n",
    "        print(\"==================================\")      \n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"5#All Date Column on BQ and DF are ok.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table pongthorn.SMartDW.view_demo_incident_inventory already exists.\n",
      "All 10 Fields as belows\n",
      "{'id': 'INTEGER', 'incident_datetime': 'DATETIME', 'close_datetime': 'STRING', 'incident_type': 'STRING', 'severity': 'STRING', 'brand': 'STRING', 'model': 'STRING', 'last_update_date': 'DATETIME', 'creation_date': 'DATETIME', 'ImportedAt': 'DATETIME'}\n",
      "Partiton Field&Type: incident_datetime - {'id': 'INTEGER', 'incident_datetime': 'DATETIME', 'close_datetime': 'STRING', 'incident_type': 'STRING', 'severity': 'STRING', 'brand': 'STRING', 'model': 'STRING', 'last_update_date': 'DATETIME', 'creation_date': 'DATETIME', 'ImportedAt': 'DATETIME'}\n",
      "Cluster Field List: ['brand', 'model']\n",
      "Date Field List: []\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "1#All Fields on BQ and DF are ok.\n",
      "==================================\n",
      "2#All Field Type on BQ and DF are the same.\n",
      "==================================\n",
      "3#Partition Name Fields on BQ and DF is ok.\n",
      "==================================\n",
      "4#All Cluster on BQ and DF are ok.\n",
      "==================================\n",
      "5#All Date Column on BQ and DF are ok.\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "isExistingTable=get_table()\n",
    "if isExistingTable:    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    bqSchema=table.schema\n",
    "    ExistingSchemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in bqSchema])\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All {len(ExistingSchemaDictNameType)} Fields as belows\")\n",
    "    print(ExistingSchemaDictNameType)\n",
    "    \n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {ExistingSchemaDictNameType}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    bqSchema,_=createBQSchemaByDF()\n",
    "    create_table(bqSchema)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rows and 10 columns are about to be imported to BQ by dataframe\n",
      "id                            int64\n",
      "incident_datetime    datetime64[ns]\n",
      "close_datetime               object\n",
      "incident_type                object\n",
      "severity                     object\n",
      "brand                        object\n",
      "model                        object\n",
      "last_update_date     datetime64[ns]\n",
      "creation_date        datetime64[ns]\n",
      "ImportedAt           datetime64[ns]\n",
      "dtype: object\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    no_rows=len(dfAll)\n",
    "    no_cols=len(dfAll.columns)\n",
    "   \n",
    "    if dfAll.empty==False:\n",
    "     if loading_from=='csv':\n",
    "        \n",
    "        dfAll.to_csv (temp_path,index=False)\n",
    "    print(f\"{no_rows} rows and {no_cols} columns are about to be imported to BQ by {loading_from}\")\n",
    "    print(dfAll.dtypes)\n",
    "    print(\"***********************************************************************************\")    \n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])  \n",
    "  logErrorMessage(listError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from dataframe\n",
      "Import data from view_demo_incident_inventory on postgresql into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:  \n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(listError)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=bqSchema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=write_to_bq_mode,\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=write_to_bq_mode,schema=bqSchema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config,)  \n",
    "\n",
    "\n",
    "    job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on {db_product} into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  msg=f\"BigQuery Error While Ingesting data with {str(e)}\"  \n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,is_load_all,completely)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if len(listError)>0:\n",
    " is_loaded_completely=0\n",
    "else:\n",
    " is_loaded_completely=1\n",
    "\n",
    "\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'is_load_all':is_load_all,'completely':is_loaded_completely}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984432-453f-4084-ba4b-8e502237dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26cc1-01f1-40ba-8c8e-de4bc7354caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e1e63-dff7-4566-b2d1-7979d0a8004b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
