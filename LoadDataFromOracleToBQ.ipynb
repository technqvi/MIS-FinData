{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Parameter variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_temp_folder='csv_temp'\n",
    "csv_error_folder='csv_error'\n",
    "\n",
    "source_name=''\n",
    "source_name=\"yip_invoice_monthly\" # df\n",
    "\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "#source_name='yip_ap_payment' # csv\n",
    "\n",
    "#source_name=\"yip_pj_status\" # csv\n",
    "\n",
    "#source_name='yip_po_listing' # df\n",
    "\n",
    "#source_name='yip_gl_account' # df\n",
    "\n",
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=True\n",
    "is_py=False\n",
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "# init_date_query='2023-01-01 00:00:00'\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "7acd5f40-3498-4135-b832-458b82d945af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter parameter on script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f80fe-b80e-42fb-98cb-cff290f5c031",
   "metadata": {},
   "source": [
    "# Check temp and error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "21674004-5a60-4ce3-bead-dfc1f8d5457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_temp_folder)==False:\n",
    "  os.mkdir(csv_temp_folder)\n",
    "if os.path.exists(csv_error_folder)==False:\n",
    "  os.mkdir(csv_error_folder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10' #'172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "host = 'mail.yipintsoi.com'\n",
    "port=  25\n",
    "sender=\"mis-bi-service@yipintsoigroup.com\"\n",
    "receivers=['pongthorn.sa@yipintsoi.com']\n",
    "#receivers=['pongthorn.sa@yipintsoi.com','mis-bi-service@yipintsoigroup.com']\n",
    "\n",
    "\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "temp_path=f'{csv_temp_folder}/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4a939-e71e-4901-8ab9-7688d54bbb41",
   "metadata": {},
   "source": [
    "# Setting BigQuery and Check DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "1cae0eb5-8769-4b4e-9f4c-f0050361f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "\n",
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "b81b25c9-9ca4-4a92-8a0b-56b744c1cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:44:06\n",
      "2023-06-13 16:44:06\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#dtStr_imported='2023-05-23 23:00:00'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorSubject,errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] =errorSubject \n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "34cb43ec-1209-4920-921f-bda3d5fbf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_error_file(): # if any error ,then move csv to investigte later\n",
    "    error_csv_path=''\n",
    "    try:\n",
    "        if os.path.exists(temp_path):\n",
    "         error_csv_file=f\"{source_name}_error_{ dt_imported.strftime('%d%m%y_%H%M%S')}.csv\"   \n",
    "         new_temp_path=f'{csv_temp_folder}/{error_csv_file}'\n",
    "         os.rename(temp_path,new_temp_path)  \n",
    "\n",
    "         error_csv_path=f'{csv_error_folder}/{error_csv_file}'\n",
    "         shutil.move(new_temp_path,error_csv_path )\n",
    "         \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "    return   error_csv_path      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        \n",
    "        error_message=f\"ETL Error on {source_name} at {dtStr_imported}\"\n",
    "        # move csv error file to examine later\n",
    "        error_csv_path  =move_error_file()\n",
    "        if error_csv_path!='':\n",
    "            error_message= f\"{error_csv_path} - {error_message}\"\n",
    "        error_message=f\"MIS-BI : {error_message}\"   \n",
    "        \n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        # add to error table into sqlite\n",
    "        recordError=dfError.to_records(index=False) \n",
    "        add_logError(recordError)\n",
    "        \n",
    "        # sened main\n",
    "        emailResult=sendMailForError(error_message, dfError.to_html(index=False))\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_invoice_monthly'  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                      yip_invoice_monthly\n",
       "first_load_col                      gl_date\n",
       "partition_date_col                  gl_date\n",
       "partition_date_type                   MONTH\n",
       "cluster_col_list         gl_date ,dept_name\n",
       "date_col_list          gl_date,invoice_date\n",
       "load_from_type                    dataframe\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "ds_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Load data into BigQuery from dataframe\n",
      "Date Column as condition to load at first = gl_date\n",
      "Partition : gl_date - MONTH\n",
      "Cluster List: ['gl_date', 'dept_name']\n",
      "Date Cols: ['gl_date', 'invoice_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    loading_from=ds_item['load_from_type']\n",
    "    print(f\"Load data into BigQuery from {loading_from}\")\n",
    "    \n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload all data:  2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isReLoadAll==True\n",
      "select * from yip_gl_account   \n",
      "           where  start_date>=to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') \n",
      "           \n"
     ]
    }
   ],
   "source": [
    "def loadData(isReLoadAll):\n",
    "    print(f\"isReLoadAll=={isReLoadAll}\")  \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "          \n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:    \n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine)\n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb737af-841a-46d5-aa18-7af6f48342a4",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "a16a8bd7-4f2c-405a-b4bf-c96cab6d591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll['last_update_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "# _dateValueTest='2023-06-09 22:45:09'\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#=============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9cc50-90a5-4f10-bbce-c9cfea3d658e",
   "metadata": {},
   "source": [
    "# Transform Dataframe prior to Ingesting it to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['ledger_id', 'ledger_name', 'code_combination_id', 'account_type', 'period_year', 'period_num', 'period_name', 'start_date', 'end_date', 'segment1', 'segment2', 'segment3', 'segment4', 'segment5', 'account_code', 'description', 'period_net_dr', 'period_net_cr', 'period_net', 'begining_balance', 'ending_balance', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 321825 entries, 0 to 321824\n",
      "Data columns (total 26 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   ledger_id            321825 non-null  int64         \n",
      " 1   ledger_name          321825 non-null  object        \n",
      " 2   code_combination_id  321825 non-null  int64         \n",
      " 3   account_type         321825 non-null  object        \n",
      " 4   period_year          321825 non-null  int64         \n",
      " 5   period_num           321825 non-null  int64         \n",
      " 6   period_name          321825 non-null  object        \n",
      " 7   start_date           321825 non-null  datetime64[ns]\n",
      " 8   end_date             321825 non-null  datetime64[ns]\n",
      " 9   segment1             321825 non-null  object        \n",
      " 10  segment2             321825 non-null  object        \n",
      " 11  segment3             321825 non-null  object        \n",
      " 12  segment4             321825 non-null  object        \n",
      " 13  segment5             321825 non-null  object        \n",
      " 14  account_code         321825 non-null  object        \n",
      " 15  description          321825 non-null  object        \n",
      " 16  period_net_dr        321825 non-null  float64       \n",
      " 17  period_net_cr        321825 non-null  float64       \n",
      " 18  period_net           321825 non-null  float64       \n",
      " 19  begining_balance     321825 non-null  float64       \n",
      " 20  ending_balance       321825 non-null  float64       \n",
      " 21  creation_date        321825 non-null  datetime64[ns]\n",
      " 22  created_by           321825 non-null  int64         \n",
      " 23  last_update_date     321825 non-null  datetime64[ns]\n",
      " 24  last_updated_by      321825 non-null  int64         \n",
      " 25  ImportedAt           321825 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](5), float64(5), int64(6), object(10)\n",
      "memory usage: 63.8+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ledger_id</th>\n",
       "      <th>ledger_name</th>\n",
       "      <th>code_combination_id</th>\n",
       "      <th>account_type</th>\n",
       "      <th>period_year</th>\n",
       "      <th>period_num</th>\n",
       "      <th>period_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>segment1</th>\n",
       "      <th>...</th>\n",
       "      <th>period_net_dr</th>\n",
       "      <th>period_net_cr</th>\n",
       "      <th>period_net</th>\n",
       "      <th>begining_balance</th>\n",
       "      <th>ending_balance</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>TCCL Set of Book</td>\n",
       "      <td>59607</td>\n",
       "      <td>E</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>MAY-23</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>287651.00</td>\n",
       "      <td>472121.52</td>\n",
       "      <td>-184470.52</td>\n",
       "      <td>-287651.00</td>\n",
       "      <td>-472121.52</td>\n",
       "      <td>2023-06-09 10:58:33</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-09 22:16:01</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-13 16:19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>TCCL Set of Book</td>\n",
       "      <td>1285170</td>\n",
       "      <td>L</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>MAY-23</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>976684.80</td>\n",
       "      <td>498635.89</td>\n",
       "      <td>478048.91</td>\n",
       "      <td>-976684.80</td>\n",
       "      <td>-498635.89</td>\n",
       "      <td>2023-06-09 10:58:33</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-09 22:16:01</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-13 16:19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>TCCL Set of Book</td>\n",
       "      <td>1536215</td>\n",
       "      <td>E</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>JUN-23</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>970757.41</td>\n",
       "      <td>970757.41</td>\n",
       "      <td>2023-06-09 10:58:33</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-09 22:16:01</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-13 16:19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>TCCL Set of Book</td>\n",
       "      <td>1536215</td>\n",
       "      <td>E</td>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>MAY-23</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>970757.41</td>\n",
       "      <td>1264335.80</td>\n",
       "      <td>-293578.39</td>\n",
       "      <td>1264335.80</td>\n",
       "      <td>970757.41</td>\n",
       "      <td>2023-06-09 10:58:33</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-09 22:16:01</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-13 16:19:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>TCCL Set of Book</td>\n",
       "      <td>59607</td>\n",
       "      <td>E</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>JUN-23</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-472121.52</td>\n",
       "      <td>-472121.52</td>\n",
       "      <td>2023-06-09 10:58:33</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-09 22:16:01</td>\n",
       "      <td>2443</td>\n",
       "      <td>2023-06-13 16:19:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ledger_id       ledger_name  code_combination_id account_type  period_year   \n",
       "0         61  TCCL Set of Book                59607            E         2023  \\\n",
       "1         61  TCCL Set of Book              1285170            L         2023   \n",
       "2         61  TCCL Set of Book              1536215            E         2023   \n",
       "3         61  TCCL Set of Book              1536215            E         2023   \n",
       "4         61  TCCL Set of Book                59607            E         2023   \n",
       "\n",
       "   period_num period_name start_date   end_date segment1  ... period_net_dr   \n",
       "0           8      MAY-23 2023-05-01 2023-05-31       12  ...     287651.00  \\\n",
       "1           8      MAY-23 2023-05-01 2023-05-31       12  ...     976684.80   \n",
       "2           9      JUN-23 2023-06-01 2023-06-30       12  ...          0.00   \n",
       "3           8      MAY-23 2023-05-01 2023-05-31       12  ...     970757.41   \n",
       "4           9      JUN-23 2023-06-01 2023-06-30       12  ...          0.00   \n",
       "\n",
       "  period_net_cr period_net begining_balance ending_balance   \n",
       "0     472121.52 -184470.52       -287651.00     -472121.52  \\\n",
       "1     498635.89  478048.91       -976684.80     -498635.89   \n",
       "2          0.00       0.00        970757.41      970757.41   \n",
       "3    1264335.80 -293578.39       1264335.80      970757.41   \n",
       "4          0.00       0.00       -472121.52     -472121.52   \n",
       "\n",
       "        creation_date  created_by    last_update_date  last_updated_by   \n",
       "0 2023-06-09 10:58:33        2443 2023-06-09 22:16:01             2443  \\\n",
       "1 2023-06-09 10:58:33        2443 2023-06-09 22:16:01             2443   \n",
       "2 2023-06-09 10:58:33        2443 2023-06-09 22:16:01             2443   \n",
       "3 2023-06-09 10:58:33        2443 2023-06-09 22:16:01             2443   \n",
       "4 2023-06-09 10:58:33        2443 2023-06-09 22:16:01             2443   \n",
       "\n",
       "           ImportedAt  \n",
       "0 2023-06-13 16:19:18  \n",
       "1 2023-06-13 16:19:18  \n",
       "2 2023-06-13 16:19:18  \n",
       "3 2023-06-13 16:19:18  \n",
       "4 2023-06-13 16:19:18  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start_date', 'end_date'] is in dataframe from Oracel View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from Oracel View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from Oracel View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4a2b7-a91d-4e08-8885-28532591208f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b65f21f-dfed-407c-959a-97b7439e0ca9",
   "metadata": {},
   "source": [
    "## Creaste bigquery schema from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of field of the schema : 26\n",
      "[SchemaField('ledger_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ledger_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('code_combination_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('account_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('period_year', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('period_num', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('period_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('start_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('end_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('segment1', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('segment2', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('segment3', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('segment4', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('segment5', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('account_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('description', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('period_net_dr', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('period_net_cr', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('period_net', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('begining_balance', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('ending_balance', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('creation_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('created_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('last_update_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('last_updated_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ImportedAt', 'DATETIME', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "schema = []\n",
    "srCols=dfAll.dtypes\n",
    "try:\n",
    "    for name, type_name in srCols.items():\n",
    "        # print(name,type_name)\n",
    "        if str(type_name) in ['int32','int64']:\n",
    "          schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='float64':\n",
    "          schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='datetime64[ns]':\n",
    "          if name in   dateCols:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "          else:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) == 'bool':\n",
    "             schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "        else: # if not found type , it will be converted to STRING\n",
    "           schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "        \n",
    "    # if  len(schema)!=len(listColDF):\n",
    "    #    listFieldBQError=[field.name for field in schema]\n",
    "    #    intersec_DF_BQ = [set(listColDF).symmetric_difference(set(listFieldBQError))]\n",
    "    #    raise Exception(f\"{len(schema)}!={len(listColDF)} , {intersec_DF_BQ} in Dataframe can not be converted to Bigquery Data type\")\n",
    "\n",
    "except Exception as e:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "   logErrorMessage(listError)\n",
    "print(\"Total number of field of the schema :\",len(schema))    \n",
    "print(schema) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "## Check whether dataframe and bigquery schema are the same\n",
    "\n",
    "## Check Existing  Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of Oracle\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Fields on BQ and DF are ok.\")\n",
    "        \n",
    "        \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"Partition Name Fields on BQ and DF is ok.\")    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Cluster on BQ and DF are ok.\")\n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Date Column on BQ and DF are ok.\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "# delete table and set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new table mismgntdata-bigquery.MIS_BI_DW.yip_gl_account\n"
     ]
    }
   ],
   "source": [
    "isExistingTable=get_table()\n",
    "if isExistingTable:    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All Fields : {listFieldBQ}\")\n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {partitionTypeBQ}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    create_table()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321825 rows and 26 are about to be imported to BQ by dataframe\n"
     ]
    }
   ],
   "source": [
    "no_rows=len(dfAll)\n",
    "no_cols=len(dfAll.columns)\n",
    "if dfAll.empty==False:\n",
    " if loading_from=='csv':\n",
    "    dfAll.to_csv (temp_path,index=False)\n",
    "print(f\"{no_rows} rows and {no_cols} are about to be imported to BQ by {loading_from}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from dataframe\n",
      "Import data from yip_gl_account on Oracle into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:  \n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(listError)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=schema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=schema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config,)  \n",
    "\n",
    "\n",
    "    result_job=job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on Oracle into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"BQ Error and Raise Exception\")  \n",
    "  print(str(e))\n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,is_load_all,completely)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if isLoadingAllItems==True:\n",
    "    is_load_all=1\n",
    "else:\n",
    "    is_load_all=0\n",
    "    \n",
    "if len(listError)>0:\n",
    " is_loaded_completely=0\n",
    "else:\n",
    " is_loaded_completely=1\n",
    "\n",
    "#dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'is_load_all':is_load_all,'completely':is_loaded_completely}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984432-453f-4084-ba4b-8e502237dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26cc1-01f1-40ba-8c8e-de4bc7354caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e1e63-dff7-4566-b2d1-7979d0a8004b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
