{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192bc4-6924-4e88-9fc1-981c816f918c",
   "metadata": {},
   "source": [
    "# Time To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf1c7f-c8b9-4017-9b06-629aa83d885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_imported=datetime.now()\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "dtStr_imported='2023-06-13 17:11:54'\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Parameter variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=False\n",
    "is_py=False\n",
    "\n",
    "source_name=''\n",
    "#source_name=\"yip_invoice_monthly\" # df\n",
    "\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "#source_name='yip_ap_payment' # csv\n",
    "\n",
    "#source_name=\"yip_pj_status\" # csv\n",
    "\n",
    "# source_name='yip_po_listing' # df\n",
    "\n",
    "#source_name='yip_gl_account' # df\n",
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "# init_date_query='2023-01-01 00:00:00'\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "\n",
    "csv_temp_folder='csv_temp'\n",
    "csv_error_folder='csv_error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25b842-861c-4543-b7bf-abe21d688752",
   "metadata": {},
   "source": [
    "# Enter parameter on script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f80fe-b80e-42fb-98cb-cff290f5c031",
   "metadata": {},
   "source": [
    "# Check temp and error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "21674004-5a60-4ce3-bead-dfc1f8d5457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_temp_folder)==False:\n",
    "  os.mkdir(csv_temp_folder)\n",
    "if os.path.exists(csv_error_folder)==False:\n",
    "  os.mkdir(csv_error_folder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10' #'172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "host = 'mail.yipintsoi.com'\n",
    "port=  25\n",
    "sender=\"mis-bi-service@yipintsoigroup.com\"\n",
    "receivers=['pongthorn.sa@yipintsoi.com']\n",
    "#receivers=['pongthorn.sa@yipintsoi.com','mis-bi-service@yipintsoigroup.com']\n",
    "\n",
    "\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "temp_path=f'{csv_temp_folder}/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'\n",
    "etlDateCols=[updateCol,'creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4a939-e71e-4901-8ab9-7688d54bbb41",
   "metadata": {},
   "source": [
    "# Setting BigQuery and Check DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1cae0eb5-8769-4b4e-9f4c-f0050361f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "\n",
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorSubject,errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] =errorSubject \n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "34cb43ec-1209-4920-921f-bda3d5fbf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_error_file(): # if any error ,then move csv to investigte later\n",
    "    error_csv_path=''\n",
    "    try:\n",
    "        if os.path.exists(temp_path):\n",
    "         error_csv_file=f\"{source_name}_error_{ dt_imported.strftime('%d%m%y_%H%M%S')}.csv\"   \n",
    "         new_temp_path=f'{csv_temp_folder}/{error_csv_file}'\n",
    "         os.rename(temp_path,new_temp_path)  \n",
    "\n",
    "         error_csv_path=f'{csv_error_folder}/{error_csv_file}'\n",
    "         shutil.move(new_temp_path,error_csv_path )\n",
    "         \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "    return   error_csv_path      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        \n",
    "        error_message=f\"ETL Error on {source_name} at {dtStr_imported}\"\n",
    "        # move csv error file to examine later\n",
    "        error_csv_path  =move_error_file()\n",
    "        if error_csv_path!='':\n",
    "            error_message= f\"{error_csv_path} - {error_message}\"\n",
    "        error_message=f\"MIS-BI : {error_message}\"   \n",
    "        \n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        # add to error table into sqlite\n",
    "        recordError=dfError.to_records(index=False) \n",
    "        add_logError(recordError)\n",
    "        \n",
    "        # sened main\n",
    "        emailResult=sendMailForError(error_message, dfError.to_html(index=False))\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_po_listing'  \n",
      "id                     yip_po_listing\n",
      "first_load_col                po_date\n",
      "partition_date_col            po_date\n",
      "partition_date_type             MONTH\n",
      "cluster_col_list                     \n",
      "date_col_list                        \n",
      "load_from_type              dataframe\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "print(ds_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Load data into BigQuery from dataframe\n",
      "Date Column as condition to load at first = po_date\n",
      "Partition : po_date - MONTH\n",
      "[] (No cluster cols)\n",
      "[] (No Date cols)\n",
      "All Reuired Date Cols\n",
      "['po_date', 'last_update_date', 'creation_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    loading_from=ds_item['load_from_type']\n",
    "    print(f\"Load data into BigQuery from {loading_from}\")\n",
    "    \n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)\n",
    "    \n",
    "dateColsToConvert=[partitionCol]+dateCols+etlDateCols\n",
    "dateColsToConvert = list(dict.fromkeys(dateColsToConvert))\n",
    "print(\"All Reuired Date Cols\")\n",
    "print(dateColsToConvert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select etl_datetime,data_source_id from etl_transaction where data_source_id='yip_po_listing' \n",
      "    order by etl_datetime desc limit 1\n",
      "    \n",
      "          etl_datetime  data_source_id\n",
      "0  2023-06-13 17:12:31  yip_po_listing\n",
      "Start Import on update_at of last ETL date :  2023-06-13 17:12:31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "\n",
    "if isLoadingAllItems==True:\n",
    " is_load_all=1\n",
    "else:\n",
    " is_load_all=0   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5997314-6afb-478c-adf1-6e328cab8556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from yip_po_listing  \n",
      "           where  last_update_date>=to_date('2023-06-13 17:12:31','yyyy-mm-dd hh24:mi:ss') \n",
      "isReLoadAll==False==0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loadData(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:   \n",
    "\n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb737af-841a-46d5-aa18-7af6f48342a4",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a16a8bd7-4f2c-405a-b4bf-c96cab6d591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll['last_update_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "\n",
    "# _dateValueTest='2023-06-13 17:11:54'\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9cc50-90a5-4f10-bbce-c9cfea3d658e",
   "metadata": {},
   "source": [
    "# Transform Dataframe prior to Ingesting it to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['po_header_id', 'po_line_id', 'org_id', 'po_number', 'po_date', 'comments', 'vendor_name', 'vendor_site_code', 'currency_code', 'rate_type', 'rate_type_name', 'rate_date', 'rate', 'buyer', 'authorization_status', 'revision_number', 'revised_date', 'approved_flag', 'approved_date', 'closed_date', 'cancel_flag', 'closed_code', 'inv_organization_code', 'inv_organization_name', 'po_amount', 'payment_amount', 'line_number', 'shipment_number', 'item_code', 'item_description', 'item_job', 'uom', 'shipment_amount', 'quantity', 'quantity_due', 'quantity_received', 'quantity_billed', 'need_by_date', 'line_closed_flag', 'line_closed_code', 'line_closed_date', 'line_cancel_flag', 'line_cancelled_by', 'line_cancel_date', 'line_attribute1', 'line_attribute2', 'line_attribute3', 'line_attribute4', 'line_attribute5', 'line_attribute6', 'line_attribute7', 'line_attribute8', 'line_attribute9', 'line_attribute10', 'line_attribute11', 'line_attribute12', 'line_attribute13', 'line_attribute14', 'line_attribute15', 'product_type', 'product_brand', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 66 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   po_header_id           16 non-null     int64         \n",
      " 1   po_line_id             16 non-null     int64         \n",
      " 2   org_id                 16 non-null     int64         \n",
      " 3   po_number              16 non-null     object        \n",
      " 4   po_date                16 non-null     datetime64[ns]\n",
      " 5   comments               16 non-null     object        \n",
      " 6   vendor_name            16 non-null     object        \n",
      " 7   vendor_site_code       16 non-null     object        \n",
      " 8   currency_code          16 non-null     object        \n",
      " 9   rate_type              3 non-null      object        \n",
      " 10  rate_type_name         3 non-null      object        \n",
      " 11  rate_date              16 non-null     datetime64[ns]\n",
      " 12  rate                   3 non-null      float64       \n",
      " 13  buyer                  16 non-null     object        \n",
      " 14  authorization_status   16 non-null     object        \n",
      " 15  revision_number        16 non-null     int64         \n",
      " 16  revised_date           4 non-null      datetime64[ns]\n",
      " 17  approved_flag          16 non-null     object        \n",
      " 18  approved_date          16 non-null     datetime64[ns]\n",
      " 19  closed_date            1 non-null      datetime64[ns]\n",
      " 20  cancel_flag            16 non-null     object        \n",
      " 21  closed_code            16 non-null     object        \n",
      " 22  inv_organization_code  16 non-null     object        \n",
      " 23  inv_organization_name  16 non-null     object        \n",
      " 24  po_amount              16 non-null     int64         \n",
      " 25  payment_amount         16 non-null     int64         \n",
      " 26  line_number            16 non-null     int64         \n",
      " 27  shipment_number        16 non-null     int64         \n",
      " 28  item_code              16 non-null     object        \n",
      " 29  item_description       16 non-null     object        \n",
      " 30  item_job               16 non-null     object        \n",
      " 31  uom                    16 non-null     object        \n",
      " 32  shipment_amount        16 non-null     float64       \n",
      " 33  quantity               16 non-null     int64         \n",
      " 34  quantity_due           16 non-null     int64         \n",
      " 35  quantity_received      16 non-null     int64         \n",
      " 36  quantity_billed        16 non-null     int64         \n",
      " 37  need_by_date           15 non-null     datetime64[ns]\n",
      " 38  line_closed_flag       1 non-null      object        \n",
      " 39  line_closed_code       16 non-null     object        \n",
      " 40  line_closed_date       1 non-null      datetime64[ns]\n",
      " 41  line_cancel_flag       16 non-null     object        \n",
      " 42  line_cancelled_by      1 non-null      float64       \n",
      " 43  line_cancel_date       1 non-null      datetime64[ns]\n",
      " 44  line_attribute1        0 non-null      object        \n",
      " 45  line_attribute2        16 non-null     object        \n",
      " 46  line_attribute3        16 non-null     object        \n",
      " 47  line_attribute4        15 non-null     object        \n",
      " 48  line_attribute5        12 non-null     object        \n",
      " 49  line_attribute6        6 non-null      object        \n",
      " 50  line_attribute7        5 non-null      object        \n",
      " 51  line_attribute8        4 non-null      object        \n",
      " 52  line_attribute9        2 non-null      object        \n",
      " 53  line_attribute10       2 non-null      object        \n",
      " 54  line_attribute11       2 non-null      object        \n",
      " 55  line_attribute12       2 non-null      object        \n",
      " 56  line_attribute13       2 non-null      object        \n",
      " 57  line_attribute14       2 non-null      object        \n",
      " 58  line_attribute15       0 non-null      object        \n",
      " 59  product_type           0 non-null      object        \n",
      " 60  product_brand          0 non-null      object        \n",
      " 61  creation_date          16 non-null     datetime64[ns]\n",
      " 62  created_by             16 non-null     int64         \n",
      " 63  last_update_date       16 non-null     datetime64[ns]\n",
      " 64  last_updated_by        16 non-null     int64         \n",
      " 65  ImportedAt             16 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](11), float64(3), int64(14), object(38)\n",
      "memory usage: 8.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>po_header_id</th>\n",
       "      <th>po_line_id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>po_number</th>\n",
       "      <th>po_date</th>\n",
       "      <th>comments</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>vendor_site_code</th>\n",
       "      <th>currency_code</th>\n",
       "      <th>rate_type</th>\n",
       "      <th>...</th>\n",
       "      <th>line_attribute13</th>\n",
       "      <th>line_attribute14</th>\n",
       "      <th>line_attribute15</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6008799</td>\n",
       "      <td>6282692</td>\n",
       "      <td>81</td>\n",
       "      <td>1122301585</td>\n",
       "      <td>2023-06-13 16:47:55</td>\n",
       "      <td>230436/3/HW</td>\n",
       "      <td>บริษัท กู๊ด มายด์ จำกัด</td>\n",
       "      <td>YIP</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-13 16:47:55</td>\n",
       "      <td>1070</td>\n",
       "      <td>2023-06-13 22:50:10</td>\n",
       "      <td>1083</td>\n",
       "      <td>2023-06-14 22:19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5955804</td>\n",
       "      <td>6228698</td>\n",
       "      <td>81</td>\n",
       "      <td>1122301543</td>\n",
       "      <td>2023-06-09 11:53:53</td>\n",
       "      <td>211415/4//IM</td>\n",
       "      <td>COVALENSE DIGITAL SOLUTIONS PTE.LTD</td>\n",
       "      <td>YIP</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-09 11:53:53</td>\n",
       "      <td>1070</td>\n",
       "      <td>2023-06-13 22:50:10</td>\n",
       "      <td>1070</td>\n",
       "      <td>2023-06-14 22:19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6004799</td>\n",
       "      <td>6278692</td>\n",
       "      <td>81</td>\n",
       "      <td>1122301579</td>\n",
       "      <td>2023-06-13 14:42:45</td>\n",
       "      <td>211415/4/IM</td>\n",
       "      <td>COVALENSE DIGITAL SOLUTIONS PTE.LTD</td>\n",
       "      <td>YIP</td>\n",
       "      <td>USD</td>\n",
       "      <td>1042</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-13 14:42:45</td>\n",
       "      <td>1070</td>\n",
       "      <td>2023-06-13 22:50:10</td>\n",
       "      <td>1088</td>\n",
       "      <td>2023-06-14 22:19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5906799</td>\n",
       "      <td>6188693</td>\n",
       "      <td>81</td>\n",
       "      <td>1122301509</td>\n",
       "      <td>2023-06-07 10:42:14</td>\n",
       "      <td>Vendor : VRCOMM for sale to บริษัท E-Square en...</td>\n",
       "      <td>บริษัท วีอาร์ซีโอเอ็มเอ็ม จำกัด</td>\n",
       "      <td>YIP</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-07 10:42:14</td>\n",
       "      <td>1071</td>\n",
       "      <td>2023-06-13 22:50:10</td>\n",
       "      <td>1843</td>\n",
       "      <td>2023-06-14 22:19:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6006799</td>\n",
       "      <td>6280692</td>\n",
       "      <td>81</td>\n",
       "      <td>1122301584</td>\n",
       "      <td>2023-06-13 15:48:24</td>\n",
       "      <td>Vendor : netapp for sale to GSB#พณ.ป.02-24/256...</td>\n",
       "      <td>NetAPP lreland Limited</td>\n",
       "      <td>YIP</td>\n",
       "      <td>USD</td>\n",
       "      <td>1042</td>\n",
       "      <td>...</td>\n",
       "      <td>Suksan.mo@yipintsoi.com</td>\n",
       "      <td>Do not include documents in physical shipment</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-06-13 15:48:24</td>\n",
       "      <td>1071</td>\n",
       "      <td>2023-06-13 22:50:10</td>\n",
       "      <td>1086</td>\n",
       "      <td>2023-06-14 22:19:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   po_header_id  po_line_id  org_id   po_number             po_date   \n",
       "0       6008799     6282692      81  1122301585 2023-06-13 16:47:55  \\\n",
       "1       5955804     6228698      81  1122301543 2023-06-09 11:53:53   \n",
       "2       6004799     6278692      81  1122301579 2023-06-13 14:42:45   \n",
       "3       5906799     6188693      81  1122301509 2023-06-07 10:42:14   \n",
       "4       6006799     6280692      81  1122301584 2023-06-13 15:48:24   \n",
       "\n",
       "                                            comments   \n",
       "0                                        230436/3/HW  \\\n",
       "1                                       211415/4//IM   \n",
       "2                                        211415/4/IM   \n",
       "3  Vendor : VRCOMM for sale to บริษัท E-Square en...   \n",
       "4  Vendor : netapp for sale to GSB#พณ.ป.02-24/256...   \n",
       "\n",
       "                           vendor_name vendor_site_code currency_code   \n",
       "0              บริษัท กู๊ด มายด์ จำกัด              YIP           THB  \\\n",
       "1  COVALENSE DIGITAL SOLUTIONS PTE.LTD              YIP           THB   \n",
       "2  COVALENSE DIGITAL SOLUTIONS PTE.LTD              YIP           USD   \n",
       "3      บริษัท วีอาร์ซีโอเอ็มเอ็ม จำกัด              YIP           THB   \n",
       "4               NetAPP lreland Limited              YIP           USD   \n",
       "\n",
       "  rate_type  ...         line_attribute13   \n",
       "0      None  ...                     None  \\\n",
       "1      None  ...                     None   \n",
       "2      1042  ...                     None   \n",
       "3      None  ...                     None   \n",
       "4      1042  ...  Suksan.mo@yipintsoi.com   \n",
       "\n",
       "                                line_attribute14  line_attribute15   \n",
       "0                                           None              None  \\\n",
       "1                                           None              None   \n",
       "2                                           None              None   \n",
       "3                                           None              None   \n",
       "4  Do not include documents in physical shipment              None   \n",
       "\n",
       "  product_type product_brand       creation_date created_by   \n",
       "0         None          None 2023-06-13 16:47:55       1070  \\\n",
       "1         None          None 2023-06-09 11:53:53       1070   \n",
       "2         None          None 2023-06-13 14:42:45       1070   \n",
       "3         None          None 2023-06-07 10:42:14       1071   \n",
       "4         None          None 2023-06-13 15:48:24       1071   \n",
       "\n",
       "     last_update_date last_updated_by          ImportedAt  \n",
       "0 2023-06-13 22:50:10            1083 2023-06-14 22:19:26  \n",
       "1 2023-06-13 22:50:10            1070 2023-06-14 22:19:26  \n",
       "2 2023-06-13 22:50:10            1088 2023-06-14 22:19:26  \n",
       "3 2023-06-13 22:50:10            1843 2023-06-14 22:19:26  \n",
       "4 2023-06-13 22:50:10            1086 2023-06-14 22:19:26  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4e7a3e0f-e334-4358-ae97-e44c1185b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['po_date'] is in dataframe from Oracel View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from Oracel View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from Oracel View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "# BigQuery Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBQSchemaByDF():\n",
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "    schema = []\n",
    "    srCols=dfAll.dtypes\n",
    "    try:\n",
    "        for name, type_name in srCols.items():\n",
    "            # print(name,type_name)\n",
    "            if str(type_name) in ['int32','int64']:\n",
    "              schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='float64':\n",
    "              schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='datetime64[ns]':\n",
    "              if name in   dateCols:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "              else:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) == 'bool':\n",
    "                 schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "            else: # if not found type , it will be converted to STRING\n",
    "               schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "\n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    schemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in schema])\n",
    "    \n",
    "    return  schema,schemaDictNameType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pbpython.com/pandas_dtypes.html\n",
    "      #https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html  \n",
    "      #https://datatofish.com/strings-to-datetime-pandas/ \n",
    "def convert_dfSchema_same_bqSChema(bqName,bqType): \n",
    "  try:  \n",
    "    if bqType in [\"INTEGER\",\"FLOAT\"]:\n",
    "        try:\n",
    "            if  bqType==\"INTEGER\":\n",
    "                dfAll[bqName]=dfAll[bqName].astype('int')\n",
    "            else:  \n",
    "                dfAll[bqName]=dfAll[bqName].astype('float')\n",
    "        except Exception as e:\n",
    "            print(f\"Extra : {bqName}-{bqType} has been converted by pd.to_numeric\")\n",
    "            dfAll[bqName]=pd.to_numeric(dfAll[bqName], errors='coerce')\n",
    "    elif bqType in [\"DATE\",\"DATETIME\"]:\n",
    "      dfAll[bqName]=pd.to_datetime(dfAll[bqName], errors='coerce',exact=False)   \n",
    "    elif bqType==\"BOOL\":\n",
    "      dfAll[bqName]=dfAll[bqName].astype('bool')\n",
    "    else:\n",
    "      dfAll[bqName]=dfAll[bqName].apply(lambda x: str(x))  \n",
    "    \n",
    "    print(f\"{bqName} has been converted  to {bqType} \")\n",
    "    \n",
    "  except Exception as ex:\n",
    "    raise ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(schema):\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of Oracle\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"1#All Fields on BQ and DF are ok.\")  \n",
    "        print(\"==================================\")\n",
    "        \n",
    "    try:\n",
    "        __,SchemaDictNameType=createBQSchemaByDF()\n",
    "\n",
    "        for key_name, val_filed_type in ExistingSchemaDictNameType.items():\n",
    "            val2=SchemaDictNameType[key_name] \n",
    "            if  val_filed_type!=val2:\n",
    "              msg=f\"{key_name}-{val_filed_type} on existing schema is not them same as {key_name}-{val2} on loading schema.\"\n",
    "              print(msg)\n",
    "              convert_dfSchema_same_bqSChema( key_name,val_filed_type)\n",
    "        print(\"2#All Field Type on BQ and DF are the same.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        msg=f'{key_name} name on the loading schema does not exists  in the existing schema on Bigquery, so the system is unable to check field type matching.'\n",
    "        print(msg)\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "       \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"3#Partition Name Fields on BQ and DF is ok.\")    \n",
    "        print(\"==================================\" )    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"4#All Cluster on BQ and DF are ok.\")\n",
    "        print(\"==================================\")      \n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"5#All Date Column on BQ and DF are ok.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "# delete table and set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mismgntdata-bigquery.MIS_BI_DW.yip_po_listing already exists.\n",
      "All 66 Fields as belows\n",
      "{'po_header_id': 'INTEGER', 'po_line_id': 'INTEGER', 'org_id': 'INTEGER', 'po_number': 'STRING', 'po_date': 'DATETIME', 'comments': 'STRING', 'vendor_name': 'STRING', 'vendor_site_code': 'STRING', 'currency_code': 'STRING', 'rate_type': 'STRING', 'rate_type_name': 'STRING', 'rate_date': 'DATETIME', 'rate': 'FLOAT', 'buyer': 'STRING', 'authorization_status': 'STRING', 'revision_number': 'INTEGER', 'revised_date': 'DATETIME', 'approved_flag': 'STRING', 'approved_date': 'DATETIME', 'closed_date': 'DATETIME', 'cancel_flag': 'STRING', 'closed_code': 'STRING', 'inv_organization_code': 'STRING', 'inv_organization_name': 'STRING', 'po_amount': 'FLOAT', 'payment_amount': 'FLOAT', 'line_number': 'INTEGER', 'shipment_number': 'INTEGER', 'item_code': 'STRING', 'item_description': 'STRING', 'item_job': 'STRING', 'uom': 'STRING', 'shipment_amount': 'FLOAT', 'quantity': 'FLOAT', 'quantity_due': 'INTEGER', 'quantity_received': 'FLOAT', 'quantity_billed': 'FLOAT', 'need_by_date': 'DATETIME', 'line_closed_flag': 'STRING', 'line_closed_code': 'STRING', 'line_closed_date': 'DATETIME', 'line_cancel_flag': 'STRING', 'line_cancelled_by': 'FLOAT', 'line_cancel_date': 'DATETIME', 'line_attribute1': 'STRING', 'line_attribute2': 'STRING', 'line_attribute3': 'STRING', 'line_attribute4': 'STRING', 'line_attribute5': 'STRING', 'line_attribute6': 'STRING', 'line_attribute7': 'STRING', 'line_attribute8': 'STRING', 'line_attribute9': 'STRING', 'line_attribute10': 'STRING', 'line_attribute11': 'STRING', 'line_attribute12': 'STRING', 'line_attribute13': 'STRING', 'line_attribute14': 'STRING', 'line_attribute15': 'STRING', 'product_type': 'STRING', 'product_brand': 'STRING', 'creation_date': 'DATETIME', 'created_by': 'INTEGER', 'last_update_date': 'DATETIME', 'last_updated_by': 'INTEGER', 'ImportedAt': 'DATETIME'}\n",
      "Partiton Field&Type: po_date - {'po_header_id': 'INTEGER', 'po_line_id': 'INTEGER', 'org_id': 'INTEGER', 'po_number': 'STRING', 'po_date': 'DATETIME', 'comments': 'STRING', 'vendor_name': 'STRING', 'vendor_site_code': 'STRING', 'currency_code': 'STRING', 'rate_type': 'STRING', 'rate_type_name': 'STRING', 'rate_date': 'DATETIME', 'rate': 'FLOAT', 'buyer': 'STRING', 'authorization_status': 'STRING', 'revision_number': 'INTEGER', 'revised_date': 'DATETIME', 'approved_flag': 'STRING', 'approved_date': 'DATETIME', 'closed_date': 'DATETIME', 'cancel_flag': 'STRING', 'closed_code': 'STRING', 'inv_organization_code': 'STRING', 'inv_organization_name': 'STRING', 'po_amount': 'FLOAT', 'payment_amount': 'FLOAT', 'line_number': 'INTEGER', 'shipment_number': 'INTEGER', 'item_code': 'STRING', 'item_description': 'STRING', 'item_job': 'STRING', 'uom': 'STRING', 'shipment_amount': 'FLOAT', 'quantity': 'FLOAT', 'quantity_due': 'INTEGER', 'quantity_received': 'FLOAT', 'quantity_billed': 'FLOAT', 'need_by_date': 'DATETIME', 'line_closed_flag': 'STRING', 'line_closed_code': 'STRING', 'line_closed_date': 'DATETIME', 'line_cancel_flag': 'STRING', 'line_cancelled_by': 'FLOAT', 'line_cancel_date': 'DATETIME', 'line_attribute1': 'STRING', 'line_attribute2': 'STRING', 'line_attribute3': 'STRING', 'line_attribute4': 'STRING', 'line_attribute5': 'STRING', 'line_attribute6': 'STRING', 'line_attribute7': 'STRING', 'line_attribute8': 'STRING', 'line_attribute9': 'STRING', 'line_attribute10': 'STRING', 'line_attribute11': 'STRING', 'line_attribute12': 'STRING', 'line_attribute13': 'STRING', 'line_attribute14': 'STRING', 'line_attribute15': 'STRING', 'product_type': 'STRING', 'product_brand': 'STRING', 'creation_date': 'DATETIME', 'created_by': 'INTEGER', 'last_update_date': 'DATETIME', 'last_updated_by': 'INTEGER', 'ImportedAt': 'DATETIME'}\n",
      "Cluster Field List: []\n",
      "Date Field List: []\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "1#All Fields on BQ and DF are ok.\n",
      "==================================\n",
      "po_amount-FLOAT on existing schema is not them same as po_amount-INTEGER on loading schema.\n",
      "po_amount has been converted  to FLOAT \n",
      "payment_amount-FLOAT on existing schema is not them same as payment_amount-INTEGER on loading schema.\n",
      "payment_amount has been converted  to FLOAT \n",
      "quantity-FLOAT on existing schema is not them same as quantity-INTEGER on loading schema.\n",
      "quantity has been converted  to FLOAT \n",
      "quantity_received-FLOAT on existing schema is not them same as quantity_received-INTEGER on loading schema.\n",
      "quantity_received has been converted  to FLOAT \n",
      "quantity_billed-FLOAT on existing schema is not them same as quantity_billed-INTEGER on loading schema.\n",
      "quantity_billed has been converted  to FLOAT \n",
      "2#All Field Type on BQ and DF are the same.\n",
      "==================================\n",
      "3#Partition Name Fields on BQ and DF is ok.\n",
      "==================================\n",
      "4#All Cluster on BQ and DF are ok.\n",
      "==================================\n",
      "5#All Date Column on BQ and DF are ok.\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "isExistingTable=get_table()\n",
    "if isExistingTable:    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    bqSchema=table.schema\n",
    "    ExistingSchemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in bqSchema])\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All {len(ExistingSchemaDictNameType)} Fields as belows\")\n",
    "    print(ExistingSchemaDictNameType)\n",
    "    \n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {ExistingSchemaDictNameType}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    bqSchema,_=createBQSchemaByDF()\n",
    "    create_table(bqSchema)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 rows and 66 columns are about to be imported to BQ by dataframe\n",
      "po_header_id                 int64\n",
      "po_line_id                   int64\n",
      "org_id                       int64\n",
      "po_number                   object\n",
      "po_date             datetime64[ns]\n",
      "                         ...      \n",
      "creation_date       datetime64[ns]\n",
      "created_by                   int64\n",
      "last_update_date    datetime64[ns]\n",
      "last_updated_by              int64\n",
      "ImportedAt          datetime64[ns]\n",
      "Length: 66, dtype: object\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    no_rows=len(dfAll)\n",
    "    no_cols=len(dfAll.columns)\n",
    "   \n",
    "    if dfAll.empty==False:\n",
    "     if loading_from=='csv':\n",
    "        \n",
    "        dfAll.to_csv (temp_path,index=False)\n",
    "    print(f\"{no_rows} rows and {no_cols} columns are about to be imported to BQ by {loading_from}\")\n",
    "    print(dfAll.dtypes)\n",
    "    print(\"***********************************************************************************\")    \n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])  \n",
    "  logErrorMessage(listError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from dataframe\n",
      "Import data from yip_po_listing on Oracle into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:  \n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(listError)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=bqSchema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=bqSchema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config,)  \n",
    "\n",
    "\n",
    "    result_job=job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on Oracle into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  msg=f\"BigQuery Error While Ingesting data with {str(e)}\"  \n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,is_load_all,completely)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if len(listError)>0:\n",
    " is_loaded_completely=0\n",
    "else:\n",
    " is_loaded_completely=1\n",
    "\n",
    "#dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'is_load_all':is_load_all,'completely':is_loaded_completely}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984432-453f-4084-ba4b-8e502237dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26cc1-01f1-40ba-8c8e-de4bc7354caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e1e63-dff7-4566-b2d1-7979d0a8004b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
