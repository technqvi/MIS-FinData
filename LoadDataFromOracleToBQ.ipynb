{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Parameter variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name=''\n",
    "source_name=\"yip_invoice_monthly\" # df/csv\n",
    "\n",
    "#source_name='yip_po_listing'\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "# source_name='yip_ap_payment' # csv\n",
    "# source_name=\"yip_pj_status\" # csv\n",
    "# source_name='yip_gl_account'\n",
    "\n",
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=False\n",
    "is_py=False\n",
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "#init_date_query='2023-04-01 00:00:00'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd5f40-3498-4135-b832-458b82d945af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10' #'172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "\n",
    "host = 'mail.yipintsoi.com'\n",
    "port=  25\n",
    "sender=\"mis-bi-service@yipintsoigroup.com\"\n",
    "receivers=['pongthorn.sa@yipintsoi.com']\n",
    "#receivers=['pongthorn.sa@yipintsoi.com','mis-bi-service@yipintsoigroup.com']\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "temp_path=f'temp/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b81b25c9-9ca4-4a92-8a0b-56b744c1cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-12 23:21:27\n",
      "2023-06-12 23:21:27\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#dtStr_imported='2023-05-23 23:00:00'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] = f\"MIS-BI : ETL-Error on {source_name} at {dtStr_imported}\"\n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        \n",
    "        recordError=dfError.to_records(index=False)\n",
    "        add_logError(recordError)\n",
    "        \n",
    "        emailResult=sendMailForError(dfError.to_html(index=False))\n",
    "        \n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_invoice_monthly'  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                                  yip_invoice_monthly\n",
       "first_load_col                                  gl_date\n",
       "partition_date_col                              gl_date\n",
       "partition_date_type                               MONTH\n",
       "cluster_col_list       INVOICE_DATE ,gl_date ,dept_name\n",
       "date_col_list                      gl_date,invoice_date\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "ds_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Date Column as condition to load at first = gl_date\n",
      "Partition : gl_date - MONTH\n",
      "Cluster List: ['invoice_date', 'gl_date', 'dept_name']\n",
      "Date Cols: ['gl_date', 'invoice_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select etl_datetime,data_source_id from etl_transaction where data_source_id='yip_invoice_monthly' \n",
      "    order by etl_datetime desc limit 1\n",
      "    \n",
      "Empty DataFrame\n",
      "Columns: [etl_datetime, data_source_id]\n",
      "Index: []\n",
      "No etl transaction , we will get started with importing all items from :  2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isReLoadAll==True\n",
      "select * from yip_invoice_monthly   \n",
      "           where  gl_date>=to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') \n",
      "           \n"
     ]
    }
   ],
   "source": [
    "def loadData(isReLoadAll):\n",
    "    print(f\"isReLoadAll=={isReLoadAll}\")  \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "          \n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:    \n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine)\n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb737af-841a-46d5-aa18-7af6f48342a4",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a16a8bd7-4f2c-405a-b4bf-c96cab6d591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll['last_update_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "# _dateValueTest='2023-06-09 22:45:09'\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#=============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['customer_trx_id', 'org_id', 'gl_date', 'invoice_date', 'dept_name', 'dept_code', 'invoice_number', 'tax_invoice_number', 'so', 'project', 'rev_con', 'cn_number', 'cn_comment', 'invoice_type', 'invoice_type_description', 'cust_code', 'cust_name', 'industry_new', 'sector', 'abbrevation', 'sale_code', 'sale_name', 'invoice_amount', 'total_cost', 'margin_amount', 'margin_percent', 'salesforce_opp', 'line_description1', 'line_description2', 'line_description3', 'line_description4', 'line_description5', 'line_description6', 'line_description7', 'line_description8', 'line_description9', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22412 entries, 0 to 22411\n",
      "Data columns (total 41 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   customer_trx_id           22412 non-null  int64         \n",
      " 1   org_id                    22412 non-null  int64         \n",
      " 2   gl_date                   22412 non-null  datetime64[ns]\n",
      " 3   invoice_date              22412 non-null  datetime64[ns]\n",
      " 4   dept_name                 22412 non-null  object        \n",
      " 5   dept_code                 22412 non-null  object        \n",
      " 6   invoice_number            22412 non-null  object        \n",
      " 7   tax_invoice_number        15355 non-null  object        \n",
      " 8   so                        38 non-null     object        \n",
      " 9   project                   12116 non-null  object        \n",
      " 10  rev_con                   1 non-null      object        \n",
      " 11  cn_number                 603 non-null    object        \n",
      " 12  cn_comment                437 non-null    object        \n",
      " 13  invoice_type              22412 non-null  object        \n",
      " 14  invoice_type_description  22412 non-null  object        \n",
      " 15  cust_code                 22412 non-null  object        \n",
      " 16  cust_name                 22412 non-null  object        \n",
      " 17  industry_new              17466 non-null  object        \n",
      " 18  sector                    17466 non-null  object        \n",
      " 19  abbrevation               17466 non-null  object        \n",
      " 20  sale_code                 22412 non-null  int64         \n",
      " 21  sale_name                 22412 non-null  object        \n",
      " 22  invoice_amount            22412 non-null  float64       \n",
      " 23  total_cost                22412 non-null  float64       \n",
      " 24  margin_amount             22412 non-null  float64       \n",
      " 25  margin_percent            22412 non-null  float64       \n",
      " 26  salesforce_opp            685 non-null    object        \n",
      " 27  line_description1         22219 non-null  object        \n",
      " 28  line_description2         15442 non-null  object        \n",
      " 29  line_description3         13174 non-null  object        \n",
      " 30  line_description4         10692 non-null  object        \n",
      " 31  line_description5         7660 non-null   object        \n",
      " 32  line_description6         4748 non-null   object        \n",
      " 33  line_description7         2599 non-null   object        \n",
      " 34  line_description8         1679 non-null   object        \n",
      " 35  line_description9         842 non-null    object        \n",
      " 36  creation_date             22412 non-null  datetime64[ns]\n",
      " 37  created_by                22412 non-null  int64         \n",
      " 38  last_update_date          22412 non-null  datetime64[ns]\n",
      " 39  last_updated_by           22412 non-null  int64         \n",
      " 40  ImportedAt                22412 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](5), float64(4), int64(5), object(27)\n",
      "memory usage: 7.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_trx_id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>gl_date</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>dept_code</th>\n",
       "      <th>invoice_number</th>\n",
       "      <th>tax_invoice_number</th>\n",
       "      <th>so</th>\n",
       "      <th>project</th>\n",
       "      <th>...</th>\n",
       "      <th>line_description5</th>\n",
       "      <th>line_description6</th>\n",
       "      <th>line_description7</th>\n",
       "      <th>line_description8</th>\n",
       "      <th>line_description9</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>791916</td>\n",
       "      <td>81</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>312001180</td>\n",
       "      <td>312001180</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-10-14 00:00:00</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-08 13:55:03</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-12 23:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>835001</td>\n",
       "      <td>81</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>312200377</td>\n",
       "      <td>312200377</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-03-03 00:00:00</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-08 13:55:03</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-12 23:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>876304</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>312300076</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-05 09:59:05</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-08 13:55:03</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-12 23:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>793721</td>\n",
       "      <td>81</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>312001420</td>\n",
       "      <td>312001420</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-11-02 00:00:00</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-08 13:55:03</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-06-12 23:21:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>830290</td>\n",
       "      <td>81</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>312200037</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-01-06 09:27:20</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-08 13:55:03</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-06-12 23:21:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_trx_id  org_id    gl_date invoice_date dept_name dept_code   \n",
       "0           791916      81 2020-10-14   2020-10-14       MGT      9996  \\\n",
       "1           835001      81 2022-03-03   2022-03-03       MGT      9996   \n",
       "2           876304      81 2023-01-05   2023-01-05       MGT      9996   \n",
       "3           793721      81 2020-11-02   2020-11-02       MGT      9996   \n",
       "4           830290      81 2022-01-06   2022-01-06       MGT      9996   \n",
       "\n",
       "  invoice_number tax_invoice_number    so project  ... line_description5   \n",
       "0      312001180          312001180  None    None  ...              None  \\\n",
       "1      312200377          312200377  None    None  ...              None   \n",
       "2      312300076               None  None    None  ...              None   \n",
       "3      312001420          312001420  None    None  ...              None   \n",
       "4      312200037               None  None    None  ...              None   \n",
       "\n",
       "  line_description6 line_description7 line_description8 line_description9   \n",
       "0              None              None              None              None  \\\n",
       "1              None              None              None              None   \n",
       "2              None              None              None              None   \n",
       "3              None              None              None              None   \n",
       "4              None              None              None              None   \n",
       "\n",
       "        creation_date created_by    last_update_date last_updated_by   \n",
       "0 2020-10-14 00:00:00       1073 2023-06-08 13:55:03            1073  \\\n",
       "1 2022-03-03 00:00:00       1073 2023-06-08 13:55:03            1073   \n",
       "2 2023-01-05 09:59:05       1073 2023-06-08 13:55:03            1073   \n",
       "3 2020-11-02 00:00:00       1073 2023-06-08 13:55:03            1152   \n",
       "4 2022-01-06 09:27:20       1073 2023-06-08 13:55:03            1073   \n",
       "\n",
       "           ImportedAt  \n",
       "0 2023-06-12 23:21:27  \n",
       "1 2023-06-12 23:21:27  \n",
       "2 2023-06-12 23:21:27  \n",
       "3 2023-06-12 23:21:27  \n",
       "4 2023-06-12 23:21:27  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gl_date', 'invoice_date', 'dept_name'] is in dataframe from Oracel View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from Oracel View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from Oracel View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83c4a2b7-a91d-4e08-8885-28532591208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65f21f-dfed-407c-959a-97b7439e0ca9",
   "metadata": {},
   "source": [
    "## Creaste bigquery schema from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of field of the schema : 38\n",
      "[SchemaField('customer_trx_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('org_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('gl_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('invoice_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('dept_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('tax_invoice_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('so', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('rev_con', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cn_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cn_comment', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_type_description', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cust_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cust_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('industry_new', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('sector', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('abbrevation', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('sale_code', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('sale_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('total_cost', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('margin_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('margin_percent', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('salesforce_opp', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description1', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description2', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description3', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description4', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description5', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description6', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_description7', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('creation_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('created_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('last_update_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('last_updated_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ImportedAt', 'DATETIME', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "schema = []\n",
    "srCols=dfAll.dtypes\n",
    "try:\n",
    "    for name, type_name in srCols.items():\n",
    "        # print(name,type_name)\n",
    "        if str(type_name) in ['int32','int64']:\n",
    "          schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='float64':\n",
    "          schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='datetime64[ns]':\n",
    "          if name in   dateCols:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "          else:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) == 'bool':\n",
    "             schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "        else: # if not found type , it will be converted to STRING\n",
    "           schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "        \n",
    "    # if  len(schema)!=len(listColDF):\n",
    "    #    listFieldBQError=[field.name for field in schema]\n",
    "    #    intersec_DF_BQ = [set(listColDF).symmetric_difference(set(listFieldBQError))]\n",
    "    #    raise Exception(f\"{len(schema)}!={len(listColDF)} , {intersec_DF_BQ} in Dataframe can not be converted to Bigquery Data type\")\n",
    "\n",
    "except Exception as e:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "   logErrorMessage(listError)\n",
    "print(\"Total number of field of the schema :\",len(schema))    \n",
    "print(schema) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "## Check whether dataframe and bigquery schema are the same\n",
    "\n",
    "## Check Existing DataSet and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of Oracle\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Fields on BQ and DF are ok.\")\n",
    "        \n",
    "        \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"Partition Name Fields on BQ and DF is ok.\")    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Cluster on BQ and DF are ok.\")\n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Date Column on BQ and DF are ok.\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "# delete table and set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mismgntdata-bigquery.MIS_BI_DW.yip_invoice_monthly already exists.\n",
      "All Fields : ['customer_trx_id', 'org_id', 'gl_date', 'invoice_date', 'dept_name', 'dept_code', 'invoice_number', 'tax_invoice_number', 'so', 'project', 'rev_con', 'cn_number', 'cn_comment', 'invoice_type', 'invoice_type_description', 'cust_code', 'cust_name', 'industry_new', 'sector', 'abbrevation', 'sale_code', 'sale_name', 'invoice_amount', 'total_cost', 'margin_amount', 'margin_percent', 'salesforce_opp', 'line_description1', 'line_description2', 'line_description3', 'line_description4', 'line_description5', 'line_description6', 'line_description7', 'line_description8', 'line_description9', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "Partiton Field&Type: gl_date - MONTH\n",
      "Cluster Field List: ['invoice_date', 'gl_date', 'dept_name']\n",
      "Date Field List: ['gl_date', 'invoice_date']\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "Partition Name Fields on BQ and DF is ok.\n",
      "All Cluster on BQ and DF are ok.\n",
      "All Date Column on BQ and DF are ok.\n",
      "yip_invoice_monthly ETL at 2023-06-12 23:03:25 raise some errors.\n",
      "Total log error=1\n",
      "        error_datetime         etl_datetime       data_source_id   \n",
      "0  2023-06-12 23:03:36  2023-06-12 23:03:25  yip_invoice_monthly  \\\n",
      "\n",
      "                                             message  \n",
      "0  Columns: ['line_description9', 'dept_code', 'l...  \n",
      "Done Log Error\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "yip_invoice_monthly ETL at 2023-06-12 23:03:25 raise some errors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCluster Field List: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclusterBQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate Field List: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdateTypeBQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mcheck_same_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     create_table()\n",
      "Cell \u001b[1;32mIn[46], line 53\u001b[0m, in \u001b[0;36mcheck_same_schema\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Date Column on BQ and DF are ok.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(listError)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mlogErrorMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlistError\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 50\u001b[0m, in \u001b[0;36mlogErrorMessage\u001b[1;34m(errorList, raise_ex)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#         emailResult=sendMailForError(dfError.to_html(index=False))\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#         error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m raise_ex\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m          \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;167;01mException\u001b[39;00m(error_message)\n",
      "\u001b[1;31mException\u001b[0m: yip_invoice_monthly ETL at 2023-06-12 23:03:25 raise some errors."
     ]
    }
   ],
   "source": [
    "if check_table():    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All Fields : {listFieldBQ}\")\n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {partitionTypeBQ}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    create_table()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac562df-e933-4563-96f8-6db9af10ad1d",
   "metadata": {},
   "source": [
    "# To load data into BQ , technically you need  to save it as CSV file first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "64c5b1f9-fe6e-4751-8125-2c2d209873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.iloc[:45501,:]  poListing\n",
    "# #dfAll=dfAll.drop(45502) # error  # for po listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dataframe to csv as source first\n",
      "1196 rows and 41 are about to be imported to BQ by csv\n"
     ]
    }
   ],
   "source": [
    "loading_from='csv' # cvs/dataframe\n",
    "no_rows=len(dfAll)\n",
    "no_cols=len(dfAll.columns)\n",
    "if dfAll.empty==False and loading_from=='csv':\n",
    "    print(\"Save dataframe to csv as source first\")\n",
    "    dfAll.to_csv (temp_path,index=False)\n",
    "print(f\"{no_rows} rows and {no_cols} are about to be imported to BQ by {loading_from}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from csv\n",
      "Import data from yip_invoice_monthly on Oracle into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:\n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(logErrorMessage)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=schema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=schema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config)  \n",
    "\n",
    "\n",
    "    result_job=job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on Oracle into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"BQ Error and Raise Exception\")  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,no_rows,is_load_all)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if isLoadingAllItems==True:\n",
    "    is_load_all=1\n",
    "else:\n",
    "    is_load_all=0\n",
    "\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted temp/yip_invoice_monthly.csv\n"
     ]
    }
   ],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc1aaf-bfc3-466c-aa61-7684f7f351c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefaa201-b2ce-4364-ade5-a01aae9d9f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b6223-edd9-4758-9081-054597dc7980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
