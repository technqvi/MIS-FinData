{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "#https://github.com/technqvi/SMart-AI/blob/main/LoadIncident_PostgresToBQ.ipynb\n",
    "#https://github.com/technqvi/AlertPriceInRange/blob/master/price_range_notification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Init constant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_name=\"yip_ar_receipt\"\n",
    "source_name=\"yip_invoice_monthly\"\n",
    "init_date_query='2020-01-01'\n",
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=False\n",
    "\n",
    "listError=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "temp_path=f'temp/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "b81b25c9-9ca4-4a92-8a0b-56b744c1cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-23 23:00:00\n",
      "2023-05-23 23:00:00\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#dtStr_imported='2023-05-23 23:00:00'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f042ef-0a73-42e0-8f4a-d8abb35c86e4",
   "metadata": {},
   "source": [
    "# Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList):\n",
    "    def logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            logErrorMessage(listError)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(dfError)\n",
    "        logError(dfError.to_records(index=False))\n",
    "        \n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        \n",
    "        # send email to admin\n",
    "        raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_invoice_monthly'  \n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Column to load at first = invoice_date\n",
      "invoice_date - MONTH\n",
      "['dept_name', 'cust_name']\n",
      "['gl_date', 'invoice_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    print(f\"Column to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"{partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     print(\"No cluster cols\")   \n",
    "     clusterCols=[]\n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     print(clusterCols)\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     print(\"No Date cols\")   \n",
    "     dateCols=[]\n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     print(dateCols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select etl_datetime,data_source_id from etl_transaction where data_source_id='yip_invoice_monthly' \n",
      "    order by etl_datetime desc limit 1\n",
      "    \n",
      "          etl_datetime       data_source_id\n",
      "0  2023-05-22 23:00:00  yip_invoice_monthly\n",
      "Start Import on update_at of last ETL date :  2023-05-22 23:00:00\n"
     ]
    }
   ],
   "source": [
    "def get_last_etl_by_ds(data_source):\n",
    "    \n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "\n",
    "dfLastETL=get_last_etl_by_ds(source_name)\n",
    "if dfLastETL.empty==False:\n",
    "  start_date_query=dfLastETL.iloc[0,0]\n",
    "  print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "else:\n",
    "   isLoadingAllItems=True \n",
    "   start_date_query=init_date_query\n",
    "   print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from yip_invoice_monthly   \n",
      "           where  last_update_date>=to_date('2023-05-22 23:00:00','yyyy-mm-dd hh24:mi:ss') \n"
     ]
    }
   ],
   "source": [
    "def loadData(isReLoadAll):\n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd') \n",
    "           \"\"\"    \n",
    "       else:    \n",
    "           sql =f\"\"\"select * from {source_name}   \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "\n",
    "       print(sql)\n",
    "       dfAll = pd.read_sql(sql, engine)\n",
    "       dfAll['ImportedAt']=dt_imported \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_trx_id', 'org_id', 'gl_date', 'invoice_date', 'dept_name', 'dept_code', 'invoice_number', 'tax_invoice_number', 'so', 'project', 'rev_con', 'cn_number', 'cn_comment', 'invoice_type', 'cust_code', 'cust_name', 'industry_new', 'sector', 'abbrevation', 'sale_code', 'sale_name', 'invoice_amount', 'total_cost', 'margin_amount', 'margin_percent', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 180 entries, 0 to 179\n",
      "Data columns (total 30 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   customer_trx_id     180 non-null    int64         \n",
      " 1   org_id              180 non-null    int64         \n",
      " 2   gl_date             180 non-null    datetime64[ns]\n",
      " 3   invoice_date        180 non-null    datetime64[ns]\n",
      " 4   dept_name           180 non-null    object        \n",
      " 5   dept_code           180 non-null    object        \n",
      " 6   invoice_number      180 non-null    object        \n",
      " 7   tax_invoice_number  33 non-null     object        \n",
      " 8   so                  0 non-null      object        \n",
      " 9   project             134 non-null    object        \n",
      " 10  rev_con             0 non-null      object        \n",
      " 11  cn_number           1 non-null      object        \n",
      " 12  cn_comment          0 non-null      object        \n",
      " 13  invoice_type        180 non-null    object        \n",
      " 14  cust_code           180 non-null    object        \n",
      " 15  cust_name           180 non-null    object        \n",
      " 16  industry_new        167 non-null    object        \n",
      " 17  sector              167 non-null    object        \n",
      " 18  abbrevation         167 non-null    object        \n",
      " 19  sale_code           180 non-null    int64         \n",
      " 20  sale_name           180 non-null    object        \n",
      " 21  invoice_amount      180 non-null    float64       \n",
      " 22  total_cost          180 non-null    float64       \n",
      " 23  margin_amount       180 non-null    float64       \n",
      " 24  margin_percent      180 non-null    float64       \n",
      " 25  creation_date       180 non-null    datetime64[ns]\n",
      " 26  created_by          180 non-null    int64         \n",
      " 27  last_update_date    180 non-null    datetime64[ns]\n",
      " 28  last_updated_by     180 non-null    int64         \n",
      " 29  ImportedAt          180 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](5), float64(4), int64(5), object(16)\n",
      "memory usage: 42.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_trx_id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>gl_date</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>dept_code</th>\n",
       "      <th>invoice_number</th>\n",
       "      <th>tax_invoice_number</th>\n",
       "      <th>so</th>\n",
       "      <th>project</th>\n",
       "      <th>...</th>\n",
       "      <th>sale_name</th>\n",
       "      <th>invoice_amount</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>margin_amount</th>\n",
       "      <th>margin_percent</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308122</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>ES</td>\n",
       "      <td>0301</td>\n",
       "      <td>222300629</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>221344</td>\n",
       "      <td>...</td>\n",
       "      <td>Yanumat  Veerabongkotmanee, Miss</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>800000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-03-07 10:29:30</td>\n",
       "      <td>1071</td>\n",
       "      <td>2023-05-23 22:45:07</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-23 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2189128</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>ES</td>\n",
       "      <td>0305</td>\n",
       "      <td>222301393</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>230135</td>\n",
       "      <td>...</td>\n",
       "      <td>ARPAPORN PROMSUWAN, Miss</td>\n",
       "      <td>490000.0</td>\n",
       "      <td>441000.00</td>\n",
       "      <td>49000.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2023-05-22 10:03:11</td>\n",
       "      <td>1071</td>\n",
       "      <td>2023-05-23 22:45:07</td>\n",
       "      <td>1071</td>\n",
       "      <td>2023-05-23 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2210126</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>2023-05-23</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9901</td>\n",
       "      <td>322300291</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Admin-Accounting, Miss</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>200.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2023-05-23 11:29:21</td>\n",
       "      <td>3072</td>\n",
       "      <td>2023-05-23 22:45:07</td>\n",
       "      <td>3072</td>\n",
       "      <td>2023-05-23 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1869127</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>322300223</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Admin-Office, Mrs.</td>\n",
       "      <td>2733.0</td>\n",
       "      <td>2924.31</td>\n",
       "      <td>-191.31</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2023-04-19 09:53:15</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-05-23 22:45:07</td>\n",
       "      <td>3072</td>\n",
       "      <td>2023-05-23 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988179</td>\n",
       "      <td>81</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>MGT</td>\n",
       "      <td>9996</td>\n",
       "      <td>322300249</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>Admin-Office, Mrs.</td>\n",
       "      <td>28237.5</td>\n",
       "      <td>30214.13</td>\n",
       "      <td>-1976.63</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>2023-05-01 14:29:26</td>\n",
       "      <td>1073</td>\n",
       "      <td>2023-05-23 22:45:07</td>\n",
       "      <td>3072</td>\n",
       "      <td>2023-05-23 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_trx_id  org_id    gl_date invoice_date dept_name dept_code   \n",
       "0          1308122      81 2023-03-07   2023-03-07        ES      0301  \\\n",
       "1          2189128      81 2023-05-22   2023-05-22        ES      0305   \n",
       "2          2210126      81 2023-05-23   2023-05-23       MGT      9901   \n",
       "3          1869127      81 2023-04-30   2023-04-30       MGT      9996   \n",
       "4          1988179      81 2023-05-01   2023-05-01       MGT      9996   \n",
       "\n",
       "  invoice_number tax_invoice_number    so project  ...   \n",
       "0      222300629               None  None  221344  ...  \\\n",
       "1      222301393               None  None  230135  ...   \n",
       "2      322300291               None  None    None  ...   \n",
       "3      322300223               None  None    None  ...   \n",
       "4      322300249               None  None    None  ...   \n",
       "\n",
       "                          sale_name invoice_amount total_cost margin_amount   \n",
       "0  Yanumat  Veerabongkotmanee, Miss       800000.0  800000.00          0.00  \\\n",
       "1          ARPAPORN PROMSUWAN, Miss       490000.0  441000.00      49000.00   \n",
       "2            Admin-Accounting, Miss          200.0       0.00        200.00   \n",
       "3                Admin-Office, Mrs.         2733.0    2924.31       -191.31   \n",
       "4                Admin-Office, Mrs.        28237.5   30214.13      -1976.63   \n",
       "\n",
       "  margin_percent       creation_date created_by    last_update_date   \n",
       "0            0.0 2023-03-07 10:29:30       1071 2023-05-23 22:45:07  \\\n",
       "1           10.0 2023-05-22 10:03:11       1071 2023-05-23 22:45:07   \n",
       "2          100.0 2023-05-23 11:29:21       3072 2023-05-23 22:45:07   \n",
       "3           -7.0 2023-04-19 09:53:15       1073 2023-05-23 22:45:07   \n",
       "4           -7.0 2023-05-01 14:29:26       1073 2023-05-23 22:45:07   \n",
       "\n",
       "  last_updated_by          ImportedAt  \n",
       "0            1152 2023-05-23 23:00:00  \n",
       "1            1071 2023-05-23 23:00:00  \n",
       "2            3072 2023-05-23 23:00:00  \n",
       "3            3072 2023-05-23 23:00:00  \n",
       "4            3072 2023-05-23 23:00:00  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfAll=dfAll.drop(columns=['receipt_number','method_name','application_type']) # receipt\n",
    "#dfAll=dfAll.drop(columns=['customer_trx_id','org_id']) # invoice\n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "83c4a2b7-a91d-4e08-8885-28532591208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65f21f-dfed-407c-959a-97b7439e0ca9",
   "metadata": {},
   "source": [
    "## Creaste bigquery schema from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchemaField('customer_trx_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('org_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('gl_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('invoice_date', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('dept_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('dept_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('tax_invoice_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('so', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('rev_con', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cn_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cn_comment', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cust_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cust_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('industry_new', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('sector', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('abbrevation', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('sale_code', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('sale_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('total_cost', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('margin_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('margin_percent', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('creation_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('created_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('last_update_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('last_updated_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ImportedAt', 'DATETIME', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "\n",
    "schema = []\n",
    "srCols=dfAll.dtypes\n",
    "for name, type_name in srCols.items():\n",
    "    # print(name,type_name)\n",
    "    if str(type_name) in ['int32','int64']:\n",
    "      schema.append(bigquery.SchemaField(name, \"INTEGER\"))\n",
    "    elif str(type_name) =='float64':\n",
    "      schema.append(bigquery.SchemaField(name, \"FLOAT\"))\n",
    "    elif str(type_name) =='datetime64[ns]':\n",
    "      if name in   dateCols:\n",
    "         schema.append(bigquery.SchemaField(name,  \"DATE\"))\n",
    "      else:\n",
    "         schema.append(bigquery.SchemaField(name,  \"DATETIME\"))\n",
    "    else:\n",
    "       schema.append(bigquery.SchemaField(name,  \"STRING\")) \n",
    "      \n",
    "print(schema)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "## Check whether dataframe and bigquery schema are the same\n",
    "\n",
    "## Check Existing DataSet and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "\n",
    "except Exception as ex:\n",
    "    raise(\"Dataset {} is not found\".format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    table = bigquery.Table(table_id,schema=schema)\n",
    "    if  partitionCol!=\"\":\n",
    "     table.time_partitioning = bigquery.TimePartitioning(\n",
    "     type_=partitionType,field=partitionCol)\n",
    "    \n",
    "    if len(clusterCols)>0:\n",
    "     table.clustering_fields = clusterCols\n",
    "\n",
    "    table = client.create_table(table) \n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "\n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are the same on BigQuery and View {source_name} \"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Fields on BQ and DF are ok.\")\n",
    "        \n",
    "        \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is not the same as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"Partition Name Fields on BQ and DF is ok.\")    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is not the same as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are the same on BigQuery defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Cluster on BQ and DF are ok.\")\n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are the same on BigQuery defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Date Column on BQ and DF are ok.\")\n",
    "\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        # delete table\n",
    "        # set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mismgntdata-bigquery.MIS_BI_DW.yip_invoice_monthly already exists.\n",
      "All Fields : ['customer_trx_id', 'org_id', 'gl_date', 'invoice_date', 'dept_name', 'dept_code', 'invoice_number', 'tax_invoice_number', 'so', 'project', 'rev_con', 'cn_number', 'cn_comment', 'invoice_type', 'cust_code', 'cust_name', 'industry_new', 'sector', 'abbrevation', 'sale_code', 'sale_name', 'invoice_amount', 'total_cost', 'margin_amount', 'margin_percent', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "Partiton Field&Type: invoice_date - MONTH\n",
      "Cluster Field List: ['dept_name', 'cust_name']\n",
      "Date Field List: ['gl_date', 'invoice_date']\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "All Fields on BQ and DF are ok.\n",
      "Partition Name Fields on BQ and DF is ok.\n",
      "All Cluster on BQ and DF are ok.\n",
      "All Date Column on BQ and DF are ok.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# table    \n",
    "try:\n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    # if no table it will call create_table\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    \n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "    clusterBQ=table.clustering_fields\n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All Fields : {listFieldBQ}\")\n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {partitionTypeBQ}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    #check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ)\n",
    "    check_same_schema()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as ex:\n",
    "    create_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac562df-e933-4563-96f8-6db9af10ad1d",
   "metadata": {},
   "source": [
    "# To load data into BQ , technically you need  to save it as CSV file first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 rows are about to be imported to BQ\n"
     ]
    }
   ],
   "source": [
    "if dfAll.empty==False:\n",
    "    no_rows=len(dfAll)\n",
    "    print(f\"{no_rows} rows are about to be imported to BQ\")\n",
    "    dfAll.to_csv (temp_path,index=False)\n",
    "else:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 180 rows and 30 columns to mismgntdata-bigquery.MIS_BI_DW.yip_invoice_monthly\n"
     ]
    }
   ],
   "source": [
    "# if isLoadingAllItems==False:\n",
    "# print(\"Load with appending\")\n",
    "\n",
    "# Addtional Try Error\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "    autodetect=False,write_disposition=\"WRITE_APPEND\"\n",
    "    )\n",
    "# else:\n",
    "#     print(\"Load with all data\")\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "#         autodetect=False,write_disposition=\"WRITE_TRUNCATE\"\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "with open(temp_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "job.result()  # Waits for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        no_rows, len(table.schema), table_id\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,no_rows,is_load_all)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except sqlite3.Error as error:\n",
    "        print(\"Failed to insert etl_transaction table\", error)\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if isLoadingAllItems==True:\n",
    "    is_load_all=1\n",
    "else:\n",
    "    is_load_all=0\n",
    "\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted temp/yip_invoice_monthly.csv\n"
     ]
    }
   ],
   "source": [
    "#Addtional Try Error\n",
    "if os.path.exists(temp_path):\n",
    "  os.remove(temp_path)\n",
    "  print(f\"Deleted {temp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "de3e6c80-0ead-4bc2-a08d-e1b9ed2bc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any error , send mail to adminstrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc1aaf-bfc3-466c-aa61-7684f7f351c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
