{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3192bc4-6924-4e88-9fc1-981c816f918c",
   "metadata": {},
   "source": [
    "# Time To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bf1c7f-c8b9-4017-9b06-629aa83d885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-03 17:11:43\n",
      "2023-08-03 17:11:43\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "# dtStr_imported='2023-06-13 17:11:54'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)\n",
    "\n",
    "\n",
    "env_path=r'.env'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Parameter variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=True\n",
    "is_py=False\n",
    "\n",
    "source_name=''\n",
    "\n",
    "#source_name='yit_ar_aging_with_cost'\n",
    "#source_name='yip_wip_project'\n",
    "source_name='yip_bg_account'\n",
    "\n",
    "#source_name=\"yip_invoice_monthly\" # df\n",
    "\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "#source_name='yip_ap_payment' # csv\n",
    "\n",
    "#source_name=\"yip_pj_status\" # csv\n",
    "\n",
    "# source_name='yip_po_listing' # df\n",
    "\n",
    "#source_name='yip_gl_account' # df\n",
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "# init_date_query='2023-01-01 00:00:00'\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "\n",
    "csv_temp_folder='csv_temp'\n",
    "csv_error_folder='csv_error'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25b842-861c-4543-b7bf-abe21d688752",
   "metadata": {},
   "source": [
    "# Enter parameter on script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f80fe-b80e-42fb-98cb-cff290f5c031",
   "metadata": {},
   "source": [
    "# Check temp and error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21674004-5a60-4ce3-bead-dfc1f8d5457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_temp_folder)==False:\n",
    "  os.mkdir(csv_temp_folder)\n",
    "if os.path.exists(csv_error_folder)==False:\n",
    "  os.mkdir(csv_error_folder)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Const Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab69b3d-b0bb-4e97-a8a2-cffdbdcd01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "\n",
    "temp_path=f'{csv_temp_folder}/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'\n",
    "etlDateCols=[updateCol,'creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4168f-99a0-4a5f-b43e-5cf68b8cc6a2",
   "metadata": {},
   "source": [
    "# Email Nofification &  Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorSubject,errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] =errorSubject \n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34cb43ec-1209-4920-921f-bda3d5fbf1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_error_file(): # if any error ,then move csv to investigte later\n",
    "    error_csv_path=''\n",
    "    try:\n",
    "        if os.path.exists(temp_path):\n",
    "         error_csv_file=f\"{source_name}_error_{ dt_imported.strftime('%d%m%y_%H%M%S')}.csv\"   \n",
    "         new_temp_path=f'{csv_temp_folder}/{error_csv_file}'\n",
    "         os.rename(temp_path,new_temp_path)  \n",
    "\n",
    "         error_csv_path=f'{csv_error_folder}/{error_csv_file}'\n",
    "         shutil.move(new_temp_path,error_csv_path )\n",
    "         \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "    return   error_csv_path      \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    \n",
    "    def add_error_to_file(error_des):\n",
    "        f = open(r'log_error.txt', 'a')\n",
    "        error_str = f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")}|{repr(error_des)}\\n'\n",
    "\n",
    "        f.write(error_str)\n",
    "        f.close()\n",
    "        print(error_str)\n",
    "        raise Exception(error_str)\n",
    "    \n",
    "    def add_logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg=f\"{data_base_file} error : {str(e)}\"\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            add_error_to_file(msg)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        \n",
    "        error_message=f\"ETL Error on {source_name} at {dtStr_imported}\"\n",
    "        # move csv error file to examine later\n",
    "        error_csv_path  =move_error_file()\n",
    "        if error_csv_path!='':\n",
    "            error_message= f\"{error_csv_path} - {error_message}\"\n",
    "        error_message=f\"MIS-BI : {error_message}\"   \n",
    "        \n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(f\"Total log error={len(dfError)}\")\n",
    "        print(dfError)\n",
    "        \n",
    "        # add to error table into sqlite\n",
    "        recordError=dfError.to_records(index=False) \n",
    "        add_logError(recordError)\n",
    "        \n",
    "        # sened main\n",
    "        emailResult=sendMailForError(error_message, dfError.to_html(index=False))\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f267b-2d60-462c-8f44-f22834dfa5b2",
   "metadata": {},
   "source": [
    "# Read Credential Config from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0af655e-f711-4b9e-af7b-a854ea13addb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red all credential confgi values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    config = dotenv_values(dotenv_path=env_path)\n",
    "    projectId=config['PROJECT_ID']\n",
    "    region=config['REGION']\n",
    "    dataset_id=config['DATASET_ID']\n",
    "    table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "    \n",
    "\n",
    "    host = config['MAIL_IP']\n",
    "    port=  int(config['MAIL_PORT'])\n",
    "    sender=config['MAIL_SENDER']\n",
    "    receivers=[config['MAIL_RECEIVER']]\n",
    "    \n",
    "    \n",
    "    # print(f\"{projectId}-{region}-{dataset_id}-{table_id}\")\n",
    "    # print(f\"{host}-{port}-{sender}-{receivers}\")\n",
    "    print(\"Red all credential confgi values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=\"Not found .env file or invalid key in .env file\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef4c71-3905-4a5c-9ee0-c520d86f3450",
   "metadata": {},
   "source": [
    "# Setting BigQuery and Check DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2cddaf3-a25c-449a-9c96-9c8678a61cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "# client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "client = bigquery.Client(project=projectId)\n",
    "\n",
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get data view as data source and data store of the data view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_bg_account'  \n",
      "id                          yip_bg_account\n",
      "first_load_col                  START_DATE\n",
      "partition_date_col              START_DATE\n",
      "partition_date_type                  MONTH\n",
      "cluster_col_list                START_DATE\n",
      "date_col_list          START_DATE,END_DATE\n",
      "load_from_type                   dataframe\n",
      "datastore_id                             1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)\n",
    "print(ds_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45fef9c2-b9f0-4c10-bd2a-3823acafe8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_store where id=1  \n",
      "oracle_yip\n",
      "Red all credential confgi values sucessfully.\n"
     ]
    }
   ],
   "source": [
    "# get data from data_store\n",
    "def get_data_store(datastore_id):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_store where id={datastore_id}  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        \n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "datastore=get_data_store(ds_item[\"datastore_id\"])\n",
    "print(datastore['data_store_name'])\n",
    "\n",
    "try:\n",
    "    _ip=datastore['database_ip']\n",
    "    _hostname=datastore['database_host']\n",
    "    _port=datastore['database_port']\n",
    "    _servicename=datastore['databases_service_name']\n",
    "    _username=datastore['databases_user']\n",
    "    _password=datastore['databases_password']\n",
    "    # print(f\"{_ip}#{_hostname}#{_port}#{_servicename}#{_username}#{_password}\")\n",
    "    \n",
    "    print(\"Red all credential confgi values sucessfully.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    msg=f\"Not found data store  {datastore['data_store_name']}\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str( msg)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Load data into BigQuery from dataframe\n",
      "Date Column as condition to load at first = start_date\n",
      "Partition : start_date - MONTH\n",
      "Cluster List: ['start_date']\n",
      "Date Cols: ['start_date', 'end_date']\n",
      "All Reuired Date Cols\n",
      "['start_date', 'end_date', 'last_update_date', 'creation_date']\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    loading_from=ds_item['load_from_type']\n",
    "    print(f\"Load data into BigQuery from {loading_from}\")\n",
    "    \n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(f\"Cluster List: {clusterCols}\")\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(f\"Date Cols: {dateCols}\")\n",
    "\n",
    "else:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,f\"Not found view {source_name} in data_source table\"])\n",
    "   logErrorMessage(listError)\n",
    "    \n",
    "dateColsToConvert=[partitionCol]+dateCols+etlDateCols\n",
    "dateColsToConvert = list(dict.fromkeys(dateColsToConvert))\n",
    "print(\"All Reuired Date Cols\")\n",
    "print(dateColsToConvert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload all data:  2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "\n",
    "if isLoadingAllItems==True:\n",
    " is_load_all=1\n",
    "else:\n",
    " is_load_all=0   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5997314-6afb-478c-adf1-6e328cab8556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from yip_bg_account   \n",
      "           where  start_date>=to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') \n",
      "           \n",
      "isReLoadAll==True==1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loadData(isReLoadAll):\n",
    "   \n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \n",
    "           \"\"\"    \n",
    "       else:   \n",
    "\n",
    "           sql =f\"\"\"select * from {source_name}  \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "            \n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine,parse_dates=dateColsToConvert)\n",
    "       print(f\"isReLoadAll=={isReLoadAll}=={is_load_all}\")  \n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb737af-841a-46d5-aa18-7af6f48342a4",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a16a8bd7-4f2c-405a-b4bf-c96cab6d591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll['last_update_date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "309e1218-7a7b-4320-84e7-d7275a3be29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.drop(columns=['line_description8','line_description9'  ,'dept_code'])\n",
    "# _dateColTest='last_update_date'\n",
    "\n",
    "# dfAll=dfAll.query(f\"{_dateColTest}<@_dateValueTest\")\n",
    "\n",
    "# _dateValueTest='2023-06-13 17:11:54'\n",
    "# dt_imported=datetime.strptime(_dateValueTest,'%Y-%m-%d %H:%M:%S')\n",
    "# dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9cc50-90a5-4f10-bbce-c9cfea3d658e",
   "metadata": {},
   "source": [
    "# Transform Dataframe prior to Ingesting it to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List columns of DF\n",
      "['ledger_id', 'ledger_name', 'code_combination_id', 'account_type', 'actual_flag', 'period_year', 'period_num', 'period_name', 'start_date', 'end_date', 'segment1', 'segment2', 'segment3', 'segment4', 'segment5', 'account_code', 'description', 'period_net_dr', 'period_net_cr', 'period_net', 'begining_balance', 'ending_balance', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126302 entries, 0 to 126301\n",
      "Data columns (total 27 columns):\n",
      " #   Column               Non-Null Count   Dtype         \n",
      "---  ------               --------------   -----         \n",
      " 0   ledger_id            126302 non-null  int64         \n",
      " 1   ledger_name          126302 non-null  object        \n",
      " 2   code_combination_id  126302 non-null  int64         \n",
      " 3   account_type         126302 non-null  object        \n",
      " 4   actual_flag          126302 non-null  object        \n",
      " 5   period_year          126302 non-null  int64         \n",
      " 6   period_num           126302 non-null  int64         \n",
      " 7   period_name          126302 non-null  object        \n",
      " 8   start_date           126302 non-null  datetime64[ns]\n",
      " 9   end_date             126302 non-null  datetime64[ns]\n",
      " 10  segment1             126302 non-null  object        \n",
      " 11  segment2             126302 non-null  object        \n",
      " 12  segment3             126302 non-null  object        \n",
      " 13  segment4             126302 non-null  object        \n",
      " 14  segment5             126302 non-null  object        \n",
      " 15  account_code         126302 non-null  object        \n",
      " 16  description          126302 non-null  object        \n",
      " 17  period_net_dr        126302 non-null  float64       \n",
      " 18  period_net_cr        126302 non-null  float64       \n",
      " 19  period_net           126302 non-null  float64       \n",
      " 20  begining_balance     126302 non-null  float64       \n",
      " 21  ending_balance       126302 non-null  float64       \n",
      " 22  creation_date        126302 non-null  datetime64[ns]\n",
      " 23  created_by           126302 non-null  int64         \n",
      " 24  last_update_date     126302 non-null  datetime64[ns]\n",
      " 25  last_updated_by      126302 non-null  int64         \n",
      " 26  ImportedAt           126302 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](5), float64(5), int64(6), object(11)\n",
      "memory usage: 26.0+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ledger_id</th>\n",
       "      <th>ledger_name</th>\n",
       "      <th>code_combination_id</th>\n",
       "      <th>account_type</th>\n",
       "      <th>actual_flag</th>\n",
       "      <th>period_year</th>\n",
       "      <th>period_num</th>\n",
       "      <th>period_name</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>...</th>\n",
       "      <th>period_net_dr</th>\n",
       "      <th>period_net_cr</th>\n",
       "      <th>period_net</th>\n",
       "      <th>begining_balance</th>\n",
       "      <th>ending_balance</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1188</td>\n",
       "      <td>LFY Set of Book</td>\n",
       "      <td>1650441</td>\n",
       "      <td>E</td>\n",
       "      <td>B</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>MAY-20</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>4330.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4330.12</td>\n",
       "      <td>11433.57</td>\n",
       "      <td>15763.69</td>\n",
       "      <td>2020-09-18 16:25:15</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-07-26 12:44:46</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-08-03 17:11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1188</td>\n",
       "      <td>LFY Set of Book</td>\n",
       "      <td>1650494</td>\n",
       "      <td>E</td>\n",
       "      <td>B</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>MAY-20</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50800.64</td>\n",
       "      <td>50800.64</td>\n",
       "      <td>2020-09-18 16:25:15</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-07-26 12:44:46</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-08-03 17:11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1188</td>\n",
       "      <td>LFY Set of Book</td>\n",
       "      <td>1650505</td>\n",
       "      <td>R</td>\n",
       "      <td>B</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>MAY-20</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36664.49</td>\n",
       "      <td>-36664.49</td>\n",
       "      <td>-13575.94</td>\n",
       "      <td>-50240.43</td>\n",
       "      <td>2020-09-21 15:38:18</td>\n",
       "      <td>1523</td>\n",
       "      <td>2023-07-26 12:44:46</td>\n",
       "      <td>1523</td>\n",
       "      <td>2023-08-03 17:11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1188</td>\n",
       "      <td>LFY Set of Book</td>\n",
       "      <td>1650508</td>\n",
       "      <td>E</td>\n",
       "      <td>B</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>MAY-20</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>3480.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3480.00</td>\n",
       "      <td>6679.00</td>\n",
       "      <td>10159.00</td>\n",
       "      <td>2020-09-18 16:25:15</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-07-26 12:44:46</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-08-03 17:11:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1188</td>\n",
       "      <td>LFY Set of Book</td>\n",
       "      <td>1660946</td>\n",
       "      <td>E</td>\n",
       "      <td>B</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>MAY-20</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2020-05-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5649.88</td>\n",
       "      <td>5649.88</td>\n",
       "      <td>2020-09-18 16:25:15</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-07-26 12:44:46</td>\n",
       "      <td>2043</td>\n",
       "      <td>2023-08-03 17:11:43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ledger_id      ledger_name  code_combination_id account_type actual_flag   \n",
       "0       1188  LFY Set of Book              1650441            E           B  \\\n",
       "1       1188  LFY Set of Book              1650494            E           B   \n",
       "2       1188  LFY Set of Book              1650505            R           B   \n",
       "3       1188  LFY Set of Book              1650508            E           B   \n",
       "4       1188  LFY Set of Book              1660946            E           B   \n",
       "\n",
       "   period_year  period_num period_name start_date   end_date  ...   \n",
       "0         2020           5      MAY-20 2020-05-01 2020-05-31  ...  \\\n",
       "1         2020           5      MAY-20 2020-05-01 2020-05-31  ...   \n",
       "2         2020           5      MAY-20 2020-05-01 2020-05-31  ...   \n",
       "3         2020           5      MAY-20 2020-05-01 2020-05-31  ...   \n",
       "4         2020           5      MAY-20 2020-05-01 2020-05-31  ...   \n",
       "\n",
       "  period_net_dr period_net_cr period_net begining_balance ending_balance   \n",
       "0       4330.12          0.00    4330.12         11433.57       15763.69  \\\n",
       "1          0.00          0.00       0.00         50800.64       50800.64   \n",
       "2          0.00      36664.49  -36664.49        -13575.94      -50240.43   \n",
       "3       3480.00          0.00    3480.00          6679.00       10159.00   \n",
       "4          0.00          0.00       0.00          5649.88        5649.88   \n",
       "\n",
       "        creation_date created_by    last_update_date  last_updated_by   \n",
       "0 2020-09-18 16:25:15       2043 2023-07-26 12:44:46             2043  \\\n",
       "1 2020-09-18 16:25:15       2043 2023-07-26 12:44:46             2043   \n",
       "2 2020-09-21 15:38:18       1523 2023-07-26 12:44:46             1523   \n",
       "3 2020-09-18 16:25:15       2043 2023-07-26 12:44:46             2043   \n",
       "4 2020-09-18 16:25:15       2043 2023-07-26 12:44:46             2043   \n",
       "\n",
       "           ImportedAt  \n",
       "0 2023-08-03 17:11:43  \n",
       "1 2023-08-03 17:11:43  \n",
       "2 2023-08-03 17:11:43  \n",
       "3 2023-08-03 17:11:43  \n",
       "4 2023-08-03 17:11:43  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll['ImportedAt']=dt_imported \n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(\"List columns of DF\")\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e7a3e0f-e334-4358-ae97-e44c1185b5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea988bcc-812f-44b6-bafa-0a16c55e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start_date', 'end_date'] is in dataframe from Oracel View\n"
     ]
    }
   ],
   "source": [
    "listColAdminConfig=[colFirstLoad,partitionCol]\n",
    "if len(clusterCols)>0:\n",
    " listColAdminConfig.extend(clusterCols)\n",
    "if len(dateCols)>0:\n",
    " listColAdminConfig.extend(dateCols)\n",
    "listColAdminConfig=list(dict.fromkeys(listColAdminConfig))\n",
    "\n",
    "checkSomeNotExistingDF = [ col   for col in listColAdminConfig if col not in listColDF ]\n",
    "if len(checkSomeNotExistingDF)>0:\n",
    "    msg=f\"Some columns in data source are not in dataframe from Oracel View = {checkSomeNotExistingDF }\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    logErrorMessage(listError)\n",
    "else:\n",
    "    print(f\"{listColAdminConfig} is in dataframe from Oracel View\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "# BigQuery Schema Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBQSchemaByDF():\n",
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "    schema = []\n",
    "    srCols=dfAll.dtypes\n",
    "    try:\n",
    "        for name, type_name in srCols.items():\n",
    "            # print(name,type_name)\n",
    "            if str(type_name) in ['int32','int64']:\n",
    "              schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='float64':\n",
    "              schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) =='datetime64[ns]':\n",
    "              if name in   dateCols:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "              else:\n",
    "                 schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "            elif str(type_name) == 'bool':\n",
    "                 schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "            else: # if not found type , it will be converted to STRING\n",
    "               schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "\n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    schemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in schema])\n",
    "    \n",
    "    return  schema,schemaDictNameType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pbpython.com/pandas_dtypes.html\n",
    "      #https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html  \n",
    "      #https://datatofish.com/strings-to-datetime-pandas/ \n",
    "def convert_dfSchema_same_bqSChema(bqName,bqType): \n",
    "  try:  \n",
    "    if bqType in [\"INTEGER\",\"FLOAT\"]:\n",
    "        try:\n",
    "            if  bqType==\"INTEGER\":\n",
    "                dfAll[bqName]=dfAll[bqName].astype('int')\n",
    "            else:  \n",
    "                dfAll[bqName]=dfAll[bqName].astype('float')\n",
    "        except Exception as e:\n",
    "            print(f\"Extra : {bqName}-{bqType} has been converted by pd.to_numeric\")\n",
    "            dfAll[bqName]=pd.to_numeric(dfAll[bqName], errors='coerce')\n",
    "    elif bqType in [\"DATE\",\"DATETIME\"]:\n",
    "      dfAll[bqName]=pd.to_datetime(dfAll[bqName], errors='coerce',exact=False)   \n",
    "    elif bqType==\"BOOL\":\n",
    "      dfAll[bqName]=dfAll[bqName].astype('bool')\n",
    "    else:\n",
    "      dfAll[bqName]=dfAll[bqName].apply(lambda x: str(x))  \n",
    "    \n",
    "    print(f\"{bqName} has been converted  to {bqType} \")\n",
    "    \n",
    "  except Exception as ex:\n",
    "    raise ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(schema):\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f5bd90-c595-426d-ad6b-49de1f19b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table():\n",
    "    try:\n",
    "        client.get_table(table_id)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of Oracle\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"1#All Fields on BQ and DF are ok.\")  \n",
    "        print(\"==================================\")\n",
    "        \n",
    "    try:\n",
    "        __,SchemaDictNameType=createBQSchemaByDF()\n",
    "\n",
    "        for key_name, val_filed_type in ExistingSchemaDictNameType.items():\n",
    "            val2=SchemaDictNameType[key_name] \n",
    "            if  val_filed_type!=val2:\n",
    "              msg=f\"{key_name}-{val_filed_type} on existing schema is not them same as {key_name}-{val2} on loading schema.\"\n",
    "              print(msg)\n",
    "              convert_dfSchema_same_bqSChema( key_name,val_filed_type)\n",
    "        print(\"2#All Field Type on BQ and DF are the same.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        msg=f'{key_name} name on the loading schema does not exists  in the existing schema on Bigquery, so the system is unable to check field type matching.'\n",
    "        print(msg)\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "       \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"3#Partition Name Fields on BQ and DF is ok.\")    \n",
    "        print(\"==================================\" )    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"4#All Cluster on BQ and DF are ok.\")\n",
    "        print(\"==================================\")      \n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"5#All Date Column on BQ and DF are ok.\")\n",
    "        print(\"==================================\")\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        \n",
    "# delete table and set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new table mismgntdata-bigquery.MIS_BI_DW.yip_bg_account\n"
     ]
    }
   ],
   "source": [
    "isExistingTable=get_table()\n",
    "if isExistingTable:    \n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    bqSchema=table.schema\n",
    "    ExistingSchemaDictNameType=dict([(f'{fieldInfo.name}',f'{fieldInfo.field_type}' )  for fieldInfo in bqSchema])\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All {len(ExistingSchemaDictNameType)} Fields as belows\")\n",
    "    print(ExistingSchemaDictNameType)\n",
    "    \n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {ExistingSchemaDictNameType}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    check_same_schema()\n",
    "else:\n",
    "    bqSchema,_=createBQSchemaByDF()\n",
    "    create_table(bqSchema)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126302 rows and 27 columns are about to be imported to BQ by dataframe\n",
      "ledger_id                       int64\n",
      "ledger_name                    object\n",
      "code_combination_id             int64\n",
      "account_type                   object\n",
      "actual_flag                    object\n",
      "period_year                     int64\n",
      "period_num                      int64\n",
      "period_name                    object\n",
      "start_date             datetime64[ns]\n",
      "end_date               datetime64[ns]\n",
      "segment1                       object\n",
      "segment2                       object\n",
      "segment3                       object\n",
      "segment4                       object\n",
      "segment5                       object\n",
      "account_code                   object\n",
      "description                    object\n",
      "period_net_dr                 float64\n",
      "period_net_cr                 float64\n",
      "period_net                    float64\n",
      "begining_balance              float64\n",
      "ending_balance                float64\n",
      "creation_date          datetime64[ns]\n",
      "created_by                      int64\n",
      "last_update_date       datetime64[ns]\n",
      "last_updated_by                 int64\n",
      "ImportedAt             datetime64[ns]\n",
      "dtype: object\n",
      "***********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    no_rows=len(dfAll)\n",
    "    no_cols=len(dfAll.columns)\n",
    "   \n",
    "    if dfAll.empty==False:\n",
    "     if loading_from=='csv':\n",
    "        \n",
    "        dfAll.to_csv (temp_path,index=False)\n",
    "    print(f\"{no_rows} rows and {no_cols} columns are about to be imported to BQ by {loading_from}\")\n",
    "    print(dfAll.dtypes)\n",
    "    print(\"***********************************************************************************\")    \n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])  \n",
    "  logErrorMessage(listError)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from dataframe\n",
      "Import data from yip_bg_account on Oracle into BQ successfully.\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:  \n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "    if   len(listError)>0:\n",
    "     logErrorMessage(listError,False)\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=bqSchema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=bqSchema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config,)  \n",
    "\n",
    "\n",
    "    result_job=job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "    \n",
    "    print(f\"Import data from {source_name} on Oracle into BQ successfully.\")\n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  msg=f\"BigQuery Error While Ingesting data with {str(e)}\"  \n",
    "  listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,is_load_all,completely)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if len(listError)>0:\n",
    " is_loaded_completely=0\n",
    "else:\n",
    " is_loaded_completely=1\n",
    "\n",
    "#dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'is_load_all':is_load_all,'completely':is_loaded_completely}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69984432-453f-4084-ba4b-8e502237dba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa26cc1-01f1-40ba-8c8e-de4bc7354caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e1e63-dff7-4566-b2d1-7979d0a8004b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
