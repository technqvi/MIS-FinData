{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "import sys \n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Parameter variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name=''\n",
    "source_name='yip_po_listing'\n",
    "#source_name=\"yip_ar_receipt\"  # df/csv\n",
    "# source_name='yip_ap_payment' # csv\n",
    "# source_name=\"yip_invoice_monthly\" # df/csv\n",
    "#source_name=\"yip_pj_status\" # csv\n",
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=True\n",
    "is_py=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd5f40-3498-4135-b832-458b82d945af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "3995ac09-578e-4cd8-add6-27bb013e9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        source_name=sys.argv[1]\n",
    "\n",
    "        if sys.argv[2]=='0':\n",
    "         isLoadingAllItems=False\n",
    "        elif sys.argv[2]=='1'  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")\n",
    "\n",
    "        ok=True \n",
    "\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        source_name = input(\"View Table Name : \")\n",
    "        source_name=source_name.lower()\n",
    "        load_option= int(input(\"Loading All Data option (1=True | 0=False): \"))\n",
    "        if load_option==0:\n",
    "         isLoadingAllItems=False\n",
    "        elif load_option==1  :\n",
    "         isLoadingAllItems=True\n",
    "        else:\n",
    "            raise Exception(\"isLoadingAllItems 1=True | 0=False\")  \n",
    "\n",
    "        print(f\"Confirm to Load view = {source_name} and Load All Data= {isLoadingAllItems}\")\n",
    "        press_Y=input(f\"Press Y=True But any key=False : \") \n",
    "        if press_Y=='Y':\n",
    "         ok=True\n",
    "\n",
    "    if ok==False:\n",
    "        print(\"No any action\")\n",
    "        quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f93d9-f490-49de-8b12-1867f2143d84",
   "metadata": {},
   "source": [
    "# Init Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "init_date_query='2020-01-01 00:00:00'\n",
    "\n",
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10' #'172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "\n",
    "host = 'mail.yipintsoi.com'\n",
    "port=  25\n",
    "sender=\"mis-bi-service@yipintsoigroup.com\"\n",
    "receivers=['pongthorn.sa@yipintsoi.com']\n",
    "#receivers=['pongthorn.sa@yipintsoi.com','mis-bi-service@yipintsoigroup.com']\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "temp_path=f'temp/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "b81b25c9-9ca4-4a92-8a0b-56b744c1cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-03 17:02:08\n",
      "2023-06-03 17:02:08\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#dtStr_imported='2023-05-23 23:00:00'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f042ef-0a73-42e0-8f4a-d8abb35c86e4",
   "metadata": {},
   "source": [
    "# Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "ed7217f0-9d25-4f42-9cfe-cdec23416d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def sendMailForError(errorHtml):\n",
    "    message = MIMEMultipart(\"alternative\")\n",
    "    message[\"Subject\"] = f\"MIS-BI : ETL-Error on {source_name} at {dtStr_imported}\"\n",
    "    message[\"From\"] = sender\n",
    "    message[\"To\"] = ','.join(receivers)\n",
    "\n",
    "    html =errorHtml \n",
    "\n",
    "    part_html = MIMEText(html, \"html\")\n",
    "    message.attach(part_html)\n",
    "\n",
    "    try:\n",
    "\n",
    "        with smtplib.SMTP(host,port) as mail_server:\n",
    "            #mail_server.login(login, password)\n",
    "            mail_server.sendmail(sender, receivers, message.as_string())\n",
    "            print(\"Successfully sent email\")\n",
    "\n",
    "    except (gaierror, ConnectionRefusedError):\n",
    "       msg='Failed to connect to the server. Bad connection settings?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPServerDisconnected:\n",
    "       msg='Failed to connect to the server. Wrong user/password?'\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "       logErrorMessage(listError)\n",
    "    except smtplib.SMTPException as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList,raise_ex=True):\n",
    "    def logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            logErrorMessage(listError)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(dfError)\n",
    "        \n",
    "        logError(dfError.to_records(index=False))\n",
    "        \n",
    "        #emailResult=sendMailForError(dfError.to_html(index=False))\n",
    "        \n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        \n",
    "        if raise_ex==True:\n",
    "         raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_po_listing'  \n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Date Column as condition to load at first = po_date\n",
      "Partition : po_date - MONTH\n",
      "[] (No cluster cols)\n",
      "[] (No Date cols)\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    colFirstLoad=colFirstLoad.strip().lower()\n",
    "    print(f\"Date Column as condition to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    partitionCol=partitionCol.strip().lower()\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     clusterCols = list(map(str.lower,clusterCols))   \n",
    "     print(clusterCols)\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     dateCols = list(map(str.lower,dateCols))      \n",
    "     print(dateCols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload all data:  2020-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from yip_po_listing  \n",
      "       where  last_update_date>=to_date('2020-01-01 00:00:00','yyyy-mm-dd hh24:mi:ss') \n"
     ]
    }
   ],
   "source": [
    "def loadData(isReLoadAll):\n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       # if isReLoadAll==True:\n",
    "       #   sql=f\"\"\"select * from {source_name}   \n",
    "       #     where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd') \n",
    "       #     \"\"\"    \n",
    "       # else:    \n",
    "       sql =f\"\"\"select * from {source_name}  \n",
    "       where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "       print(sql)\n",
    "       \n",
    "       dfAll = pd.read_sql(sql, engine)\n",
    "       dfAll['ImportedAt']=dt_imported \n",
    "    \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['po_header_id', 'po_line_id', 'org_id', 'po_number', 'po_date', 'comments', 'vendor_name', 'vendor_site_code', 'currency_code', 'rate_type', 'rate_type_name', 'rate_date', 'rate', 'buyer', 'authorization_status', 'revision_number', 'revised_date', 'approved_flag', 'approved_date', 'closed_date', 'cancel_flag', 'closed_code', 'inv_organization_code', 'inv_organization_name', 'po_amount', 'payment_amount', 'line_number', 'shipment_number', 'item_code', 'item_description', 'item_job', 'uom', 'shipment_amount', 'quantity', 'quantity_due', 'quantity_received', 'quantity_billed', 'need_by_date', 'line_closed_flag', 'line_closed_code', 'line_closed_date', 'line_cancel_flag', 'line_cancelled_by', 'line_cancel_date', 'line_attribute1', 'line_attribute2', 'line_attribute3', 'line_attribute4', 'line_attribute5', 'line_attribute6', 'line_attribute7', 'line_attribute8', 'line_attribute9', 'line_attribute10', 'line_attribute11', 'line_attribute12', 'line_attribute13', 'line_attribute14', 'line_attribute15', 'product_type', 'product_brand', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157913 entries, 0 to 157912\n",
      "Data columns (total 66 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   po_header_id           157913 non-null  int64         \n",
      " 1   po_line_id             157913 non-null  int64         \n",
      " 2   org_id                 157913 non-null  int64         \n",
      " 3   po_number              157913 non-null  object        \n",
      " 4   po_date                157913 non-null  datetime64[ns]\n",
      " 5   comments               141406 non-null  object        \n",
      " 6   vendor_name            157913 non-null  object        \n",
      " 7   vendor_site_code       157913 non-null  object        \n",
      " 8   currency_code          157913 non-null  object        \n",
      " 9   rate_type              11202 non-null   object        \n",
      " 10  rate_type_name         11202 non-null   object        \n",
      " 11  rate_date              157913 non-null  datetime64[ns]\n",
      " 12  rate                   11202 non-null   float64       \n",
      " 13  buyer                  157913 non-null  object        \n",
      " 14  authorization_status   157648 non-null  object        \n",
      " 15  revision_number        157913 non-null  int64         \n",
      " 16  revised_date           16470 non-null   datetime64[ns]\n",
      " 17  approved_flag          157668 non-null  object        \n",
      " 18  approved_date          157066 non-null  datetime64[ns]\n",
      " 19  closed_date            145760 non-null  datetime64[ns]\n",
      " 20  cancel_flag            9686 non-null    object        \n",
      " 21  closed_code            157842 non-null  object        \n",
      " 22  inv_organization_code  157913 non-null  object        \n",
      " 23  inv_organization_name  157913 non-null  object        \n",
      " 24  po_amount              157913 non-null  float64       \n",
      " 25  payment_amount         157913 non-null  float64       \n",
      " 26  line_number            157913 non-null  int64         \n",
      " 27  shipment_number        157913 non-null  int64         \n",
      " 28  item_code              157913 non-null  object        \n",
      " 29  item_description       157913 non-null  object        \n",
      " 30  item_job               62972 non-null   object        \n",
      " 31  uom                    157913 non-null  object        \n",
      " 32  shipment_amount        157913 non-null  float64       \n",
      " 33  quantity               157913 non-null  float64       \n",
      " 34  quantity_due           157913 non-null  int64         \n",
      " 35  quantity_received      157913 non-null  float64       \n",
      " 36  quantity_billed        157913 non-null  float64       \n",
      " 37  need_by_date           151263 non-null  object        \n",
      " 38  line_closed_flag       3072 non-null    object        \n",
      " 39  line_closed_code       157617 non-null  object        \n",
      " 40  line_closed_date       149906 non-null  datetime64[ns]\n",
      " 41  line_cancel_flag       12963 non-null   object        \n",
      " 42  line_cancelled_by      7470 non-null    float64       \n",
      " 43  line_cancel_date       7470 non-null    datetime64[ns]\n",
      " 44  line_attribute1        1529 non-null    object        \n",
      " 45  line_attribute2        92564 non-null   object        \n",
      " 46  line_attribute3        62972 non-null   object        \n",
      " 47  line_attribute4        50392 non-null   object        \n",
      " 48  line_attribute5        25292 non-null   object        \n",
      " 49  line_attribute6        14868 non-null   object        \n",
      " 50  line_attribute7        9192 non-null    object        \n",
      " 51  line_attribute8        7001 non-null    object        \n",
      " 52  line_attribute9        5041 non-null    object        \n",
      " 53  line_attribute10       3908 non-null    object        \n",
      " 54  line_attribute11       3187 non-null    object        \n",
      " 55  line_attribute12       2706 non-null    object        \n",
      " 56  line_attribute13       2381 non-null    object        \n",
      " 57  line_attribute14       2081 non-null    object        \n",
      " 58  line_attribute15       1800 non-null    object        \n",
      " 59  product_type           0 non-null       object        \n",
      " 60  product_brand          0 non-null       object        \n",
      " 61  creation_date          157913 non-null  datetime64[ns]\n",
      " 62  created_by             157913 non-null  int64         \n",
      " 63  last_update_date       157913 non-null  datetime64[ns]\n",
      " 64  last_updated_by        157913 non-null  int64         \n",
      " 65  ImportedAt             157913 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](10), float64(8), int64(9), object(39)\n",
      "memory usage: 79.5+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>po_header_id</th>\n",
       "      <th>po_line_id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>po_number</th>\n",
       "      <th>po_date</th>\n",
       "      <th>comments</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>vendor_site_code</th>\n",
       "      <th>currency_code</th>\n",
       "      <th>rate_type</th>\n",
       "      <th>...</th>\n",
       "      <th>line_attribute13</th>\n",
       "      <th>line_attribute14</th>\n",
       "      <th>line_attribute15</th>\n",
       "      <th>product_type</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4310</td>\n",
       "      <td>7312</td>\n",
       "      <td>81</td>\n",
       "      <td>112051592</td>\n",
       "      <td>2005-07-08 09:19:54</td>\n",
       "      <td>9996-BLM เครื่องใช้สำนักงาน</td>\n",
       "      <td>บริษัท ไทยพัฒน์ ซัพพลาย จำกัด</td>\n",
       "      <td>TCCL</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-07-08 09:19:54</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-28 15:41:34</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-06-03 17:02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4461</td>\n",
       "      <td>7538</td>\n",
       "      <td>81</td>\n",
       "      <td>112051646</td>\n",
       "      <td>2005-07-18 10:38:15</td>\n",
       "      <td>0201-B SALES แฟ้มปกขาวใช้ที่แผนก BS</td>\n",
       "      <td>บริษัท ไทยพัฒน์ ซัพพลาย จำกัด</td>\n",
       "      <td>TCCL</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-07-18 10:38:15</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-28 15:41:34</td>\n",
       "      <td>1151</td>\n",
       "      <td>2023-06-03 17:02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4957</td>\n",
       "      <td>8313</td>\n",
       "      <td>81</td>\n",
       "      <td>112051805</td>\n",
       "      <td>2005-08-08 09:29:33</td>\n",
       "      <td>9996-BLM  เครื่องใช้สำนักงาน</td>\n",
       "      <td>บริษัท ไทยพัฒน์ ซัพพลาย จำกัด</td>\n",
       "      <td>TCCL</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-08-08 09:29:33</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-28 15:41:34</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-06-03 17:02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4869</td>\n",
       "      <td>8173</td>\n",
       "      <td>81</td>\n",
       "      <td>112051770</td>\n",
       "      <td>2005-08-03 11:22:24</td>\n",
       "      <td>0399 ES Adm. HP TONER สำหรับใช้ในแผนก</td>\n",
       "      <td>บริษัท เมโทรซิสเต็มส์คอร์ปอเรชั่น จำกัด (มหาชน)</td>\n",
       "      <td>YIP</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-08-03 11:22:24</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-28 15:41:34</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-06-03 17:02:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4998</td>\n",
       "      <td>8374</td>\n",
       "      <td>81</td>\n",
       "      <td>112051832</td>\n",
       "      <td>2005-08-09 15:38:03</td>\n",
       "      <td>9996-BLM เครื่องใช้สำนักงาน</td>\n",
       "      <td>บริษัท ตากอรุณสิน จำกัด</td>\n",
       "      <td>YIP</td>\n",
       "      <td>THB</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-08-09 15:38:03</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-05-28 15:41:34</td>\n",
       "      <td>1152</td>\n",
       "      <td>2023-06-03 17:02:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   po_header_id  po_line_id  org_id  po_number             po_date   \n",
       "0          4310        7312      81  112051592 2005-07-08 09:19:54  \\\n",
       "1          4461        7538      81  112051646 2005-07-18 10:38:15   \n",
       "2          4957        8313      81  112051805 2005-08-08 09:29:33   \n",
       "3          4869        8173      81  112051770 2005-08-03 11:22:24   \n",
       "4          4998        8374      81  112051832 2005-08-09 15:38:03   \n",
       "\n",
       "                                comments   \n",
       "0            9996-BLM เครื่องใช้สำนักงาน  \\\n",
       "1    0201-B SALES แฟ้มปกขาวใช้ที่แผนก BS   \n",
       "2           9996-BLM  เครื่องใช้สำนักงาน   \n",
       "3  0399 ES Adm. HP TONER สำหรับใช้ในแผนก   \n",
       "4            9996-BLM เครื่องใช้สำนักงาน   \n",
       "\n",
       "                                       vendor_name vendor_site_code   \n",
       "0                    บริษัท ไทยพัฒน์ ซัพพลาย จำกัด             TCCL  \\\n",
       "1                    บริษัท ไทยพัฒน์ ซัพพลาย จำกัด             TCCL   \n",
       "2                    บริษัท ไทยพัฒน์ ซัพพลาย จำกัด             TCCL   \n",
       "3  บริษัท เมโทรซิสเต็มส์คอร์ปอเรชั่น จำกัด (มหาชน)              YIP   \n",
       "4                          บริษัท ตากอรุณสิน จำกัด              YIP   \n",
       "\n",
       "  currency_code rate_type  ... line_attribute13 line_attribute14   \n",
       "0           THB      None  ...             None             None  \\\n",
       "1           THB      None  ...             None             None   \n",
       "2           THB      None  ...             None             None   \n",
       "3           THB      None  ...             None             None   \n",
       "4           THB      None  ...             None             None   \n",
       "\n",
       "   line_attribute15 product_type product_brand       creation_date created_by   \n",
       "0              None         None          None 2005-07-08 09:19:54       1152  \\\n",
       "1              None         None          None 2005-07-18 10:38:15       1152   \n",
       "2              None         None          None 2005-08-08 09:29:33       1152   \n",
       "3              None         None          None 2005-08-03 11:22:24       1152   \n",
       "4              None         None          None 2005-08-09 15:38:03       1152   \n",
       "\n",
       "     last_update_date last_updated_by          ImportedAt  \n",
       "0 2023-05-28 15:41:34            1152 2023-06-03 17:02:08  \n",
       "1 2023-05-28 15:41:34            1151 2023-06-03 17:02:08  \n",
       "2 2023-05-28 15:41:34            1152 2023-06-03 17:02:08  \n",
       "3 2023-05-28 15:41:34            1152 2023-06-03 17:02:08  \n",
       "4 2023-05-28 15:41:34            1152 2023-06-03 17:02:08  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfAll=dfAll.drop(columns=['receipt_number','method_name','application_type']) # receipt\n",
    "#dfAll=dfAll.drop(columns=['customer_trx_id','org_id']) # invoice\n",
    "\n",
    "# dfAll=dfAll.drop(columns=['product_type','product_brand'])\n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "83c4a2b7-a91d-4e08-8885-28532591208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65f21f-dfed-407c-959a-97b7439e0ca9",
   "metadata": {},
   "source": [
    "## Creaste bigquery schema from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of field of the schema : 66\n",
      "[SchemaField('po_header_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('po_line_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('org_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('po_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('po_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('comments', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('vendor_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('vendor_site_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('currency_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('rate_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('rate_type_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('rate_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('rate', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('buyer', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('authorization_status', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('revision_number', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('revised_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('approved_flag', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('approved_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('closed_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('cancel_flag', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('closed_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('inv_organization_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('inv_organization_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('po_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('payment_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('line_number', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('shipment_number', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('item_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('item_description', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('item_job', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('uom', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('shipment_amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('quantity', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('quantity_due', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('quantity_received', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('quantity_billed', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('need_by_date', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_closed_flag', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_closed_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_closed_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('line_cancel_flag', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_cancelled_by', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('line_cancel_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('line_attribute1', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute2', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute3', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute4', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute5', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute6', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute7', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute8', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute9', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute10', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute11', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute12', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute13', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute14', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('line_attribute15', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('product_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('product_brand', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('creation_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('created_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('last_update_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('last_updated_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ImportedAt', 'DATETIME', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "#https://cloud.google.com/bigquery/docs/schemas\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "schema = []\n",
    "srCols=dfAll.dtypes\n",
    "try:\n",
    "    for name, type_name in srCols.items():\n",
    "        # print(name,type_name)\n",
    "        if str(type_name) in ['int32','int64']:\n",
    "          schema.append(bigquery.SchemaField(name, \"INTEGER\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='float64':\n",
    "          schema.append(bigquery.SchemaField(name, \"FLOAT\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) =='datetime64[ns]':\n",
    "          if name in   dateCols:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATE\", mode=\"NULLABLE\"))\n",
    "          else:\n",
    "             schema.append(bigquery.SchemaField(name,  \"DATETIME\", mode=\"NULLABLE\"))\n",
    "        elif str(type_name) == 'bool':\n",
    "             schema.append(bigquery.SchemaField(name,  \"BOOL\", mode=\"NULLABLE\"))\n",
    "        else: # if not found type , it will be converted to STRING\n",
    "           schema.append(bigquery.SchemaField(name,  \"STRING\", mode=\"NULLABLE\")) \n",
    "        \n",
    "    # if  len(schema)!=len(listColDF):\n",
    "    #    listFieldBQError=[field.name for field in schema]\n",
    "    #    intersec_DF_BQ = [set(listColDF).symmetric_difference(set(listFieldBQError))]\n",
    "    #    raise Exception(f\"{len(schema)}!={len(listColDF)} , {intersec_DF_BQ} in Dataframe can not be converted to Bigquery Data type\")\n",
    "\n",
    "except Exception as e:\n",
    "   listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "   logErrorMessage(listError)\n",
    "print(\"Total number of field of the schema :\",len(schema))    \n",
    "print(schema) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "## Check whether dataframe and bigquery schema are the same\n",
    "\n",
    "## Check Existing DataSet and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except Exception as ex:\n",
    "    msg=f\"Dataset {dataset_id} is not found\"\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg]) \n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    try:  \n",
    "        table = bigquery.Table(table_id,schema=schema)\n",
    "        if  partitionCol!=\"\":\n",
    "         table.time_partitioning = bigquery.TimePartitioning(\n",
    "         type_=partitionType,field=partitionCol)\n",
    "\n",
    "        if len(clusterCols)>0:\n",
    "         table.clustering_fields = clusterCols\n",
    "\n",
    "        table = client.create_table(table) \n",
    "        print(\n",
    "            \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "        logErrorMessage(listError)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are NOT THE SAME on BigQuery and ViewTable {source_name} of Oracle\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Fields on BQ and DF are ok.\")\n",
    "        \n",
    "        \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is NOT THE SAME as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"Partition Name Fields on BQ and DF is ok.\")    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is NOT THE SAME as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Cluster on BQ and DF are ok.\")\n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are NOT THE SAME on BigQuery and ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Date Column on BQ and DF are ok.\")\n",
    "\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        # delete table\n",
    "        # set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new table mismgntdata-bigquery.MIS_BI_DW.yip_po_listing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# try to create table    \n",
    "try:\n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    # if no table it will call create_table\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All Fields : {listFieldBQ}\")\n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {partitionTypeBQ}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    #check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ)\n",
    "    check_same_schema()\n",
    "        \n",
    "except Exception as ex:\n",
    "    #print(\"Don't create table\")\n",
    "    create_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac562df-e933-4563-96f8-6db9af10ad1d",
   "metadata": {},
   "source": [
    "# To load data into BQ , technically you need  to save it as CSV file first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "64c5b1f9-fe6e-4751-8125-2c2d209873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll=dfAll.iloc[:45501,:]  poListing\n",
    "# #dfAll=dfAll.drop(45502) # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dataframe to csv as source first\n",
      "157913 rows and 66 are about to be imported to BQ by csv\n"
     ]
    }
   ],
   "source": [
    "loading_from='csv' # cvs/dataframe\n",
    "no_rows=len(dfAll)\n",
    "no_cols=len(dfAll.columns)\n",
    "if dfAll.empty==False and loading_from=='csv':\n",
    "    print(\"Save dataframe to csv as source first\")\n",
    "    dfAll.to_csv (temp_path,index=False)\n",
    "print(f\"{no_rows} rows and {no_cols} are about to be imported to BQ by {loading_from}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "4b0722a7-a889-435a-a3fb-e74529edaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectBQError(x_job):\n",
    " if x_job.errors is not None:\n",
    "    for error in x_job.errors:\n",
    "      msg=f\"{error['reason']} - {error['message']}\"\n",
    "      listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,msg])\n",
    "      logErrorMessage(listError,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load bulk data from csv\n",
      "yip_po_listing ETL at 2023-06-03 17:02:08 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing  \\\n",
      "\n",
      "                                             message  \n",
      "0  invalid - Error while reading data, error mess...  \n",
      "Done Log Error\n",
      "yip_po_listing ETL at 2023-06-03 17:02:08 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing  \\\n",
      "1  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "\n",
      "                                             message  \n",
      "0  invalid - Error while reading data, error mess...  \n",
      "1  invalid - Error while reading data, error mess...  \n",
      "Done Log Error\n",
      "yip_po_listing ETL at 2023-06-03 17:02:08 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing  \\\n",
      "1  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "2  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "\n",
      "                                             message  \n",
      "0  invalid - Error while reading data, error mess...  \n",
      "1  invalid - Error while reading data, error mess...  \n",
      "2  invalid - Error while reading data, error mess...  \n",
      "Done Log Error\n",
      "yip_po_listing ETL at 2023-06-03 17:02:08 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing  \\\n",
      "1  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "2  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "3  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "\n",
      "                                             message  \n",
      "0  invalid - Error while reading data, error mess...  \n",
      "1  invalid - Error while reading data, error mess...  \n",
      "2  invalid - Error while reading data, error mess...  \n",
      "3  invalid - Error while reading data, error mess...  \n",
      "Done Log Error\n",
      "yip_po_listing ETL at 2023-06-03 17:02:08 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing  \\\n",
      "1  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "2  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "3  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "4  2023-06-03 17:03:42  2023-06-03 17:02:08  yip_po_listing   \n",
      "\n",
      "                                             message  \n",
      "0  invalid - Error while reading data, error mess...  \n",
      "1  invalid - Error while reading data, error mess...  \n",
      "2  invalid - Error while reading data, error mess...  \n",
      "3  invalid - Error while reading data, error mess...  \n",
      "4  invalid - Error while reading data, error mess...  \n",
      "Done Log Error\n"
     ]
    }
   ],
   "source": [
    "# cannot auto detect because some column , there are Y,N,R  For R BQ is known as Bool\n",
    "#https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    print(f\"Load bulk data from {loading_from}\")\n",
    "    if loading_from=='csv' :\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "            schema=schema,autodetect=False,\n",
    "            max_bad_records=(no_rows-1),\n",
    "            # autodetect=True,\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            )\n",
    "        with open(temp_path, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    else :\n",
    "        # job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")  # ok for POs Listing\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\",schema=schema)\n",
    "        job = client.load_table_from_dataframe(dfAll, table_id, job_config=job_config)  \n",
    "\n",
    "\n",
    "    job.result()  # Waits for the job to complete.\n",
    "    \n",
    "    # error but continue\n",
    "    collectBQError(job)   \n",
    "   \n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "  print(\"BQ Error and Raise Exception\")  \n",
    "  collectBQError(job)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,no_rows,is_load_all)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "        logErrorMessage(listError)\n",
    "        print(\"Failed to insert etl_transaction table\", str(e))\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if isLoadingAllItems==True:\n",
    "    is_load_all=1\n",
    "else:\n",
    "    is_load_all=0\n",
    "\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted temp/yip_po_listing.csv\n"
     ]
    }
   ],
   "source": [
    "#Addtional Try Error\n",
    "try:\n",
    "    if os.path.exists(temp_path):\n",
    "      os.remove(temp_path)\n",
    "      print(f\"Deleted {temp_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    logErrorMessage(listError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc1aaf-bfc3-466c-aa61-7684f7f351c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefaa201-b2ce-4364-ade5-a01aae9d9f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b6223-edd9-4758-9081-054597dc7980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
