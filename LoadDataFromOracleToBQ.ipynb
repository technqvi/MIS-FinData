{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ee8ceb-2e4e-4bf2-a571-acd97c73a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "import os\n",
    "\n",
    "import cx_Oracle\n",
    "import sqlalchemy\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "#https://github.com/technqvi/SMart-AI/blob/main/LoadIncident_PostgresToBQ.ipynb\n",
    "#https://github.com/technqvi/AlertPriceInRange/blob/master/price_range_notification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14e6355-af8c-46d7-8cbe-6f6fdd0000df",
   "metadata": {},
   "source": [
    "# Init constant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8121238b-8a75-46d6-8886-d1fc88880f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name=\"yip_ar_receipt\"\n",
    "#source_name=\"yip_invoice_monthly\"\n",
    "init_date_query='2020-01-01'\n",
    "# set True whatever , you want to reload all items\n",
    "isLoadingAllItems=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5850bfd-8dd0-4bb2-bbc2-8f704ee0a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "listError=[]\n",
    "\n",
    "projectId='mismgntdata-bigquery'\n",
    "region='asia-southeast1'\n",
    "dataset_id='MIS_BI_DW'\n",
    "table_id = f\"{projectId}.{dataset_id}.{source_name}\"\n",
    "\n",
    "_ip='172.30.57.10'\n",
    "_hostname='YIPGERP'\n",
    "_port=1521\n",
    "_servicename='PROD'\n",
    "_username='yipgbi'\n",
    "_password='yipgbi'\n",
    "\n",
    "\n",
    "data_base_file=r'D:\\ETL_Orable_To_BQ\\etl_web_admin\\etl_config_transaction.db'\n",
    "sqlite3.register_adapter(np.int64, lambda val: int(val))\n",
    "sqlite3.register_adapter(np.int32, lambda val: int(val))\n",
    "\n",
    "json_credential_file=r'C:\\Windows\\mismgntdata-bigquery--bq-loader-34713c332847.json'\n",
    "\n",
    "temp_path=f'temp/{source_name}.csv'\n",
    "\n",
    "\n",
    "start_date_query=''\n",
    "updateCol='last_update_date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b81b25c9-9ca4-4a92-8a0b-56b744c1cd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-29 14:25:28\n",
      "2023-05-29 14:25:28\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now()\n",
    "dtStr_imported=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#dtStr_imported='2023-05-23 23:00:00'\n",
    "\n",
    "dt_imported=datetime.strptime(dtStr_imported,\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(dtStr_imported)\n",
    "print(dt_imported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f042ef-0a73-42e0-8f4a-d8abb35c86e4",
   "metadata": {},
   "source": [
    "# Manage Log Error Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b133498e-6330-40ce-8d7f-4c00eeab1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logErrorMessage(errorList):\n",
    "    def logError(recordList):\n",
    "        try:\n",
    "            sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "            cursor = sqliteConnection.cursor()\n",
    "            sqlite_insert_query = \"\"\"\n",
    "            INSERT INTO log_error\n",
    "            (error_datetime,etl_datetime, data_source_id,message)  VALUES (?,?,?,?);\n",
    "             \"\"\"\n",
    "            cursor.executemany(sqlite_insert_query, recordList)\n",
    "            print(\"Done Log Error\")\n",
    "            sqliteConnection.commit()\n",
    "            cursor.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "            logErrorMessage(listError)\n",
    "        finally:\n",
    "            if sqliteConnection:\n",
    "                sqliteConnection.close()\n",
    "            \n",
    "    if len(errorList)>0:\n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        print(error_message)\n",
    "        \n",
    "        dfError=pd.DataFrame(data=errorList,columns=[\"error_datetime\",\"etl_datetime\",\"data_source_id\",\"message\"])\n",
    "        print(dfError)\n",
    "        logError(dfError.to_records(index=False))\n",
    "        \n",
    "        error_message=f\"{source_name} ETL at {dt_imported} raise some errors.\"\n",
    "        \n",
    "        # send email to admin\n",
    "        raise  Exception(error_message)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cada0-53c7-4004-ab0e-29bb8a3683b0",
   "metadata": {},
   "source": [
    "# Get & Set Oracle ViewName and other configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "699f0142-e421-4899-ae80-687c39a0ccc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from data_source where id='yip_ar_receipt'  \n"
     ]
    }
   ],
   "source": [
    "# get data from data_source\n",
    "def get_ds(data_source_name):\n",
    "   try: \n",
    "        conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        sql_ds=f\"\"\"select * from data_source where id='{data_source_name}'  \"\"\"\n",
    "        print(sql_ds)\n",
    "        df_item=pd.read_sql_query(sql_ds, conn)\n",
    "        if df_item.empty==False:\n",
    "           return df_item.iloc[0,:]\n",
    "        else:\n",
    "           return None\n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "ds_item=get_ds(source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e6b500f-a80e-4910-ac1a-15d40a2974d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data source config data\n",
      "Column to load at first = receipt_date\n",
      "Partition : apply_date - MONTH\n",
      "[] (No cluster cols)\n",
      "[] (No Date cols)\n"
     ]
    }
   ],
   "source": [
    "if ds_item is not None:\n",
    "    print(\"Load data source config data\")\n",
    "\n",
    "    colFirstLoad=ds_item['first_load_col']\n",
    "    print(f\"Column to load at first = {colFirstLoad}\")\n",
    "\n",
    "    partitionCol=ds_item['partition_date_col']  # required DateTime Type\n",
    "    if   ds_item['partition_date_type']==\"DAY\":\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    elif ds_item['partition_date_type']==\"MONTH\":\n",
    "     partitionType=bigquery.TimePartitioningType.MONTH    \n",
    "    elif ds_item['partition_date_type']==\"YEAR\":\n",
    "     partitionType=bigquery.TimePartitioningType.YEAR   \n",
    "    else:\n",
    "     partitionType=bigquery.TimePartitioningType.DAY\n",
    "    \n",
    "    print(f\"Partition : {partitionCol} - {partitionType}\")\n",
    "    \n",
    "        \n",
    "    if ds_item['cluster_col_list']=='':\n",
    "     clusterCols=[]   \n",
    "     print(f\"{clusterCols} (No cluster cols)\")   \n",
    "     \n",
    "    else:\n",
    "     clusterCols=  ds_item['cluster_col_list'].split(',') \n",
    "     clusterCols = list(map(str.strip,clusterCols))\n",
    "     print(clusterCols)\n",
    "\n",
    "    if ds_item['date_col_list']=='':\n",
    "     dateCols=[]   \n",
    "     print(f\"{dateCols} (No Date cols)\")   \n",
    "     \n",
    "    else:\n",
    "     dateCols=  ds_item['date_col_list'].split(',') \n",
    "     dateCols = list(map(str.strip,dateCols))\n",
    "     print(dateCols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7fbc6-59e4-4dee-8a60-b9d7bf24df6a",
   "metadata": {},
   "source": [
    "# List Last ETL Transacton by Datasource Name\n",
    "### Get last etl of the specific view to perform incremental update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b7dc53a9-342c-4e67-a9ad-85c249845fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload all data:  2020-01-01\n"
     ]
    }
   ],
   "source": [
    "def get_last_etl_by_ds(data_source):\n",
    "   try: \n",
    "    conn = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "    sql_last_etl=f\"\"\"select etl_datetime,data_source_id from etl_transaction where data_source_id='{data_source}' \n",
    "    order by etl_datetime desc limit 1\n",
    "    \"\"\"\n",
    "    print(sql_last_etl)\n",
    "    df_item=pd.read_sql_query(sql_last_etl, conn)\n",
    "    print(df_item)\n",
    "    return df_item\n",
    "    \n",
    "   except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)]) \n",
    "       logErrorMessage(listError)\n",
    "\n",
    "if isLoadingAllItems==False:\n",
    "    dfLastETL=get_last_etl_by_ds(source_name)\n",
    "    if dfLastETL.empty==False:\n",
    "      start_date_query=dfLastETL.iloc[0,0]\n",
    "      print(f\"Start Import on update_at of last ETL date :  {start_date_query}\" ) \n",
    "    else:\n",
    "       isLoadingAllItems=True \n",
    "       start_date_query=init_date_query\n",
    "       print(f\"No etl transaction , we will get started with importing all items from :  {start_date_query}\" ) \n",
    "else:\n",
    "   start_date_query=init_date_query\n",
    "   print(f\"Reload all data:  {start_date_query}\" ) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233d732-e586-4138-8241-45ea7df0c9b4",
   "metadata": {},
   "source": [
    "# Load data from Oracel  as DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "834285ba-52c9-4449-91cd-be7f7cbe10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select * from yip_ar_receipt   \n",
      "           where  receipt_date>=to_date('2020-01-01','yyyy-mm-dd') \n",
      "           \n"
     ]
    }
   ],
   "source": [
    "def loadData(isReLoadAll):\n",
    "    try:\n",
    "       engine = sqlalchemy.create_engine(f\"oracle+cx_oracle://{_username}:{_password}@{_ip}:{_port}/?service_name={_servicename}\")\n",
    "       if isReLoadAll==True:\n",
    "         sql=f\"\"\"select * from {source_name}   \n",
    "           where  {colFirstLoad}>=to_date('{start_date_query}','yyyy-mm-dd') \n",
    "           \"\"\"    \n",
    "       else:    \n",
    "           sql =f\"\"\"select * from {source_name}   \n",
    "           where  {updateCol}>=to_date('{start_date_query}','yyyy-mm-dd hh24:mi:ss') \"\"\"\n",
    "\n",
    "       print(sql)\n",
    "       dfAll = pd.read_sql(sql, engine)\n",
    "       dfAll['ImportedAt']=dt_imported \n",
    "       return dfAll \n",
    "    except Exception as e:\n",
    "       listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "       logErrorMessage(listError)\n",
    "    \n",
    "\n",
    "dfAll=loadData(isLoadingAllItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea247b06-3a51-4132-be26-79cd281ba16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cash_receipt_id', 'org_id', 'receipt_number', 'method_name', 'receipt_date', 'application_type', 'apply_date', 'amount', 'amount_applied', 'amount_thb', 'exchange_rate', 'currency_code', 'invoice_number', 'invoice_date', 'term_days', 'term_name', 'due_date', 'cust_account_id', 'cust_code', 'cust_name', 'so', 'project', 'day_receipt', 'sale_code', 'sale_name', 'industry_new', 'sector', 'abbrevation', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22723 entries, 0 to 22722\n",
      "Data columns (total 33 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cash_receipt_id   22723 non-null  int64         \n",
      " 1   org_id            22723 non-null  int64         \n",
      " 2   receipt_number    22723 non-null  object        \n",
      " 3   method_name       22723 non-null  object        \n",
      " 4   receipt_date      22723 non-null  datetime64[ns]\n",
      " 5   application_type  22723 non-null  object        \n",
      " 6   apply_date        22723 non-null  datetime64[ns]\n",
      " 7   amount            22723 non-null  float64       \n",
      " 8   amount_applied    22723 non-null  float64       \n",
      " 9   amount_thb        22723 non-null  float64       \n",
      " 10  exchange_rate     22723 non-null  float64       \n",
      " 11  currency_code     22723 non-null  object        \n",
      " 12  invoice_number    22698 non-null  object        \n",
      " 13  invoice_date      22698 non-null  datetime64[ns]\n",
      " 14  term_days         22667 non-null  float64       \n",
      " 15  term_name         22667 non-null  object        \n",
      " 16  due_date          22698 non-null  datetime64[ns]\n",
      " 17  cust_account_id   22723 non-null  int64         \n",
      " 18  cust_code         22723 non-null  object        \n",
      " 19  cust_name         22723 non-null  object        \n",
      " 20  so                43 non-null     object        \n",
      " 21  project           12127 non-null  object        \n",
      " 22  day_receipt       22698 non-null  float64       \n",
      " 23  sale_code         22417 non-null  float64       \n",
      " 24  sale_name         21577 non-null  object        \n",
      " 25  industry_new      17820 non-null  object        \n",
      " 26  sector            17820 non-null  object        \n",
      " 27  abbrevation       17820 non-null  object        \n",
      " 28  creation_date     22723 non-null  datetime64[ns]\n",
      " 29  created_by        22723 non-null  int64         \n",
      " 30  last_update_date  22723 non-null  datetime64[ns]\n",
      " 31  last_updated_by   22723 non-null  int64         \n",
      " 32  ImportedAt        22723 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](7), float64(7), int64(5), object(14)\n",
      "memory usage: 5.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cash_receipt_id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>receipt_number</th>\n",
       "      <th>method_name</th>\n",
       "      <th>receipt_date</th>\n",
       "      <th>application_type</th>\n",
       "      <th>apply_date</th>\n",
       "      <th>amount</th>\n",
       "      <th>amount_applied</th>\n",
       "      <th>amount_thb</th>\n",
       "      <th>...</th>\n",
       "      <th>sale_code</th>\n",
       "      <th>sale_name</th>\n",
       "      <th>industry_new</th>\n",
       "      <th>sector</th>\n",
       "      <th>abbrevation</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>created_by</th>\n",
       "      <th>last_update_date</th>\n",
       "      <th>last_updated_by</th>\n",
       "      <th>ImportedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170090</td>\n",
       "      <td>81</td>\n",
       "      <td>KB/14327707</td>\n",
       "      <td>เช็ค</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>364485.98</td>\n",
       "      <td>364485.98</td>\n",
       "      <td>364485.98</td>\n",
       "      <td>...</td>\n",
       "      <td>100000071.0</td>\n",
       "      <td>Admin-BS, Ms.</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>INFORMATION &amp; COMMUNICATION TECHNOLOGY</td>\n",
       "      <td>ICT</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-29 14:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170309</td>\n",
       "      <td>81</td>\n",
       "      <td>TFR/TMB</td>\n",
       "      <td>เช็ค</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2020-01-14</td>\n",
       "      <td>208000.00</td>\n",
       "      <td>208000.00</td>\n",
       "      <td>208000.00</td>\n",
       "      <td>...</td>\n",
       "      <td>100001504.0</td>\n",
       "      <td>Sirima Kongchana, Miss</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>INFORMATION &amp; COMMUNICATION TECHNOLOGY</td>\n",
       "      <td>ICT</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-29 14:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172514</td>\n",
       "      <td>81</td>\n",
       "      <td>OFF312000420</td>\n",
       "      <td>Payroll Clearing</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>214.00</td>\n",
       "      <td>214.00</td>\n",
       "      <td>214.00</td>\n",
       "      <td>...</td>\n",
       "      <td>100000081.0</td>\n",
       "      <td>Admin-Office, Mrs.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-29 14:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172517</td>\n",
       "      <td>81</td>\n",
       "      <td>OFF312000423</td>\n",
       "      <td>Payroll Clearing</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>535.00</td>\n",
       "      <td>535.00</td>\n",
       "      <td>535.00</td>\n",
       "      <td>...</td>\n",
       "      <td>100000081.0</td>\n",
       "      <td>Admin-Office, Mrs.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-29 14:25:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172520</td>\n",
       "      <td>81</td>\n",
       "      <td>OFF312000426</td>\n",
       "      <td>Payroll Clearing</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>CASH</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>214.00</td>\n",
       "      <td>214.00</td>\n",
       "      <td>214.00</td>\n",
       "      <td>...</td>\n",
       "      <td>100000081.0</td>\n",
       "      <td>Admin-Office, Mrs.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-07 18:32:21</td>\n",
       "      <td>1067</td>\n",
       "      <td>2023-05-29 14:25:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cash_receipt_id  org_id receipt_number       method_name receipt_date   \n",
       "0           170090      81    KB/14327707              เช็ค   2020-01-06  \\\n",
       "1           170309      81        TFR/TMB              เช็ค   2020-01-14   \n",
       "2           172514      81   OFF312000420  Payroll Clearing   2020-03-23   \n",
       "3           172517      81   OFF312000423  Payroll Clearing   2020-03-23   \n",
       "4           172520      81   OFF312000426  Payroll Clearing   2020-03-23   \n",
       "\n",
       "  application_type apply_date     amount  amount_applied  amount_thb  ...   \n",
       "0             CASH 2020-01-06  364485.98       364485.98   364485.98  ...  \\\n",
       "1             CASH 2020-01-14  208000.00       208000.00   208000.00  ...   \n",
       "2             CASH 2020-03-23     214.00          214.00      214.00  ...   \n",
       "3             CASH 2020-03-23     535.00          535.00      535.00  ...   \n",
       "4             CASH 2020-03-23     214.00          214.00      214.00  ...   \n",
       "\n",
       "     sale_code               sale_name industry_new   \n",
       "0  100000071.0           Admin-BS, Ms.   TECHNOLOGY  \\\n",
       "1  100001504.0  Sirima Kongchana, Miss   TECHNOLOGY   \n",
       "2  100000081.0      Admin-Office, Mrs.         None   \n",
       "3  100000081.0      Admin-Office, Mrs.         None   \n",
       "4  100000081.0      Admin-Office, Mrs.         None   \n",
       "\n",
       "                                   sector  abbrevation       creation_date   \n",
       "0  INFORMATION & COMMUNICATION TECHNOLOGY          ICT 2023-05-07 18:32:21  \\\n",
       "1  INFORMATION & COMMUNICATION TECHNOLOGY          ICT 2023-05-07 18:32:21   \n",
       "2                                    None         None 2023-05-07 18:32:21   \n",
       "3                                    None         None 2023-05-07 18:32:21   \n",
       "4                                    None         None 2023-05-07 18:32:21   \n",
       "\n",
       "  created_by    last_update_date last_updated_by          ImportedAt  \n",
       "0       1067 2023-05-07 18:32:21            1067 2023-05-29 14:25:28  \n",
       "1       1067 2023-05-07 18:32:21            1067 2023-05-29 14:25:28  \n",
       "2       1067 2023-05-07 18:32:21            1067 2023-05-29 14:25:28  \n",
       "3       1067 2023-05-07 18:32:21            1067 2023-05-29 14:25:28  \n",
       "4       1067 2023-05-07 18:32:21            1067 2023-05-29 14:25:28  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfAll=dfAll.drop(columns=['receipt_number','method_name','application_type']) # receipt\n",
    "#dfAll=dfAll.drop(columns=['customer_trx_id','org_id']) # invoice\n",
    "\n",
    "listColDF=dfAll.columns.tolist()\n",
    "print(listColDF)\n",
    "\n",
    "print(dfAll.info())\n",
    "dfAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85200a71-7135-44a0-883b-50d64c0a2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfAll.empty:\n",
    "    print(\"No row to import to BQ\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b3f97-ea01-4268-a1af-e23a99793971",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83c4a2b7-a91d-4e08-8885-28532591208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = service_account.Credentials.from_service_account_file(json_credential_file)\n",
    "client = bigquery.Client(credentials=credentials, project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65f21f-dfed-407c-959a-97b7439e0ca9",
   "metadata": {},
   "source": [
    "## Creaste bigquery schema from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55493758-3f7a-4625-9d7e-d8eef8919d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SchemaField('cash_receipt_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('org_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('receipt_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('method_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('receipt_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('application_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('apply_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('amount', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('amount_applied', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('amount_thb', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('exchange_rate', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('currency_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('invoice_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('term_days', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('term_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('due_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('cust_account_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('cust_code', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('cust_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('so', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('day_receipt', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('sale_code', 'FLOAT', 'NULLABLE', None, None, (), None), SchemaField('sale_name', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('industry_new', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('sector', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('abbrevation', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('creation_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('created_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('last_update_date', 'DATETIME', 'NULLABLE', None, None, (), None), SchemaField('last_updated_by', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('ImportedAt', 'DATETIME', 'NULLABLE', None, None, (), None)]\n"
     ]
    }
   ],
   "source": [
    "# schema = [\n",
    "# bigquery.SchemaField(\"CUSTOMER_TRX_ID\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"GL_DATE\", \"DATE\", mode=\"NULLABLE\"),\n",
    "# bigquery.SchemaField(\"DEPT_NAME\", \"STRING\", mode=\"NULLABLE\"),      \n",
    "# bigquery.SchemaField(\"INVOICE_AMOUNT\", \"FLOAT\", mode=\"NULLABLE\"),    \n",
    "# bigquery.SchemaField(\"LAST_UPDATE_DATE\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "# ]\n",
    "\n",
    "schema = []\n",
    "srCols=dfAll.dtypes\n",
    "for name, type_name in srCols.items():\n",
    "    # print(name,type_name)\n",
    "    if str(type_name) in ['int32','int64']:\n",
    "      schema.append(bigquery.SchemaField(name, \"INTEGER\"))\n",
    "    elif str(type_name) =='float64':\n",
    "      schema.append(bigquery.SchemaField(name, \"FLOAT\"))\n",
    "    elif str(type_name) =='datetime64[ns]':\n",
    "      if name in   dateCols:\n",
    "         schema.append(bigquery.SchemaField(name,  \"DATE\"))\n",
    "      else:\n",
    "         schema.append(bigquery.SchemaField(name,  \"DATETIME\"))\n",
    "    else:\n",
    "       schema.append(bigquery.SchemaField(name,  \"STRING\")) \n",
    "      \n",
    "print(schema)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99618a47-0801-4b6a-bd7b-f99bf641b98b",
   "metadata": {},
   "source": [
    "## Check whether dataframe and bigquery schema are the same\n",
    "\n",
    "## Check Existing DataSet and Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "275cd71b-985f-41a5-933c-c4020cb73336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MIS_BI_DW already exists\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "try:\n",
    "    dataset = client.get_dataset(f\"{projectId}.{dataset_id}\")\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "\n",
    "except Exception as ex:\n",
    "    raise(\"Dataset {} is not found\".format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c811b4ff-ecda-468d-b5fc-9b16018c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table():\n",
    "    table = bigquery.Table(table_id,schema=schema)\n",
    "    if  partitionCol!=\"\":\n",
    "     table.time_partitioning = bigquery.TimePartitioning(\n",
    "     type_=partitionType,field=partitionCol)\n",
    "    \n",
    "    if len(clusterCols)>0:\n",
    "     table.clustering_fields = clusterCols\n",
    "\n",
    "    table = client.create_table(table) \n",
    "    print(\n",
    "        \"Created new table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81539c6e-49db-49e9-b5e4-19a98d9fca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ):\n",
    "def check_same_schema():\n",
    "    print(\"===============================================================================================\")\n",
    "    print(\"Check every columns name and partition cluster and date type column on table against dataframe\")\n",
    "    def find_difference(dfX,bqX):\n",
    "        intersec_DF_BQ = [set(dfX).symmetric_difference(set(bqX))]\n",
    "        list_DF_BQ=[]\n",
    "        if len(intersec_DF_BQ)>0:\n",
    "         for item in intersec_DF_BQ:\n",
    "            list_DF_BQ=list_DF_BQ+list(item)\n",
    "        return list_DF_BQ \n",
    "        \n",
    "    listColumnX=find_difference(listColDF,listFieldBQ)\n",
    "    if len(listColumnX)>0:\n",
    "        e=f\"Columns: {listColumnX} are the same on BigQuery and View {source_name} \"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Fields on BQ and DF are ok.\")\n",
    "        \n",
    "        \n",
    "    # PartitionName\n",
    "    if partitionNameBQ!=partitionCol:\n",
    "        e=f\"Partition Column :{partitionNameBQ} in BQ is not the same as {partitionCol} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"Partition Name Fields on BQ and DF is ok.\")    \n",
    "        \n",
    "\n",
    "    # # PartitionDateType\n",
    "    if partitionTypeBQ!=partitionType:\n",
    "        e=f\"Partition Date Type :{partitionTypeBQ} in BQ is not the same as {partitionType} defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    \n",
    "    # Cluster List\n",
    "    listClusterX=find_difference(clusterCols,clusterBQ)\n",
    "    if len( listClusterX)>0:\n",
    "        e=f\"Cluster columns : {listClusterX} are the same on BigQuery defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Cluster on BQ and DF are ok.\")\n",
    "    \n",
    "    # Date Type List\n",
    "    \n",
    "        # Cluster List\n",
    "    listDateColX=find_difference(dateCols,dateTypeBQ)\n",
    "    if len( listDateColX)>0:\n",
    "        e=f\"Date columns : {listDateColX} are the same on BigQuery defined on ETL Config on Web Admin\"\n",
    "        listError.append([datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),dtStr_imported,source_name,str(e)])\n",
    "    else:\n",
    "        print(\"All Date Column on BQ and DF are ok.\")\n",
    "\n",
    "\n",
    "    if len(listError)>0:\n",
    "        logErrorMessage(listError)\n",
    "        # delete table\n",
    "        # set isLoading=True to load all data\n",
    "        \n",
    "#       isLoadingAllItems=True\n",
    "#       start_date_query=init_date_query\n",
    "\n",
    "#       print(\"ReLoad Data due to something in schema changed\")  \n",
    "#       dfAll=loadData(isLoadingAllItems)\n",
    "#       print(dfAll.info())  \n",
    "\n",
    "#       print(\"Delete table and re-create new one.\")\n",
    "#       client.delete_table(table_id, not_found_ok=True)  \n",
    "#       create_table()\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "df2a0173-bf96-4181-bde8-a9d9c140e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mismgntdata-bigquery.MIS_BI_DW.yip_ar_receipt already exists.\n",
      "All Fields : ['cash_receipt_id', 'org_id', 'receipt_number', 'method_name', 'receipt_date', 'application_type', 'apply_date', 'amount', 'amount_applied', 'amount_thb', 'exchange_rate', 'currency_code', 'invoice_number', 'invoice_date', 'term_days', 'term_name', 'due_date', 'cust_account_id', 'cust_code', 'cust_name', 'so', 'project', 'day_receipt', 'sale_code', 'sale_name', 'industry_new', 'sector', 'abbrevation', 'creation_date', 'created_by', 'last_update_date', 'last_updated_by', 'ImportedAt']\n",
      "Partiton Field&Type: receipt_date - MONTH\n",
      "Cluster Field List: []\n",
      "Date Field List: ['receipt_date', 'apply_date', 'invoice_date', 'due_date']\n",
      "===============================================================================================\n",
      "Check every columns name and partition cluster and date type column on table against dataframe\n",
      "All Fields on BQ and DF are ok.\n",
      "All Cluster on BQ and DF are ok.\n",
      "yip_ar_receipt ETL at 2023-05-29 14:25:28 raise some errors.\n",
      "        error_datetime         etl_datetime  data_source_id   \n",
      "0  2023-05-29 14:27:14  2023-05-29 14:25:28  yip_ar_receipt  \\\n",
      "1  2023-05-29 14:27:14  2023-05-29 14:25:28  yip_ar_receipt   \n",
      "\n",
      "                                             message  \n",
      "0  Partition Column :receipt_date in BQ is not th...  \n",
      "1  Date columns : ['apply_date', 'receipt_date', ...  \n",
      "Done Log Error\n"
     ]
    },
    {
     "ename": "Conflict",
     "evalue": "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/mismgntdata-bigquery/datasets/MIS_BI_DW/tables?prettyPrint=false: Already Exists: Table mismgntdata-bigquery:MIS_BI_DW.yip_ar_receipt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 25\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mcheck_same_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "Cell \u001b[1;32mIn[66], line 54\u001b[0m, in \u001b[0;36mcheck_same_schema\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(listError)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mlogErrorMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlistError\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 33\u001b[0m, in \u001b[0;36mlogErrorMessage\u001b[1;34m(errorList)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# send email to admin\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;167;01mException\u001b[39;00m(error_message)\n",
      "\u001b[1;31mException\u001b[0m: yip_ar_receipt ETL at 2023-05-29 14:25:28 raise some errors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     check_same_schema()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[65], line 10\u001b[0m, in \u001b[0;36mcreate_table\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(clusterCols)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      8\u001b[0m  table\u001b[38;5;241m.\u001b[39mclustering_fields \u001b[38;5;241m=\u001b[39m clusterCols\n\u001b[1;32m---> 10\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new table \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mproject, table\u001b[38;5;241m.\u001b[39mdataset_id, table\u001b[38;5;241m.\u001b[39mtable_id)\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\mis_data_google\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:779\u001b[0m, in \u001b[0;36mClient.create_table\u001b[1;34m(self, table, exists_ok, retry, timeout)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset_id}\n\u001b[1;32m--> 779\u001b[0m     api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.createTable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Table\u001b[38;5;241m.\u001b[39mfrom_api_repr(api_response)\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core_exceptions\u001b[38;5;241m.\u001b[39mConflict:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\mis_data_google\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:813\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[0;32m    811\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[0;32m    812\u001b[0m     ):\n\u001b[1;32m--> 813\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\mis_data_google\\lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\mis_data_google\\lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\mis_data_google\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mConflict\u001b[0m: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/mismgntdata-bigquery/datasets/MIS_BI_DW/tables?prettyPrint=false: Already Exists: Table mismgntdata-bigquery:MIS_BI_DW.yip_ar_receipt"
     ]
    }
   ],
   "source": [
    "\n",
    "# table    \n",
    "try:\n",
    "    table=client.get_table(table_id)\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    # if no table it will call create_table\n",
    "    \n",
    "    listFieldBQ=[field.name for field in table.schema]\n",
    "    \n",
    "    # required field\n",
    "    partitionNameBQ=table.time_partitioning.field\n",
    "    partitionTypeBQ=table.partitioning_type\n",
    "\n",
    "    clusterBQ=table.clustering_fields\n",
    "    if clusterBQ is None : clusterBQ =[]\n",
    "        \n",
    "    dateTypeBQ=[field.name for field in table.schema if field.field_type=='DATE']\n",
    "    \n",
    "    \n",
    "    print(f\"All Fields : {listFieldBQ}\")\n",
    "    print(f\"Partiton Field&Type: {partitionNameBQ} - {partitionTypeBQ}\")\n",
    "    print(f\"Cluster Field List: {clusterBQ}\")\n",
    "    print(f\"Date Field List: {dateTypeBQ}\")\n",
    "    \n",
    "    #check_same_schema(listFieldBQ,partitionNameBQ,partitionTypeBQ,clusterBQ,dateTypeBQ)\n",
    "    check_same_schema()\n",
    "        \n",
    "except Exception as ex:\n",
    "    create_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac562df-e933-4563-96f8-6db9af10ad1d",
   "metadata": {},
   "source": [
    "# To load data into BQ , technically you need  to save it as CSV file first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b278c1c-50ed-446a-9e1a-11129557fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22233 rows are about to be imported to BQ\n"
     ]
    }
   ],
   "source": [
    "if dfAll.empty==False:\n",
    "    no_rows=len(dfAll)\n",
    "    print(f\"{no_rows} rows are about to be imported to BQ\")\n",
    "    dfAll.to_csv (temp_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9425d0-984d-4446-a353-7bb916da82e2",
   "metadata": {},
   "source": [
    "# Load data from CSV file to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e74bd60-ed17-4269-8524-f82bf422b91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22233 rows and 30 columns to mismgntdata-bigquery.MIS_BI_DW.yip_invoice_monthly\n"
     ]
    }
   ],
   "source": [
    "# if isLoadingAllItems==False:\n",
    "# print(\"Load with appending\")\n",
    "\n",
    "# Addtional Try Error\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "    autodetect=False,write_disposition=\"WRITE_APPEND\"\n",
    "    )\n",
    "# else:\n",
    "#     print(\"Load with all data\")\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1,\n",
    "#         autodetect=False,write_disposition=\"WRITE_TRUNCATE\"\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "with open(temp_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "job.result()  # Waits for the job to complete.\n",
    "\n",
    "table = client.get_table(table_id)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        no_rows, len(table.schema), table_id\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c93e0-eb5b-43db-9296-7e7ab1b66a43",
   "metadata": {},
   "source": [
    "# Create Transation and delete csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf4ca4b4-7085-4384-b8ea-655426dc3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ETL Trasaction\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Addtional Try Error    \n",
    "def insertETLTrans(recordList):\n",
    "    try:\n",
    "        sqliteConnection = sqlite3.connect(os.path.abspath(data_base_file))\n",
    "        cursor = sqliteConnection.cursor()\n",
    "        sqlite_insert_query = \"\"\"\n",
    "        INSERT INTO etl_transaction\n",
    "        (etl_datetime, data_source_id,no_rows,is_load_all)  VALUES (?,?,?,?);\n",
    "         \"\"\"\n",
    "\n",
    "        cursor.executemany(sqlite_insert_query, recordList)\n",
    "        print(\"Done ETL Trasaction\")\n",
    "        sqliteConnection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    except sqlite3.Error as error:\n",
    "        print(\"Failed to insert etl_transaction table\", error)\n",
    "    finally:\n",
    "        if sqliteConnection:\n",
    "            sqliteConnection.close()\n",
    "            \n",
    "\n",
    "\n",
    "if isLoadingAllItems==True:\n",
    "    is_load_all=1\n",
    "else:\n",
    "    is_load_all=0\n",
    "\n",
    "dfETFTran=pd.DataFrame.from_records([{'etl_datetime':dtStr_imported,'data_source_id':source_name,'no_rows':no_rows,'is_load_all':is_load_all}])\n",
    "recordsToInsert=list(dfETFTran.to_records(index=False))\n",
    "insertETLTrans(recordsToInsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e88468-f281-4385-bfab-472b0e65aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted temp/yip_invoice_monthly.csv\n"
     ]
    }
   ],
   "source": [
    "#Addtional Try Error\n",
    "if os.path.exists(temp_path):\n",
    "  os.remove(temp_path)\n",
    "  print(f\"Deleted {temp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de3e6c80-0ead-4bc2-a08d-e1b9ed2bc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any error , send mail to adminstrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc1aaf-bfc3-466c-aa61-7684f7f351c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
